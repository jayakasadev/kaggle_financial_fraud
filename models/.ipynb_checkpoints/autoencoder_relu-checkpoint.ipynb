{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 21:18:17 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 22%   49C    P2    46W / 200W |    861MiB /  8118MiB |     19%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           388MiB |\r\n",
      "|    0      2077      G   compiz                                       194MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    63MiB |\r\n",
      "|    0     13425      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\")(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\")(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\")(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\")(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\")(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\")(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 1074972479081.5903 - acc: 0.5876 - val_loss: 45597213088.5526 - val_acc: 0.6759\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 39841412444.0861 - acc: 0.7925 - val_loss: 37548051843.9053 - val_acc: 0.7912\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 37278299802.1287 - acc: 0.7476 - val_loss: 37275732397.6290 - val_acc: 0.7134\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 37119754922.8967 - acc: 0.6947 - val_loss: 37257892249.7529 - val_acc: 0.6992\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 37151848854.9902 - acc: 0.8024 - val_loss: 37331357235.7120 - val_acc: 0.8134\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 37055934749.9683 - acc: 0.8378 - val_loss: 37240511209.0107 - val_acc: 0.7906\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 36982573088.4224 - acc: 0.8392 - val_loss: 37206681696.9059 - val_acc: 0.8497\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 37015184752.2620 - acc: 0.8131 - val_loss: 37058784211.6611 - val_acc: 0.8472\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 36947717012.1732 - acc: 0.8402 - val_loss: 37022926780.0431 - val_acc: 0.8560\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 36989133121.6859 - acc: 0.7910 - val_loss: 37000161604.7364 - val_acc: 0.7826\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 36657372186.4203 - acc: 0.7844 - val_loss: 32587742420.8430 - val_acc: 0.7661\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 30808586905.4960 - acc: 0.7567 - val_loss: 30488769906.1215 - val_acc: 0.7650\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 29906252935.3991 - acc: 0.7700 - val_loss: 30652689414.1157 - val_acc: 0.7902\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 29671297261.0315 - acc: 0.7736 - val_loss: 32744347111.3863 - val_acc: 0.7180\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 24280674818.1762 - acc: 0.7566 - val_loss: 6082698156.3264 - val_acc: 0.7644\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 6247932740.9492 - acc: 0.7442 - val_loss: 8835039360.9832 - val_acc: 0.5398\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 5324884493.7978 - acc: 0.7472 - val_loss: 4099637113.6152 - val_acc: 0.8098\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 2957072261.3475 - acc: 0.8325 - val_loss: 2127536120.5138 - val_acc: 0.8651\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 2339125357.9491 - acc: 0.8725 - val_loss: 2041045284.5510 - val_acc: 0.8760\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 2189827224.1166 - acc: 0.8627 - val_loss: 2004653971.5555 - val_acc: 0.9084\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 2323918486.9742 - acc: 0.8829 - val_loss: 2043929760.9726 - val_acc: 0.8643\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 1950172325.6062 - acc: 0.8839 - val_loss: 1852266757.8215 - val_acc: 0.8697\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 2049774951.1406 - acc: 0.8864 - val_loss: 1782228819.4247 - val_acc: 0.8758\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 1937949154.3770 - acc: 0.8786 - val_loss: 3724693373.0126 - val_acc: 0.8133\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 1961355022.5250 - acc: 0.8762 - val_loss: 1828175734.3801 - val_acc: 0.8622\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 1905131813.9167 - acc: 0.8548 - val_loss: 1756891419.7282 - val_acc: 0.8652\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 2222948879.8127 - acc: 0.8598 - val_loss: 1742788155.7829 - val_acc: 0.8782\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 2037274802.2016 - acc: 0.8546 - val_loss: 1824084156.0947 - val_acc: 0.9176\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 1855742297.1269 - acc: 0.8653 - val_loss: 1913938639.4842 - val_acc: 0.8578\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 1948871491.5653 - acc: 0.8600 - val_loss: 1716015616.5356 - val_acc: 0.8138\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 1932330033.4862 - acc: 0.8603 - val_loss: 2015734339.8965 - val_acc: 0.9036\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1838286171.8957 - acc: 0.8819 - val_loss: 1698420672.6551 - val_acc: 0.8678\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1838338928.1727 - acc: 0.8768 - val_loss: 1684086579.4329 - val_acc: 0.8685\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1871158699.7776 - acc: 0.8652 - val_loss: 1544129180.0475 - val_acc: 0.8345\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1664437061.8301 - acc: 0.8492 - val_loss: 1477869339.5157 - val_acc: 0.9140\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1249512114.6385 - acc: 0.9271 - val_loss: 1127094335.6787 - val_acc: 0.9463\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 769195967.4394 - acc: 0.9498 - val_loss: 530291599.7389 - val_acc: 0.9621\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 613308439.0709 - acc: 0.9550 - val_loss: 484868425.4564 - val_acc: 0.9469\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 635992992.9911 - acc: 0.9469 - val_loss: 425793773.8312 - val_acc: 0.9538\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 691997629.9077 - acc: 0.9446 - val_loss: 434132703.0810 - val_acc: 0.9380\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 544387941.8614 - acc: 0.9344 - val_loss: 407041870.0422 - val_acc: 0.9463\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 551310639.6353 - acc: 0.9285 - val_loss: 428964935.8314 - val_acc: 0.9142\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 602732959.4119 - acc: 0.9240 - val_loss: 449240483.2553 - val_acc: 0.9401\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 499789957.8155 - acc: 0.9242 - val_loss: 442051617.8437 - val_acc: 0.9199\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 562797653.7763 - acc: 0.9207 - val_loss: 402320897.0213 - val_acc: 0.9331\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 650530353.5917 - acc: 0.9214 - val_loss: 748921365.9152 - val_acc: 0.9428\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 667854586.6488 - acc: 0.9214 - val_loss: 390691041.4657 - val_acc: 0.9310\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 451274855.0922 - acc: 0.9210 - val_loss: 399416122.2801 - val_acc: 0.9079\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 516663814.5954 - acc: 0.9171 - val_loss: 372035107.7912 - val_acc: 0.9167\n",
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 502653633.6296 - acc: 0.9212 - val_loss: 559473157.8411 - val_acc: 0.8958\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 538544178.9434 - acc: 0.9162 - val_loss: 364664193.4404 - val_acc: 0.9164\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 495768430.8961 - acc: 0.9172 - val_loss: 371384239.8193 - val_acc: 0.9193\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 479059578.7250 - acc: 0.9172 - val_loss: 359785433.3579 - val_acc: 0.9236\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 477367126.6921 - acc: 0.9189 - val_loss: 355465943.0534 - val_acc: 0.9078\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 427716750.6369 - acc: 0.9163 - val_loss: 459700868.5547 - val_acc: 0.9008\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 612469642.3985 - acc: 0.9209 - val_loss: 389622887.8697 - val_acc: 0.9140\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 478888371.4272 - acc: 0.9132 - val_loss: 356564703.4296 - val_acc: 0.9138\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 451511719.1087 - acc: 0.9130 - val_loss: 353889816.7131 - val_acc: 0.9216\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 620033702.3008 - acc: 0.9148 - val_loss: 520272136.1886 - val_acc: 0.9297\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 438688048.5973 - acc: 0.9127 - val_loss: 371012347.1513 - val_acc: 0.9244\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 549444607.4922 - acc: 0.9148 - val_loss: 377951960.0880 - val_acc: 0.9235\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 495088308.7975 - acc: 0.9147 - val_loss: 461398990.5637 - val_acc: 0.9231\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 497021731.9143 - acc: 0.9127 - val_loss: 546492596.3232 - val_acc: 0.9186\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 414805833.0127 - acc: 0.9110 - val_loss: 388785389.8897 - val_acc: 0.9341\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 473469402.5421 - acc: 0.9121 - val_loss: 385480177.7517 - val_acc: 0.9033\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 484225187.3301 - acc: 0.9111 - val_loss: 351424776.9224 - val_acc: 0.9105\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421756758.1934 - acc: 0.9154 - val_loss: 405720182.4077 - val_acc: 0.9247\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 461875106.9367 - acc: 0.9137 - val_loss: 355460628.3141 - val_acc: 0.9278\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 426629936.6094 - acc: 0.9156 - val_loss: 372853740.3027 - val_acc: 0.9234\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 441250174.7797 - acc: 0.9141 - val_loss: 403963573.4482 - val_acc: 0.9332\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 505036632.6997 - acc: 0.9175 - val_loss: 356476302.8440 - val_acc: 0.8964\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 422261600.1534 - acc: 0.9171 - val_loss: 342996726.3854 - val_acc: 0.9269\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 452573662.9323 - acc: 0.9132 - val_loss: 347281573.6663 - val_acc: 0.9341\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 477088879.6866 - acc: 0.9218 - val_loss: 361846746.4384 - val_acc: 0.9457\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 417993035.4198 - acc: 0.9203 - val_loss: 383417301.5627 - val_acc: 0.9318\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 392996281.1864 - acc: 0.9169 - val_loss: 349729594.8075 - val_acc: 0.9261\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 480345344.5560 - acc: 0.9150 - val_loss: 683729010.4991 - val_acc: 0.8755\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 401119011.5532 - acc: 0.9150 - val_loss: 374631455.1137 - val_acc: 0.9086\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 424443135.6367 - acc: 0.9159 - val_loss: 653766428.3960 - val_acc: 0.8895\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 439416267.8788 - acc: 0.9119 - val_loss: 374903010.7372 - val_acc: 0.9183\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 409008575.8615 - acc: 0.9127 - val_loss: 338305751.4997 - val_acc: 0.9061\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 404029905.5074 - acc: 0.9140 - val_loss: 357790622.3086 - val_acc: 0.9053\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 465787243.9981 - acc: 0.9116 - val_loss: 419631498.0316 - val_acc: 0.9124\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 428200689.2036 - acc: 0.9135 - val_loss: 333767797.2531 - val_acc: 0.9100\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 381879435.4654 - acc: 0.9175 - val_loss: 352093470.1581 - val_acc: 0.9282\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 451894810.3896 - acc: 0.9163 - val_loss: 338735026.3755 - val_acc: 0.9301\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 424164736.4969 - acc: 0.9130 - val_loss: 363878066.5590 - val_acc: 0.9100\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 361907643.6205 - acc: 0.9096 - val_loss: 430636096.8201 - val_acc: 0.8869\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 589346592.1524 - acc: 0.9185 - val_loss: 332446687.6057 - val_acc: 0.9065\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 372319058.4682 - acc: 0.9158 - val_loss: 467043102.3706 - val_acc: 0.9191\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 436538613.2572 - acc: 0.9082 - val_loss: 379919954.8312 - val_acc: 0.9239\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 381641071.8295 - acc: 0.9094 - val_loss: 436613739.2041 - val_acc: 0.9166\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 519273461.6114 - acc: 0.9112 - val_loss: 332179969.1827 - val_acc: 0.9180\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 378342938.3373 - acc: 0.9065 - val_loss: 439854378.1362 - val_acc: 0.8866\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 358123992.0267 - acc: 0.9093 - val_loss: 365612400.5937 - val_acc: 0.8964\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 399810660.6680 - acc: 0.9050 - val_loss: 556580845.2150 - val_acc: 0.8919\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 366942013.3956 - acc: 0.9037 - val_loss: 455707139.1927 - val_acc: 0.9083\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 374938973.1570 - acc: 0.8999 - val_loss: 368797915.8344 - val_acc: 0.9088\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 396275154.5050 - acc: 0.9008 - val_loss: 387089854.8480 - val_acc: 0.8832\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 384734689.1320 - acc: 0.9074 - val_loss: 337441224.9598 - val_acc: 0.9222\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 443345698.7597 - acc: 0.9032 - val_loss: 1657436320.1666 - val_acc: 0.8531\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 446256407.4015 - acc: 0.9028 - val_loss: 329513900.6572 - val_acc: 0.9138\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 393628118.9801 - acc: 0.9033 - val_loss: 353126616.9818 - val_acc: 0.8832\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 374716374.5355 - acc: 0.9021 - val_loss: 457096945.3448 - val_acc: 0.9042\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 397742881.6923 - acc: 0.9025 - val_loss: 1322910626.6203 - val_acc: 0.8779\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 438433028.4023 - acc: 0.9062 - val_loss: 383887059.7550 - val_acc: 0.9146\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 389497295.5420 - acc: 0.9075 - val_loss: 423177343.7177 - val_acc: 0.9043\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 371331015.4016 - acc: 0.9037 - val_loss: 892559404.0438 - val_acc: 0.8690\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 380684250.6285 - acc: 0.9047 - val_loss: 340888477.5710 - val_acc: 0.9204\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 392341879.9207 - acc: 0.9054 - val_loss: 324462544.7601 - val_acc: 0.9206\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 367172524.0046 - acc: 0.9069 - val_loss: 334544704.6286 - val_acc: 0.9294\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 457205895.3554 - acc: 0.9073 - val_loss: 346734196.2317 - val_acc: 0.8925\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 365174522.8936 - acc: 0.9058 - val_loss: 329264752.1290 - val_acc: 0.9114\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 408726167.1487 - acc: 0.9054 - val_loss: 4051277172.8675 - val_acc: 0.8633\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 404351760.7103 - acc: 0.9082 - val_loss: 326739225.8766 - val_acc: 0.9179\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 371898531.4798 - acc: 0.9051 - val_loss: 317945106.7773 - val_acc: 0.9100\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 381939599.9166 - acc: 0.9061 - val_loss: 326839367.4418 - val_acc: 0.8818\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 351130366.3198 - acc: 0.9045 - val_loss: 465133669.2872 - val_acc: 0.8810\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 439396773.4529 - acc: 0.9052 - val_loss: 339580798.6219 - val_acc: 0.9227\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 333061666.6689 - acc: 0.9037 - val_loss: 460761212.0281 - val_acc: 0.8816\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 416951962.6715 - acc: 0.9099 - val_loss: 889664542.7200 - val_acc: 0.9221\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 392308825.2481 - acc: 0.9045 - val_loss: 1612202890.1304 - val_acc: 0.8613\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 364458833.0141 - acc: 0.9071 - val_loss: 326796738.2562 - val_acc: 0.8901\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 381936318.1033 - acc: 0.9037 - val_loss: 444620923.6555 - val_acc: 0.9224\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 421480696.6367 - acc: 0.9032 - val_loss: 344019910.6708 - val_acc: 0.8839\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 371162060.3856 - acc: 0.8993 - val_loss: 358530915.4621 - val_acc: 0.9172\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 417725393.0231 - acc: 0.9062 - val_loss: 319527838.7020 - val_acc: 0.9218\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 358477046.5347 - acc: 0.9029 - val_loss: 321278248.9142 - val_acc: 0.9172\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 418461911.9347 - acc: 0.9036 - val_loss: 308206110.6238 - val_acc: 0.8980\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 387213450.4306 - acc: 0.9049 - val_loss: 320416122.6779 - val_acc: 0.8925\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 342321737.5316 - acc: 0.9004 - val_loss: 329459599.3626 - val_acc: 0.9047\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 366255472.2266 - acc: 0.8992 - val_loss: 444194364.8186 - val_acc: 0.8692\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 413678460.6936 - acc: 0.9012 - val_loss: 977055003.9655 - val_acc: 0.9038\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 353000581.9905 - acc: 0.8963 - val_loss: 728446300.0202 - val_acc: 0.8981\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 476426682.8289 - acc: 0.9033 - val_loss: 318461622.3993 - val_acc: 0.9053\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 431371165.7064 - acc: 0.8996 - val_loss: 5708012163.4476 - val_acc: 0.8765\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 350648731.3126 - acc: 0.9000 - val_loss: 327761228.7413 - val_acc: 0.8989\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 404418557.0775 - acc: 0.9010 - val_loss: 1938751463.7031 - val_acc: 0.9071\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 366885656.3193 - acc: 0.9012 - val_loss: 308168201.6026 - val_acc: 0.9164\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 347904810.5830 - acc: 0.9010 - val_loss: 323267339.8676 - val_acc: 0.9093\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 372066586.4667 - acc: 0.8961 - val_loss: 375504835.1208 - val_acc: 0.8848\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 370473185.9519 - acc: 0.8962 - val_loss: 684070181.7427 - val_acc: 0.8898\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 352093475.3248 - acc: 0.8949 - val_loss: 303772688.3652 - val_acc: 0.9099\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 368306796.3513 - acc: 0.8955 - val_loss: 303512794.7746 - val_acc: 0.8993\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 346941732.0343 - acc: 0.8899 - val_loss: 528900486.3826 - val_acc: 0.8493\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 396634217.4474 - acc: 0.8919 - val_loss: 310545801.5842 - val_acc: 0.9024\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 348339436.0597 - acc: 0.8914 - val_loss: 305995431.5422 - val_acc: 0.8878\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 385978595.1036 - acc: 0.8932 - val_loss: 382422798.3602 - val_acc: 0.9004\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 420290704.0175 - acc: 0.9003 - val_loss: 307799932.4959 - val_acc: 0.9078\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 394468234.1599 - acc: 0.8965 - val_loss: 330832977.5439 - val_acc: 0.9076\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 334138801.8065 - acc: 0.8925 - val_loss: 434535501.3564 - val_acc: 0.9073\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 521903454.7385 - acc: 0.8904 - val_loss: 298054201.3149 - val_acc: 0.9037\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 396558923.4216 - acc: 0.8948 - val_loss: 306013982.7212 - val_acc: 0.9107\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 340708814.5072 - acc: 0.8943 - val_loss: 338864273.5391 - val_acc: 0.8937\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 393803534.8627 - acc: 0.8894 - val_loss: 1618757860.0971 - val_acc: 0.8472\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 377795182.1879 - acc: 0.8966 - val_loss: 1586044175.5068 - val_acc: 0.8941\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 425871648.8829 - acc: 0.8973 - val_loss: 317125719.4910 - val_acc: 0.8894\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 382281449.8546 - acc: 0.8974 - val_loss: 2022566113.0027 - val_acc: 0.8642\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 438820943.3390 - acc: 0.8919 - val_loss: 346356468.2264 - val_acc: 0.9140\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 364497447.2807 - acc: 0.9000 - val_loss: 295415453.2415 - val_acc: 0.8911\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 349867990.8663 - acc: 0.8893 - val_loss: 301712011.7463 - val_acc: 0.8896\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 383360792.9123 - acc: 0.8890 - val_loss: 295021142.1662 - val_acc: 0.8939\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 318869822.2830 - acc: 0.8921 - val_loss: 287491747.5572 - val_acc: 0.8856\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 370397834.3288 - acc: 0.8920 - val_loss: 291916098.3124 - val_acc: 0.8788\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 428282488.4114 - acc: 0.8868 - val_loss: 789046541.5164 - val_acc: 0.8840\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 328124053.0373 - acc: 0.8899 - val_loss: 395240600.4677 - val_acc: 0.8763\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 379554064.4676 - acc: 0.8892 - val_loss: 320738996.3123 - val_acc: 0.9000\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 418322811.3880 - acc: 0.9003 - val_loss: 296279024.8734 - val_acc: 0.8956\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 321467103.5827 - acc: 0.8933 - val_loss: 332599789.6505 - val_acc: 0.8782\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 380819261.3186 - acc: 0.8952 - val_loss: 309297933.1091 - val_acc: 0.9090\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 375619213.6092 - acc: 0.8913 - val_loss: 319823941.9388 - val_acc: 0.9055\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 328564360.0992 - acc: 0.8937 - val_loss: 296220565.4837 - val_acc: 0.8974\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 355743151.1288 - acc: 0.8928 - val_loss: 329213833.8504 - val_acc: 0.9045\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 372518817.0453 - acc: 0.8953 - val_loss: 335466822.4055 - val_acc: 0.9032\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 351844580.1465 - acc: 0.8957 - val_loss: 295528652.5649 - val_acc: 0.8990\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 381432376.2205 - acc: 0.8966 - val_loss: 377319490.0149 - val_acc: 0.9055\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 330678897.9241 - acc: 0.8936 - val_loss: 325867077.3165 - val_acc: 0.8700\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 332043323.0708 - acc: 0.8947 - val_loss: 341320710.9606 - val_acc: 0.8836\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 427495642.5425 - acc: 0.9029 - val_loss: 276411666.1361 - val_acc: 0.9106\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 341329619.1563 - acc: 0.8979 - val_loss: 316321821.1168 - val_acc: 0.9118\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 400656864.7696 - acc: 0.8971 - val_loss: 288837085.7066 - val_acc: 0.9029\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 374873691.2457 - acc: 0.9009 - val_loss: 289944168.5634 - val_acc: 0.9040\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 345714797.4681 - acc: 0.8983 - val_loss: 287534647.5950 - val_acc: 0.8972\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 363211557.0738 - acc: 0.8949 - val_loss: 280081538.8279 - val_acc: 0.9017\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 382976107.4867 - acc: 0.8966 - val_loss: 305165270.0676 - val_acc: 0.8761\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 321617271.1743 - acc: 0.8935 - val_loss: 330680237.2358 - val_acc: 0.8981\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 352069973.9531 - acc: 0.8957 - val_loss: 290458964.3560 - val_acc: 0.8988\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 387894515.2824 - acc: 0.8987 - val_loss: 299192103.3091 - val_acc: 0.9022\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 331806245.9923 - acc: 0.9002 - val_loss: 302100299.9078 - val_acc: 0.9009\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 398663867.2518 - acc: 0.9010 - val_loss: 307688291.6614 - val_acc: 0.9056\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 375867544.7520 - acc: 0.9012 - val_loss: 289385120.5553 - val_acc: 0.8931\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 451801133.2168 - acc: 0.9046 - val_loss: 284087226.1892 - val_acc: 0.9079\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 356336224.7944 - acc: 0.9079 - val_loss: 317675848.0837 - val_acc: 0.8959\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 331548434.5245 - acc: 0.9149 - val_loss: 305350299.3960 - val_acc: 0.9303\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 372005389.0757 - acc: 0.9138 - val_loss: 284620476.9967 - val_acc: 0.9133\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 311994263.4908 - acc: 0.9082 - val_loss: 284267452.4784 - val_acc: 0.9302\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 418439830.9769 - acc: 0.9182 - val_loss: 969952293.9413 - val_acc: 0.8806\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 342724387.5417 - acc: 0.9146 - val_loss: 274456130.1490 - val_acc: 0.8976\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 340768225.3792 - acc: 0.9101 - val_loss: 281155801.2612 - val_acc: 0.9131\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 330431570.2484 - acc: 0.9107 - val_loss: 307052779.5013 - val_acc: 0.8925\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE2.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa843a170b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuUXGWd7vHvU33JtRMg3TAhARMRVOQwAQPCIA4cFRJAEEEuiqOOy+icAXEpjHAcUZlzzjjjGcZhBgVcZrwCIsiYGYNk0ODlcE1iwHCJCQiTJpCEQAIhl779zh97d1FpqnZ3OtlV3ezns1avqtq1q+pXu6vr6fd99363IgIzMzOAUqMLMDOzkcOhYGZmZQ4FMzMrcyiYmVmZQ8HMzMocCmZmVuZQMBsiSd+W9L+GuO6Tkt61u89jVm8OBTMzK3MomJlZmUPBXlPSbptLJT0k6WVJ35K0n6TbJb0k6U5Je1esf7qkhyVtknSXpDdX3HeEpGXp434IjB3wWqdJWp4+9m5Jhw+z5o9LWi3peUkLJO2fLpekf5S0XtLm9D0dlt53iqRH0tqelnTJsDaY2QAOBXstOgt4N3AI8B7gduB/Au0kn/lPAUg6BLgR+DTQASwE/l1Sq6RW4N+A7wH7AD9Kn5f0sUcC84FPAFOA64AFksbsSqGS/jvwt8A5wFTgKeCm9O6TgHek72Mv4FxgY3rft4BPREQbcBjwi115XbNaRmUoSJqf/ve0YgjrviP9b69H0tkVy2dJuif9L/EhSefmW7XV0T9HxLqIeBr4NXBfRPw2InYAtwFHpOudC/w0Iv4zIrqB/wuMA/4EOAZoAb4WEd0RcQvwQMVrfBy4LiLui4jeiPgOsCN93K74IDA/Ipal9V0OHCtpBtANtAFvAhQRj0bEM+njuoFDJU2KiBciYtkuvq5ZVaMyFIBvA3OGuO5/AR8BbhiwfCvwZxHxlvS5viZprz1VoDXUuorr26rcnphe35/kP3MAIqIPWANMS+97OnaeMfKpiuuvAz6bdh1tkrQJOCB93K4YWMMWktbAtIj4BfAvwDXAOknXS5qUrnoWcArwlKRfSjp2F1/XrKpRGQoR8Svg+cplkg6S9DNJSyX9WtKb0nWfjIiHgL4Bz/H7iFiVXl8LrCfpQrDiWEvy5Q4kffgkX+xPA88A09Jl/Q6suL4G+N8RsVfFz/iIuHE3a5hA0h31NEBEXB0RbwXeQtKNdGm6/IGIOAPYl6Sb6+ZdfF2zqkZlKNRwPXBR+gd0CfD1oT5Q0tFAK/B4TrXZyHQzcKqkd0pqAT5L0gV0N3AP0AN8SlKzpPcBR1c89pvAJyW9LR0QniDpVEltu1jDDcBH0+7MMcD/IenuelLSUenztwAvA9uB3nTM44OSJqfdXi8CvbuxHczKXhOhIGkiST/wjyQtJxn0mzrEx04lGUz8aNp9YAURESuBC4B/Bp4jGZR+T0R0RUQX8D6SrscXSMYfflzx2CUk4wr/kt6/Ol13V2v4OfAF4FaS1slBwHnp3ZNIwucFki6mjSTjHgAfAp6U9CLwyfR9mO02jdaT7KQDcf8REYel/awrI6JmEEj6drr+LRXLJgF3AX8bET/KtWAzs1HgNdFSiIgXgT9Iej+U9+/+46zHpLsc3gZ814FgZpYYlS0FSTcCJ5Dsd74O+CLJftrfIOk2agFuiogrJR1F8uW/N0mf7LMR8RZJFwD/Cjxc8dQfiYjldXsjZmYjzKgMBTMzy8drovvIzMz2jOZGF7Cr2tvbY8aMGY0uw8xsVFm6dOlzETHosVijLhRmzJjBkiVLGl2GmdmoIumpwddy95GZmVVwKJiZWZlDwczMykbdmIKZ2XB0d3fT2dnJ9u3bG11KrsaOHcv06dNpaWkZ1uMdCmZWCJ2dnbS1tTFjxgx2nvz2tSMi2LhxI52dncycOXNYz+HuIzMrhO3btzNlypTXbCAASGLKlCm71RpyKJhZYbyWA6Hf7r7HwoTCkief56pFK+nq8ezYZma1FCYUlv3XC1z9i9V09zoUzKz+Nm3axNe/PuRzf5WdcsopbNq0KYeKqitMKJTSJlWfJwA0swaoFQq9vdknzVu4cCF77VW/08cXZu8jlUOhwYWYWSFddtllPP7448yaNYuWlhYmTpzI1KlTWb58OY888gjvfe97WbNmDdu3b+fiiy9m3rx5wCtT+2zZsoW5c+fy9re/nbvvvptp06bxk5/8hHHjxu3ROgsTCqV07MVThZvZl//9YR5Z++Iefc5D95/EF9/zlpr3f+UrX2HFihUsX76cu+66i1NPPZUVK1aUdx2dP38+++yzD9u2beOoo47irLPOYsqUKTs9x6pVq7jxxhv55je/yTnnnMOtt97KBRfs2TOxFigU3FIws5Hj6KOP3ulYgquvvprbbrsNgDVr1rBq1apXhcLMmTOZNWsWAG9961t58skn93hdBQqF5NJjCmaW9R99vUyYMKF8/a677uLOO+/knnvuYfz48ZxwwglVjzUYM2ZM+XpTUxPbtm3b43UVZqBZHmg2swZqa2vjpZdeqnrf5s2b2XvvvRk/fjyPPfYY9957b52re0WBWgpJKDgTzKwRpkyZwnHHHcdhhx3GuHHj2G+//cr3zZkzh2uvvZbDDz+cN77xjRxzzDENq7NAoZBcuqVgZo1yww03VF0+ZswYbr/99qr39Y8btLe3s2LFivLySy65ZI/XBwXqPvJAs5nZ4AoTCv3TgfQ5FczMasotFCTNl7Re0ooa90vS1ZJWS3pI0pF51QIeUzAzG4o8WwrfBuZk3D8XODj9mQd8I8daKKXv1GMKZma15RYKEfEr4PmMVc4AvhuJe4G9JE3Nqx7PfWRmNrhGjilMA9ZU3O5Ml72KpHmSlkhasmHDhmG9mOc+MjMbXCNDodqZIKp+ZUfE9RExOyJmd3R0DOvFPPeRmTXScKfOBvja177G1q1b93BF1TUyFDqBAypuTwfW5vVi3iXVzBpptIRCIw9eWwBcKOkm4G3A5oh4Jq8X88FrZtZIlVNnv/vd72bffffl5ptvZseOHZx55pl8+ctf5uWXX+acc86hs7OT3t5evvCFL7Bu3TrWrl3LiSeeSHt7O4sXL861ztxCQdKNwAlAu6RO4ItAC0BEXAssBE4BVgNbgY/mVQu80lLodVPBzG6/DJ793Z59zj/6bzD3KzXvrpw6e9GiRdxyyy3cf//9RASnn346v/rVr9iwYQP7778/P/3pT4FkTqTJkydz1VVXsXjxYtrb2/dszVXkFgoRcf4g9wfwl3m9/kA+TsHMRopFixaxaNEijjjiCAC2bNnCqlWrOP7447nkkkv43Oc+x2mnncbxxx9f99qKM/eRj1Mws34Z/9HXQ0Rw+eWX84lPfOJV9y1dupSFCxdy+eWXc9JJJ3HFFVfUtbYCTXPh4xTMrHEqp84++eSTmT9/Plu2bAHg6aefZv369axdu5bx48dzwQUXcMkll7Bs2bJXPTZvxWkpeO8jM2ugyqmz586dywc+8AGOPfZYACZOnMj3v/99Vq9ezaWXXkqpVKKlpYVvfCOZ6GHevHnMnTuXqVOnjt6B5pHGxymYWaMNnDr74osv3un2QQcdxMknn/yqx1100UVcdNFFudbWrzDdR24pmJkNrjChIB+nYGY2qMKEgifEM7MidB/v7nssXCgU4DNhZlWMHTuWjRs3vqaDISLYuHEjY8eOHfZzFG6g2S0Fs2KaPn06nZ2dDHem5dFi7NixTJ8+fdiPL0woeOpss2JraWlh5syZjS5jxCtQ91Fy6ZaCmVltBQqF/jEFh4KZWS2FC4W+vgYXYmY2ghUmFHycgpnZ4AoTCj6i2cxscMUJhfSdekzBzKy2woRCk1sKZmaDKkwo9B+n0OuWgplZTYUJBU+dbWY2uAKFgifEMzMbTPFCwccpmJnVVJhQ8HEKZmaDK0wolEqeOtvMbDDFCQW3FMzMBlWgUPBxCmZmgylMKHhMwcxscIUJBU+dbWY2uMKFgruPzMxqyzUUJM2RtFLSakmXVbn/QEmLJf1W0kOSTsmrFg80m5kNLrdQkNQEXAPMBQ4Fzpd06IDV/hq4OSKOAM4Dvp5jPYBbCmZmWfJsKRwNrI6IJyKiC7gJOGPAOgFMSq9PBtbmVYznPjIzG1yeoTANWFNxuzNdVulLwAWSOoGFwEXVnkjSPElLJC3ZsGHDsIrx3EdmZoPLMxRUZdnAb+TzgW9HxHTgFOB7kl5VU0RcHxGzI2J2R0fHsIrxQLOZ2eDyDIVO4ICK29N5dffQx4CbASLiHmAs0J5HMT5OwcxscHmGwgPAwZJmSmolGUheMGCd/wLeCSDpzSShMLz+oUE0ee4jM7NB5RYKEdEDXAjcATxKspfRw5KulHR6utpngY9LehC4EfhI5DQS/MrU2U4FM7NamvN88ohYSDKAXLnsiorrjwDH5VlDv1eOU6jHq5mZjU6FOaLZ52g2MxtcYUIBktaCj1MwM6utYKEg731kZpahgKHQ6CrMzEauQoWC5OMUzMyyFCoUSpKPUzAzy1CwUPBxCmZmWQoWCh5TMDPLUqhQ8JiCmVm2QoVCqSQfp2BmlqFYoeDuIzOzTAULBXcfmZllKVQoyC0FM7NMhQoFz31kZpatUKHQ5LmPzMwyFSoU3H1kZpatUKFQKnmg2cwsS7FCwXMfmZllKlwouKVgZlZboUJBgl4PKpiZ1VSoUHD3kZlZtoKFggeazcyyFCwUPKZgZpalUKHg4xTMzLIVKhQ8zYWZWbaChYJbCmZmWQoWCh5oNjPLkmsoSJojaaWk1ZIuq7HOOZIekfSwpBtyrsctBTOzDM15PbGkJuAa4N1AJ/CApAUR8UjFOgcDlwPHRcQLkvbNqx7wmIKZ2WDybCkcDayOiCciogu4CThjwDofB66JiBcAImJ9jvV4l1Qzs0HkGQrTgDUVtzvTZZUOAQ6R9P8k3StpTo71JKHQl+crmJmNbrl1HwGqsmzgv+nNwMHACcB04NeSDouITTs9kTQPmAdw4IEHDr8gDzSbmWXKs6XQCRxQcXs6sLbKOj+JiO6I+AOwkiQkdhIR10fE7IiY3dHRMeyCmkqe+8jMLEueofAAcLCkmZJagfOABQPW+TfgRABJ7STdSU/kVZDHFMzMsuUWChHRA1wI3AE8CtwcEQ9LulLS6elqdwAbJT0CLAYujYiNedXk7iMzs2x5jikQEQuBhQOWXVFxPYDPpD+58xHNZmbZhtRSkHSxpElKfEvSMkkn5V3cnubjFMzMsg21++jPI+JF4CSgA/go8JXcqsqJWwpmZtmGGgr9u5eeAvxrRDxI9V1ORzR5oNnMLNNQQ2GppEUkoXCHpDZg1B0GVvI5ms3MMg11oPljwCzgiYjYKmkfki6kUcXnaDYzyzbUlsKxwMqI2CTpAuCvgc35lZWPUsm7pJqZZRlqKHwD2Crpj4G/Ap4CvptbVTnxmIKZWbahhkJPekzBGcA/RcQ/AW35lZUPdx+ZmWUb6pjCS5IuBz4EHJ+eK6Elv7Ly4TOvmZllG2pL4VxgB8nxCs+STIH91dyqyomPUzAzyzakUEiD4AfAZEmnAdsjYhSOKbilYGaWZajTXJwD3A+8HzgHuE/S2XkWlgePKZiZZRvqmMLngaP6T5cpqQO4E7glr8Ly4DEFM7NsQx1TKA04f/LGXXjsiOHzKZiZZRtqS+Fnku4Abkxvn8uAKbFHg1LJA81mZlmGFAoRcamks4DjSCbCuz4ibsu1shx46mwzs2xDPslORNwK3JpjLbnzLqlmZtkyQ0HSS0C1r1GRnDhtUi5V5cRjCmZm2TJDISJG3VQWWSToc1PBzKymUbcH0e7wcQpmZtkKFgo+TsHMLEvBQsEDzWZmWQoVCpLodUvBzKymQoWCj1MwM8tWsFBw95GZWZaChYIHms3MshQqFJTukuouJDOz6goVCiUJwMcqmJnVkGsoSJojaaWk1ZIuy1jvbEkhaXae9ZSSTHAXkplZDbmFgqQm4BpgLnAocL6kQ6us1wZ8Crgvr1r6ldJU8GCzmVl1ebYUjgZWR8QTEdEF3AScUWW9vwH+HtieYy1AMvcRuKVgZlZLnqEwDVhTcbszXVYm6QjggIj4j6wnkjRP0hJJSzZs2DDsgjymYGaWLc9QUJVl5a9jSSXgH4HPDvZEEXF9RMyOiNkdHR3DLqhJ/d1HTgUzs2ryDIVO4ICK29OBtRW324DDgLskPQkcAyzIc7DZ3UdmZtnyDIUHgIMlzZTUCpwHLOi/MyI2R0R7RMyIiBnAvcDpEbEkr4JK8kCzmVmW3EIhInqAC4E7gEeBmyPiYUlXSjo9r9fN0r9Lqg9eMzOrbsjnaB6OiFgILByw7Ioa656QZy3gXVLNzAZTqCOa5YFmM7NMhQoFH9FsZpatYKHg4xTMzLIULBSSS7cUzMyqK1Qo9I8p9Hqk2cysqkKFgruPzMyyFSwUkkt3H5mZVVewUPBxCmZmWQoVCp77yMwsW6FC4ZUxBYeCmVk1hQwFdx+ZmVVXsFBILt19ZGZWXaFCoTz3UV+DCzEzG6EKFQpNJU+IZ2aWpVCh8Mr5FBpbh5nZSFWwUHBLwcwsS6FCwccpmJllK1QoeJdUM7NshQwFH7xmZlZdwUIhuXRLwcysukKFgs/RbGaWrVCh4COazcyyFSsUSj7JjplZlmKFglsKZmaZChUKPkezmVm2QoWCz9FsZpatYKGQXLr7yMysuoKFgo9oNjPLkmsoSJojaaWk1ZIuq3L/ZyQ9IukhST+X9Lp860ku3VIwM6sut1CQ1ARcA8wFDgXOl3TogNV+C8yOiMOBW4C/z6se8DQXZmaDybOlcDSwOiKeiIgu4CbgjMoVImJxRGxNb94LTM+xHncfmZkNIs9QmAasqbjdmS6r5WPA7dXukDRP0hJJSzZs2DDsgprSd+vuIzOz6vIMBVVZVvXbWNIFwGzgq9Xuj4jrI2J2RMzu6OgYfkFuKZiZZWrO8bk7gQMqbk8H1g5cSdK7gM8DfxoRO3Ksx2MKZmaDyLOl8ABwsKSZklqB84AFlStIOgK4Djg9ItbnWAvg4xTMzAaTWyhERA9wIXAH8Chwc0Q8LOlKSaenq30VmAj8SNJySQtqPN0eUR5o7svzVczMRq88u4+IiIXAwgHLrqi4/q48X38gH6dgZpatkEc0OxPMzKorZCi4pWBmVl3BQiG59C6pZmbVFSoUfI5mM7NshQqF/paCj1MwM6uuYKHgI5rNzLIUMhR8Ok4zs+oKFQryhHhmZpkKFQo+TsHMLFvBQiG5dEvBzKy6goWCB5rNzLIUNBScCmZm1RQsFJJLH6dgZlZdwULB3UdmZlkKFQqeOtvMLFvBQkFIbimYmdVSqFCApAvJYwpmZtUVMBTcfWRmVkvhQkGSu4/MzGooXCi4pWBmVlsBQ0Ge+8jMrIbmRhdQN73d8MyDlCT63H9kZlZVcVoKv/w7mD+Hd+p+jymYmdVQnJbCsRfCE3fxD51X8euli1n22HhKpRKhEqEmQIRKIBGUkvM5qwQq8dz417Ny6pmcdewhTNtrXKPfiZlZbjTa9tmfPXt2LFmyZHgP3vESG278C7qeeZQd3d0o+iiR/CgCEYg+SumlCJqilyl6kWdjb36ut3HAIUey37g+przwEGMmTaHt7H/Zs2/QzCwHkpZGxOzB1itOSwFgTBsdH/n+rj/uD79m8l1XcdZTixn7+58BsCXGMlHbua75vXzktBMZ09y0h4s1M6u/YoXCcM08nnEzjyd6dvDc+mdYvxW6Xt7ErNv+lPUP/Jh/Gjudv5rzpkZXaWa22xwKu0DNY2jffwbt/QvuPozzX/wdZ977FH9xwkG0jW1pZHlmZrst172PJM2RtFLSakmXVbl/jKQfpvffJ2lGnvXscW86lYO2/Y5ZXcv4ze03Ql9foysyM9stuQ00S2oCfg+8G+gEHgDOj4hHKtb5H8DhEfFJSecBZ0bEuVnPu1sDzXvaMw/Cde8o31zV9AZW7H82+7zhKCaMH8/YlhZaW5pRqUSpqQQ0USqJ1s1/oHXTarr3eSN9k1+X7vGUzN7aRzKTqxAliVJfF+rrhaZmmlpaaGpupbm5laaWVppbxtLcOobmpnRvKWusvl7o3gZjJla/r7QL4059vbByIWx9Hg57H4xpG/wxPTugt2to6w7Vsyvgzi/CtNlw/Gdgx0vQMh5ax++516imayt0b4XmMcn76e2Gl56FSdOgVJw96fekoQ405xkKxwJfioiT09uXA0TE31asc0e6zj2SmoFngY7IKGpEhUIE3P3PPBcTWbzyOU5c+03ae9fXvYzt0cJ2xrBDrfRW9Ai+shFVvh6ovGQMXTTTSw/N9NBEL03ECA2XV6oa2udVQ1xvV0W5EpVvB1Ai2Dc20EoPG7U3WxmH6KOJPibFS0xgGxu1F9sYV+P5dn5nE9nKPrEJgJcZx0btjdL3lbxilKtopoe2eJlxbAfgBSbzkqoE04BXVfk5kucrEYyPbbTSXf48tcfz7KCVCWxjO62MpYteSjyrjp0+a3vSOLbTERvLt1/QZMbHNsbQxVbGslH77PTZrnxXIhjHNlqjmy610E0LPTTttN7uij34XLtqw+xPM/vUjw/rsSNh76NpwJqK253A22qtExE9kjYDU4DnKleSNA+YB3DggQfmVe+uk+C4T9EOvP/tQFxK17rHWP/4Q+zo7qKrq4fu3h6irw8i+Ym+XraN248X295A2+bfM2b7c0QEJVV+3UQ6vXfQo1b61Ax9Paivm+hNftTbDb1dqLcLerajnm2oeyuKvvJzQPLVoXglEip1l8bQq2aaooemvm5K9A7+niPz5m4RMaQ/uMHXiSGut2sGBo2Iim0LK1v+lG1NbXR0ddIcXfTv5Ly1aRLbSxOY1LORlthR8/kq519ZpyZuazuezc1TeNvmnzG29+XyWaKCnS971cy2UhsvN7XRpybau55mbN/WzPcSKG2h7vylur00gR610BI7aOnbwaNNk1nU/kEO3PYYh2/5DetbD2Bc78t0dK3Z6TO2J/Wolftap7OtNIHWvh20dz/NjtJ4nmvZn/26nmJi7+Zyxf0qf9M7SuPoUQvN0U1zdFOKnj1W2579RMUuP+OYtvbBV9pNeYZCtXc78PMzlHWIiOuB6yFpKex+aTmRaP2jNzP9j948xAe8NddybHQ7qnztggZWAScCcALwyYbWYfWRZ+dcJ3BAxe3pwNpa66TdR5OB53OsyczMMuQZCg8AB0uaKakVOA9YMGCdBcCH0+tnA7/IGk8wM7N85dZ9lI4RXAjcATQB8yPiYUlXAksiYgHwLeB7klaTtBDOy6seMzMbXK4Hr0XEQmDhgGVXVFzfDrw/zxrMzGzovMOvmZmVORTMzKzMoWBmZmUOBTMzKxt1J9mRtAF4apgPb2fA0dIjyEitzXXtGte160Zqba+1ul4XER2DrTTqQmF3SFoylLk/GmGk1ua6do3r2nUjtbai1uXuIzMzK3MomJlZWdFC4fpGF5BhpNbmunaN69p1I7W2QtZVqDEFMzPLVrSWgpmZZXAomJlZWWFCQdIcSSslrZZ0WQPrOEDSYkmPSnpY0sXp8i9JelrS8vTnlAbU9qSk36WvvyRdto+k/5S0Kr3cu841vbFimyyX9KKkTzdqe0maL2m9pBUVy6puIyWuTj9zD0k6ss51fVXSY+lr3yZpr3T5DEnbKrbdtXWuq+bvTtLl6fZaKenkvOrKqO2HFXU9KWl5urwu2yzj+6F+n7GIeM3/kEzd/TjweqAVeBA4tEG1TAWOTK+3Ab8HDgW+BFzS4O30JNA+YNnfA5el1y8D/q7Bv8dngdc1ansB7wCOBFYMto2AU4DbSc4weAxwX53rOgloTq//XUVdMyrXa8D2qvq7S/8OHgTGADPTv9mmetY24P5/AK6o5zbL+H6o22esKC2Fo4HVEfFERHQBNwFnNKKQiHgmIpal118CHiU5V/VIdQbwnfT6d4D3NrCWdwKPR8Rwj2jfbRHxK159dsBa2+gM4LuRuBfYS9LUetUVEYsiyicovpfk7Id1VWN71XIGcFNE7IiIPwCrSf52616bJAHnADfm9fo1aqr1/VC3z1hRQmEasKbidicj4ItY0gzgCOC+dNGFaRNwfr27aVIBLJK0VNK8dNl+EfEMJB9YYN8G1NXvPHb+I2309upXaxuNpM/dn5P8R9lvpqTfSvqlpOMbUE+1391I2l7HA+siYlXFsrpuswHfD3X7jBUlFFRlWUP3xZU0EbgV+HREvAh8AzgImAU8Q9J0rbfjIuJIYC7wl5Le0YAaqlJyStfTgR+li0bC9hrMiPjcSfo80AP8IF30DHBgRBwBfAa4QdKkOpZU63c3IrZX6nx2/gekrtusyvdDzVWrLNutbVaUUOgEDqi4PR1Y26BakNRC8gv/QUT8GCAi1kVEb0T0Ad8kx2ZzLRGxNr1cD9yW1rCuvzmaXq6vd12pucCyiFiX1tjw7VWh1jZq+OdO0oeB04APRtoJnXbPbEyvLyXpuz+kXjVl/O4avr0AJDUD7wN+2L+sntus2vcDdfyMFSUUHgAOljQz/Y/zPGBBIwpJ+yq/BTwaEVdVLK/sBzwTWDHwsTnXNUFSW/91kkHKFSTb6cPpah8GflLPuirs9J9bo7fXALW20QLgz9I9RI4BNvd3AdSDpDnA54DTI2JrxfIOSU3p9dcDBwNP1LGuWr+7BcB5ksZImpnWdX+96qrwLuCxiOjsX1CvbVbr+4F6fsbyHk0fKT8ko/S/J0n4zzewjreTNO8eApanP6cA3wN+ly5fAEytc12vJ9nz40Hg4f5tBEwBfg6sSi/3acA2Gw9sBCZXLGvI9iIJpmeAbpL/0j5WaxuRNO2vST9zvwNm17mu1ST9zf2fs2vTdc9Kf8cPAsuA99S5rpq/O+Dz6fZaCcyt9+8yXf5t4JMD1q3LNsv4fqjbZ8zTXJiZWVlRuo/MzGwIHApmZlbmUDAzszKHgpmZlTkUzMyszKFgVkeSTpD0H42uw6wWh4KZmZU5FMyqkHSBpPvTufOvk9QkaYukf5C0TNLPJXWk686SdK9eOW9B/1z3b5B0p6QH08cclD79REm3KDnXwQ/So1jNRgSHgtlw+43YAAABVklEQVQAkt4MnEsyQeAsoBf4IDCBZP6lI4FfAl9MH/Jd4HMRcTjJUaX9y38AXBMRfwz8CcnRs5DMfPlpknnyXw8cl/ubMhui5kYXYDYCvRN4K/BA+k/8OJIJyPp4ZZK07wM/ljQZ2Csifpku/w7wo3QeqWkRcRtARGwHSJ/v/kjn1VFyZq8ZwG/yf1tmg3MomL2agO9ExOU7LZS+MGC9rDlisrqEdlRc78V/hzaCuPvI7NV+DpwtaV8onx/3dSR/L2en63wA+E1EbAZeqDjpyoeAX0YyB36npPemzzFG0vi6vguzYfB/KGYDRMQjkv6a5Cx0JZJZNP8SeBl4i6SlwGaScQdIpjK+Nv3SfwL4aLr8Q8B1kq5Mn+P9dXwbZsPiWVLNhkjSloiY2Og6zPLk7iMzMytzS8HMzMrcUjAzszKHgpmZlTkUzMyszKFgZmZlDgUzMyv7/9Am2SzZBk7FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE2.h5')\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.    322819.7    28919.469 348633.75  462520.88  123339.32\n",
      "      0.         0.         0.         0.         0.   ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
