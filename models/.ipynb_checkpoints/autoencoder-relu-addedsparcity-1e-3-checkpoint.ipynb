{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 23:39:01 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 22%   49C    P2    47W / 200W |    872MiB /  8118MiB |     20%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           389MiB |\r\n",
      "|    0      2077      G   compiz                                       190MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    77MiB |\r\n",
      "|    0     19984      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\", activity_regularizer=regularizers.l1(1e-3))(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1059758200607.8086 - acc: 0.5989 - val_loss: 877115350460.9384 - val_acc: 0.6003\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 866445625675.7878 - acc: 0.5890 - val_loss: 873391040380.6328 - val_acc: 0.6050\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 863131616452.8427 - acc: 0.6008 - val_loss: 871390800025.1746 - val_acc: 0.5798\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 860811911422.0198 - acc: 0.6158 - val_loss: 867487124783.5527 - val_acc: 0.6793\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 856643364548.0928 - acc: 0.6664 - val_loss: 865623551624.2859 - val_acc: 0.6163\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 842510003553.0531 - acc: 0.6663 - val_loss: 832028525448.7234 - val_acc: 0.7067\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 821610262305.2550 - acc: 0.7048 - val_loss: 830143785336.4785 - val_acc: 0.7088\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 820964527495.0109 - acc: 0.7108 - val_loss: 829660062269.8311 - val_acc: 0.7107\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 821070366793.5436 - acc: 0.7192 - val_loss: 829579845100.9789 - val_acc: 0.7011\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 820636533117.6693 - acc: 0.7262 - val_loss: 829316751948.4766 - val_acc: 0.7270\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 820792591513.4653 - acc: 0.7361 - val_loss: 829403858177.0009 - val_acc: 0.7367\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 820313733390.8680 - acc: 0.7359 - val_loss: 829079026243.5847 - val_acc: 0.7388\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 464149336417.8107 - acc: 0.7658 - val_loss: 2762007473.4664 - val_acc: 0.7811\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2983342657.6499 - acc: 0.7454 - val_loss: 2449783208.0740 - val_acc: 0.7233\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2689093637.2282 - acc: 0.7311 - val_loss: 2316663749.0357 - val_acc: 0.7335\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2609982905.0766 - acc: 0.7328 - val_loss: 2320519188.5877 - val_acc: 0.8040\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2624361432.7054 - acc: 0.7307 - val_loss: 2369087253.7923 - val_acc: 0.7184\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2421892975.5310 - acc: 0.7326 - val_loss: 2197210905.9289 - val_acc: 0.7317\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2555083219.0508 - acc: 0.7319 - val_loss: 2180885868.8029 - val_acc: 0.7329\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2465915157.5480 - acc: 0.7280 - val_loss: 2238042435.6652 - val_acc: 0.7267\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2364719793.2440 - acc: 0.7256 - val_loss: 2161535169.8596 - val_acc: 0.7055\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2352451354.8659 - acc: 0.7176 - val_loss: 2159016803.5061 - val_acc: 0.7122\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2275561000.8438 - acc: 0.7074 - val_loss: 2325978296.0574 - val_acc: 0.7008\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2266690733.4244 - acc: 0.7006 - val_loss: 2118376194.8139 - val_acc: 0.6997\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2315228578.7741 - acc: 0.6982 - val_loss: 2115385720.5113 - val_acc: 0.6990\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2639348686.2187 - acc: 0.6979 - val_loss: 2102832648.1602 - val_acc: 0.7161\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 2200961056.4878 - acc: 0.6959 - val_loss: 2090908811.9272 - val_acc: 0.6912\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 2283247651.6807 - acc: 0.6932 - val_loss: 2055218916.9722 - val_acc: 0.6946\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 2276603304.7305 - acc: 0.6920 - val_loss: 2068391170.0344 - val_acc: 0.7050\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 2230022352.6424 - acc: 0.6986 - val_loss: 2156524980.9869 - val_acc: 0.6966\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 2235901013.9994 - acc: 0.7008 - val_loss: 2123738795.4739 - val_acc: 0.6999\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2172069915.9050 - acc: 0.7012 - val_loss: 2227206229.7281 - val_acc: 0.6999\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2239749913.1089 - acc: 0.7062 - val_loss: 2026019463.4812 - val_acc: 0.6913\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2176086427.8475 - acc: 0.7010 - val_loss: 2120911142.3288 - val_acc: 0.6980\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2131103386.3881 - acc: 0.7065 - val_loss: 2085880258.7800 - val_acc: 0.7150\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2131344431.3907 - acc: 0.7098 - val_loss: 1951092062.1750 - val_acc: 0.7146\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 2154926969.6406 - acc: 0.7123 - val_loss: 1957741648.4146 - val_acc: 0.7091\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 2118508935.4865 - acc: 0.7183 - val_loss: 1959092027.2938 - val_acc: 0.7290\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 2040374457.8252 - acc: 0.7236 - val_loss: 1929249423.8777 - val_acc: 0.7373\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 2021071821.0741 - acc: 0.7375 - val_loss: 2475171721.5080 - val_acc: 0.7560\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 2005791291.2657 - acc: 0.7568 - val_loss: 2011249957.6851 - val_acc: 0.7535\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2159150022.6952 - acc: 0.7622 - val_loss: 1922489628.6473 - val_acc: 0.8216\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1949687061.1249 - acc: 0.7964 - val_loss: 2111470582.9924 - val_acc: 0.7800\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1942793154.8415 - acc: 0.8029 - val_loss: 1776605851.2378 - val_acc: 0.8105\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1909386008.8154 - acc: 0.8115 - val_loss: 1760001765.5732 - val_acc: 0.8058\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1846242274.5223 - acc: 0.8277 - val_loss: 1732576897.0989 - val_acc: 0.8304\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1996905489.6058 - acc: 0.8243 - val_loss: 1726909938.5401 - val_acc: 0.8317\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1845975030.6220 - acc: 0.8426 - val_loss: 6055996877.3391 - val_acc: 0.7896\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1919468167.4903 - acc: 0.8546 - val_loss: 1714713985.0260 - val_acc: 0.8747\n",
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1785898745.7416 - acc: 0.8515 - val_loss: 1743543118.7411 - val_acc: 0.8441\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1799595454.0057 - acc: 0.8436 - val_loss: 1692727820.5697 - val_acc: 0.8387\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1766275179.0699 - acc: 0.8465 - val_loss: 1703729352.3425 - val_acc: 0.8571\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1785971710.2931 - acc: 0.8472 - val_loss: 1703046095.8211 - val_acc: 0.8110\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1796907702.7940 - acc: 0.8515 - val_loss: 1677515283.8006 - val_acc: 0.8484\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1759382233.7352 - acc: 0.8455 - val_loss: 1687627196.6882 - val_acc: 0.8588\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1805107241.2953 - acc: 0.8464 - val_loss: 1628575492.9024 - val_acc: 0.8593\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1697624499.2277 - acc: 0.8268 - val_loss: 1674918147.7029 - val_acc: 0.7290\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1775471369.0960 - acc: 0.7484 - val_loss: 2602089710.1407 - val_acc: 0.7045\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1749367276.2977 - acc: 0.7683 - val_loss: 1609154834.1623 - val_acc: 0.7894\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1703461796.8395 - acc: 0.8049 - val_loss: 1583085915.6477 - val_acc: 0.8166\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1701654416.8358 - acc: 0.8067 - val_loss: 1612459956.0716 - val_acc: 0.8207\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1729476966.0589 - acc: 0.8129 - val_loss: 2062862178.5832 - val_acc: 0.7959\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1673635589.5864 - acc: 0.8089 - val_loss: 1542149021.4998 - val_acc: 0.7962\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1687155970.1918 - acc: 0.8229 - val_loss: 1526607217.7883 - val_acc: 0.8162\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1639571095.1669 - acc: 0.8331 - val_loss: 1530343119.3182 - val_acc: 0.8374\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1611386116.3895 - acc: 0.8212 - val_loss: 1452013364.0226 - val_acc: 0.8093\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1585340575.2058 - acc: 0.8050 - val_loss: 1418522382.6807 - val_acc: 0.8074\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1555551005.4411 - acc: 0.8047 - val_loss: 1367460942.7386 - val_acc: 0.8130\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1500962525.1981 - acc: 0.8080 - val_loss: 1365489124.9131 - val_acc: 0.8094\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1485669440.3442 - acc: 0.8092 - val_loss: 1317607039.6278 - val_acc: 0.8088\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1466509396.8024 - acc: 0.8111 - val_loss: 1298973429.4773 - val_acc: 0.8077\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1442065428.1082 - acc: 0.8073 - val_loss: 1494159375.0227 - val_acc: 0.8074\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1409383283.2857 - acc: 0.8068 - val_loss: 1305904709.2507 - val_acc: 0.8063\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1444740551.3687 - acc: 0.8014 - val_loss: 1279082069.6577 - val_acc: 0.8088\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1385826434.5512 - acc: 0.8041 - val_loss: 1271191656.4739 - val_acc: 0.8128\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1437649506.5221 - acc: 0.8076 - val_loss: 1333405747.7057 - val_acc: 0.8053\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1370714670.5065 - acc: 0.8221 - val_loss: 1241254596.2523 - val_acc: 0.8199\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1397979101.1078 - acc: 0.8211 - val_loss: 1300870296.1259 - val_acc: 0.8199\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1335167979.3521 - acc: 0.8192 - val_loss: 1236561175.1665 - val_acc: 0.8195\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1357499299.6554 - acc: 0.8294 - val_loss: 1210343208.4789 - val_acc: 0.8260\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1388551412.0285 - acc: 0.8248 - val_loss: 1239595103.3569 - val_acc: 0.8417\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1393384645.3662 - acc: 0.8316 - val_loss: 1280075201.9451 - val_acc: 0.8334\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1317218760.5702 - acc: 0.8303 - val_loss: 1205357950.6810 - val_acc: 0.8341\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1389513442.5766 - acc: 0.8414 - val_loss: 1197923351.6041 - val_acc: 0.8356\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1301206548.9551 - acc: 0.8359 - val_loss: 1255368787.7994 - val_acc: 0.8168\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1320437817.3774 - acc: 0.8378 - val_loss: 1217377193.1176 - val_acc: 0.8244\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1318148926.5043 - acc: 0.8479 - val_loss: 1508944227.5174 - val_acc: 0.8243\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1342982172.7234 - acc: 0.8442 - val_loss: 1169789924.1888 - val_acc: 0.8571\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1333881526.4097 - acc: 0.8446 - val_loss: 1383661202.1397 - val_acc: 0.8431\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1365145572.5787 - acc: 0.8612 - val_loss: 1162078392.1793 - val_acc: 0.8718\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1286326215.5568 - acc: 0.8550 - val_loss: 1178967909.4437 - val_acc: 0.8736\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1306932144.7765 - acc: 0.8678 - val_loss: 1474718758.7614 - val_acc: 0.8228\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1315651042.9753 - acc: 0.8496 - val_loss: 1966885494.6680 - val_acc: 0.8696\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1304730471.5443 - acc: 0.8772 - val_loss: 1148286880.7286 - val_acc: 0.8761\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1317327611.8782 - acc: 0.8670 - val_loss: 1157033605.2733 - val_acc: 0.8655\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1259000846.0340 - acc: 0.8640 - val_loss: 1755567407.2157 - val_acc: 0.8641\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1309500624.7146 - acc: 0.8593 - val_loss: 1177174725.9837 - val_acc: 0.8664\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1301132915.9003 - acc: 0.8584 - val_loss: 1135350832.5146 - val_acc: 0.8723\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1257526110.8189 - acc: 0.8734 - val_loss: 1255410700.9770 - val_acc: 0.8938\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1279768738.0016 - acc: 0.8886 - val_loss: 1216285999.0158 - val_acc: 0.8755\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1219915891.2918 - acc: 0.8990 - val_loss: 1313772101.9699 - val_acc: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1184613954.2332 - acc: 0.9134 - val_loss: 1045387629.1776 - val_acc: 0.9180\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1250721105.2618 - acc: 0.8979 - val_loss: 1030879096.9639 - val_acc: 0.9280\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1191481245.8886 - acc: 0.9149 - val_loss: 1097229128.3060 - val_acc: 0.9050\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1149542799.9665 - acc: 0.9212 - val_loss: 1364868716.3591 - val_acc: 0.9313\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1188698214.4649 - acc: 0.9081 - val_loss: 1066527333.4676 - val_acc: 0.9405\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1216726509.3752 - acc: 0.9169 - val_loss: 1005497945.4373 - val_acc: 0.9114\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1143101445.8551 - acc: 0.9112 - val_loss: 1143033428.8166 - val_acc: 0.9195\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1189213292.6317 - acc: 0.9090 - val_loss: 992304391.1480 - val_acc: 0.9382\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1259392853.8174 - acc: 0.9084 - val_loss: 1035892600.6093 - val_acc: 0.9087\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1124632489.1216 - acc: 0.9108 - val_loss: 1105010648.7068 - val_acc: 0.9276\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1131207729.3139 - acc: 0.9168 - val_loss: 1012020979.6881 - val_acc: 0.9002\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1120312790.4573 - acc: 0.9097 - val_loss: 1092325692.5574 - val_acc: 0.9135\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1173955000.6711 - acc: 0.9152 - val_loss: 3060109843.1041 - val_acc: 0.9043\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1106406476.5009 - acc: 0.9190 - val_loss: 1041393958.5162 - val_acc: 0.9427\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1112259474.6220 - acc: 0.9244 - val_loss: 1021420282.2791 - val_acc: 0.8974\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1073254507.1815 - acc: 0.9229 - val_loss: 2096201094.7771 - val_acc: 0.8802\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1101760829.2696 - acc: 0.9187 - val_loss: 11654950969.8579 - val_acc: 0.8277\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1133316909.1105 - acc: 0.9200 - val_loss: 994640526.8115 - val_acc: 0.9307\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1112165804.9803 - acc: 0.9056 - val_loss: 985337358.3897 - val_acc: 0.9205\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1125409398.3633 - acc: 0.9015 - val_loss: 972567941.1098 - val_acc: 0.8982\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1109838463.4558 - acc: 0.9061 - val_loss: 994605461.1196 - val_acc: 0.9028\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1134445985.2598 - acc: 0.8981 - val_loss: 973633223.6132 - val_acc: 0.9006\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1081442095.1679 - acc: 0.8791 - val_loss: 1008242308.9659 - val_acc: 0.9089\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1138603233.8439 - acc: 0.8914 - val_loss: 988507132.7120 - val_acc: 0.9102\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1054078806.7550 - acc: 0.9019 - val_loss: 2310846784.5482 - val_acc: 0.8323\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1093564415.8629 - acc: 0.8911 - val_loss: 1042154391.1804 - val_acc: 0.8724\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1069210932.3778 - acc: 0.8935 - val_loss: 974100124.6234 - val_acc: 0.8807\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1150582481.0383 - acc: 0.8938 - val_loss: 1090888945.9127 - val_acc: 0.8497\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1062315861.7269 - acc: 0.8833 - val_loss: 959227278.4437 - val_acc: 0.8899\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1094523689.1160 - acc: 0.8882 - val_loss: 1336623118.7725 - val_acc: 0.8679\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1096542037.6185 - acc: 0.8777 - val_loss: 984504957.3407 - val_acc: 0.8933\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1105338669.8890 - acc: 0.8820 - val_loss: 1095953351.7616 - val_acc: 0.8829\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1059472570.0861 - acc: 0.8742 - val_loss: 959387130.3677 - val_acc: 0.9104\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1060837084.9825 - acc: 0.8785 - val_loss: 946921241.0054 - val_acc: 0.8748\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1072468990.7157 - acc: 0.8833 - val_loss: 999322672.7396 - val_acc: 0.9075\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1052315156.8387 - acc: 0.8791 - val_loss: 967635043.1415 - val_acc: 0.8807\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1032990792.7410 - acc: 0.8829 - val_loss: 1018613867.7782 - val_acc: 0.8806\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1025555459.7012 - acc: 0.8730 - val_loss: 948303387.3038 - val_acc: 0.8757\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1064335918.0171 - acc: 0.8796 - val_loss: 992235646.1982 - val_acc: 0.8596\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1054384511.9528 - acc: 0.8745 - val_loss: 970025764.9187 - val_acc: 0.9036\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1026521983.8583 - acc: 0.8790 - val_loss: 951273638.3439 - val_acc: 0.8701\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1067597067.4056 - acc: 0.8900 - val_loss: 951265602.9780 - val_acc: 0.9052\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1082580158.2199 - acc: 0.8836 - val_loss: 1252902185.8884 - val_acc: 0.8864\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1056303546.5401 - acc: 0.8841 - val_loss: 1099721592.5213 - val_acc: 0.8985\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1024267427.3702 - acc: 0.8896 - val_loss: 1254676202.1084 - val_acc: 0.9246\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1042531277.6719 - acc: 0.8915 - val_loss: 1214993949.7198 - val_acc: 0.9306\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1062275045.1685 - acc: 0.8902 - val_loss: 933904086.9503 - val_acc: 0.9067\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1025962024.4213 - acc: 0.8864 - val_loss: 979816469.3648 - val_acc: 0.9147\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1012045991.9618 - acc: 0.8838 - val_loss: 939201144.6577 - val_acc: 0.8951\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1038714886.1035 - acc: 0.8893 - val_loss: 923926953.8230 - val_acc: 0.8729\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1030114169.4657 - acc: 0.8827 - val_loss: 3687028882.6464 - val_acc: 0.9097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1039166411.3127 - acc: 0.8913 - val_loss: 957202456.8463 - val_acc: 0.8947\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1033339395.9870 - acc: 0.8925 - val_loss: 1616446229.8086 - val_acc: 0.8713\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1081236098.2620 - acc: 0.8921 - val_loss: 924586651.3900 - val_acc: 0.8625\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 985607521.2366 - acc: 0.8852 - val_loss: 921595327.7435 - val_acc: 0.8766\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1048646934.4819 - acc: 0.8816 - val_loss: 958170731.0590 - val_acc: 0.8706\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1091421588.9726 - acc: 0.8733 - val_loss: 954550255.1290 - val_acc: 0.8734\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 976887872.4527 - acc: 0.8962 - val_loss: 1796460672.7418 - val_acc: 0.8576\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1025075388.9584 - acc: 0.8997 - val_loss: 990101013.7835 - val_acc: 0.9263\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1050079029.8106 - acc: 0.8962 - val_loss: 928943952.9861 - val_acc: 0.9173\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1046617946.9543 - acc: 0.9044 - val_loss: 909813459.1154 - val_acc: 0.9042\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1030713500.5519 - acc: 0.8962 - val_loss: 1048786137.2789 - val_acc: 0.8801\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1022526051.6202 - acc: 0.8894 - val_loss: 1030888345.9025 - val_acc: 0.9321\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1043387131.0946 - acc: 0.9225 - val_loss: 920433771.4010 - val_acc: 0.9360\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1026741147.4246 - acc: 0.9068 - val_loss: 929358624.7965 - val_acc: 0.9210\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1034704376.2516 - acc: 0.9098 - val_loss: 914633242.0119 - val_acc: 0.9315\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 999587740.3781 - acc: 0.9018 - val_loss: 961638082.5820 - val_acc: 0.8829\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1042564532.5409 - acc: 0.8937 - val_loss: 910313784.2805 - val_acc: 0.8849\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 955718301.3558 - acc: 0.8952 - val_loss: 967046195.9811 - val_acc: 0.8629\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1061350535.9033 - acc: 0.9020 - val_loss: 919351039.0337 - val_acc: 0.8856\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 988933761.7511 - acc: 0.9082 - val_loss: 978181604.7854 - val_acc: 0.9145\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 993983524.4478 - acc: 0.8930 - val_loss: 1447071496.5273 - val_acc: 0.8787\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1097639888.4215 - acc: 0.9049 - val_loss: 909132051.2455 - val_acc: 0.9299\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 970187954.7808 - acc: 0.9053 - val_loss: 1187848490.2455 - val_acc: 0.8999\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 995161928.1705 - acc: 0.8998 - val_loss: 952976058.4174 - val_acc: 0.9120\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1000298128.3167 - acc: 0.8987 - val_loss: 898739210.6698 - val_acc: 0.8912\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1017324454.7464 - acc: 0.9058 - val_loss: 1406689838.9844 - val_acc: 0.8964\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1027887378.5182 - acc: 0.9049 - val_loss: 911533113.0513 - val_acc: 0.9033\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 973262161.5055 - acc: 0.9055 - val_loss: 908019218.2566 - val_acc: 0.9048\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1072262414.7622 - acc: 0.8986 - val_loss: 895874104.4214 - val_acc: 0.8913\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 972483004.2521 - acc: 0.8938 - val_loss: 921646354.8645 - val_acc: 0.9196\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1022736589.9578 - acc: 0.8948 - val_loss: 951443993.9761 - val_acc: 0.8960\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1010174011.8490 - acc: 0.8970 - val_loss: 940056542.3020 - val_acc: 0.8936\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1004532341.2644 - acc: 0.8981 - val_loss: 893392268.5212 - val_acc: 0.8870\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1000315937.8014 - acc: 0.9008 - val_loss: 1130532401.7380 - val_acc: 0.8720\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 970379155.5348 - acc: 0.9028 - val_loss: 1015167220.5890 - val_acc: 0.9034\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1020001202.0146 - acc: 0.8994 - val_loss: 897343744.7563 - val_acc: 0.8890\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 986688038.8188 - acc: 0.8960 - val_loss: 942338112.0673 - val_acc: 0.9145\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 987889361.6444 - acc: 0.9010 - val_loss: 896634357.7351 - val_acc: 0.9024\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 962744336.0431 - acc: 0.9024 - val_loss: 896710402.5706 - val_acc: 0.9226\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1015427181.6805 - acc: 0.9005 - val_loss: 1089635521.2196 - val_acc: 0.9212\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 998439287.1314 - acc: 0.9002 - val_loss: 981004960.5482 - val_acc: 0.8695\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1007289894.6717 - acc: 0.9057 - val_loss: 954650985.3892 - val_acc: 0.8939\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 981183035.2261 - acc: 0.9033 - val_loss: 959941649.4111 - val_acc: 0.8800\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1021341669.7194 - acc: 0.9085 - val_loss: 1053148243.1952 - val_acc: 0.8856\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1185067073.3200 - acc: 0.8954 - val_loss: 889998206.3089 - val_acc: 0.9057\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 942909206.4443 - acc: 0.9036 - val_loss: 1153550123.4802 - val_acc: 0.9384\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1017738063.9840 - acc: 0.9048 - val_loss: 920911150.5871 - val_acc: 0.8971\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1064444698.2396 - acc: 0.8940 - val_loss: 930745452.6684 - val_acc: 0.8955\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE4.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8f2e3c64a8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE4.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.        0.   117254.83 274850.28 453132.38 152003.61      0.\n",
      "      0.        0.        0.        0.  ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
