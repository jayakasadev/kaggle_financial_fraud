{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 22 12:17:55 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "|  0%   46C    P8    16W / 200W |    727MiB /  8118MiB |     17%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1231      G   /usr/lib/xorg/Xorg                           396MiB |\r\n",
      "|    0      3235      G   compiz                                       207MiB |\r\n",
      "|    0      4305      G   ...-token=A0B80834DCD76E08518AE62F4F37B1C4   120MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 11)\n",
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815      170136.00       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295       21249.00        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145         181.00            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671         181.00            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720       41554.00        29885.86   \n",
      "5     1   PAYMENT   7817.71    C90045638       53860.00        46042.29   \n",
      "6     1   PAYMENT   7107.77   C154988899      183195.00       176087.23   \n",
      "7     1   PAYMENT   7861.64  C1912850431      176087.23       168225.59   \n",
      "8     1   PAYMENT   4024.36  C1265012928        2671.00            0.00   \n",
      "9     1     DEBIT   5337.77   C712410124       41720.00        36382.23   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0            0.00        0               0  \n",
      "1  M2044282225             0.0            0.00        0               0  \n",
      "2   C553264065             0.0            0.00        1               0  \n",
      "3    C38997010         21182.0            0.00        1               0  \n",
      "4  M1230701703             0.0            0.00        0               0  \n",
      "5   M573487274             0.0            0.00        0               0  \n",
      "6   M408069119             0.0            0.00        0               0  \n",
      "7   M633326333             0.0            0.00        0               0  \n",
      "8  M1176932104             0.0            0.00        0               0  \n",
      "9   C195600860         41898.0        40348.79        0               0  \n",
      "This is the transformed_datatset\n",
      "(6362620, 18)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \\\n",
      "0            0.00        0        0         0      0        1         0   \n",
      "1            0.00        0        0         0      0        1         0   \n",
      "2            0.00        1        0         0      0        0         1   \n",
      "3            0.00        1        0         1      0        0         0   \n",
      "4            0.00        0        0         0      0        1         0   \n",
      "5            0.00        0        0         0      0        1         0   \n",
      "6            0.00        0        0         0      0        1         0   \n",
      "7            0.00        0        0         0      0        1         0   \n",
      "8            0.00        0        0         0      0        1         0   \n",
      "9        40348.79        0        0         0      1        0         0   \n",
      "\n",
      "   C_orig  M_orig     orig_id  C_dest  M_dest     dest_id  \n",
      "0       1       0  1231006815       0       1  1979787155  \n",
      "1       1       0  1666544295       0       1  2044282225  \n",
      "2       1       0  1305486145       1       0   553264065  \n",
      "3       1       0   840083671       1       0    38997010  \n",
      "4       1       0  2048537720       0       1  1230701703  \n",
      "5       1       0    90045638       0       1   573487274  \n",
      "6       1       0   154988899       0       1   408069119  \n",
      "7       1       0  1912850431       0       1   633326333  \n",
      "8       1       0  1265012928       0       1  1176932104  \n",
      "9       1       0   712410124       1       0   195600860  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/PS_20174392719_1491204439457_log.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "\n",
    "columns=['dest_type', \"dest_id\"]\n",
    "dest_split = fraud_dataset.nameDest.str.split('(\\d+)').tolist()\n",
    "# print(dest_split)\n",
    "dest_split = pd.DataFrame([x[:2] for x in dest_split], columns=columns)\n",
    "dest_split[columns[1]] = pd.to_numeric(dest_split[columns[1]])\n",
    "# encode categorical attributes into a binary one-hot encoded representation\n",
    "nameDest_categ_transformed = pd.concat([pd.get_dummies(dest_split[columns[0]]), dest_split[columns[1]]], axis = 1)\n",
    "nameDest_categ_transformed = nameDest_categ_transformed.rename(columns = {\"C\":\"C_dest\", \"M\":\"M_dest\"})\n",
    "\n",
    "columns=['orig_type', \"orig_id\"]\n",
    "orig_split = fraud_dataset.nameOrig.str.split('(\\d+)').tolist()\n",
    "orig_split = pd.DataFrame([x[:2] for x in orig_split], columns=columns)\n",
    "# print(orig_split.head(10))\n",
    "orig_split[columns[1]] = pd.to_numeric(orig_split[columns[1]])\n",
    "# encode categorical attributes into a binary one-hot encoded representation\n",
    "orig_type = pd.get_dummies(orig_split[columns[0]])\n",
    "# print(type(orig_split))\n",
    "M_orig = orig_type.copy(deep=True)\n",
    "orig_type = orig_type.rename(columns = {\"C\":\"C_orig\"})\n",
    "M_orig = M_orig.rename(columns = {\"C\":\"M_orig\"})\n",
    "M_orig[:] = 0\n",
    "nameOrig_categ_transformed = pd.concat([orig_type, M_orig, orig_split[columns[1]]], axis = 1)\n",
    "\n",
    "type_categ_transformed = pd.get_dummies(fraud_dataset[\"type\"])\n",
    "\n",
    "transformed_datatset = pd.concat([fraud_dataset, type_categ_transformed, nameOrig_categ_transformed, nameDest_categ_transformed], axis = 1)\n",
    "\n",
    "# remove unnecessary fields from the dataset and save the labels\n",
    "drop_labels = [\"type\", \"nameOrig\", \"nameDest\", \"isFlaggedFraud\"]\n",
    "for label in drop_labels:\n",
    "    transformed_datatset.pop(label)\n",
    "\n",
    "print(\"This is the transformed_datatset\")\n",
    "print(transformed_datatset.shape)\n",
    "print(transformed_datatset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 18)\n",
      "X_train:  (5090096, 17)\n",
      "X_train:  (4072076, 17)\n",
      "X_val:  (1018020, 17)\n",
      "X_test:  (1272524, 18)\n",
      "X_test:  (1272524, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(transformed_datatset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [16, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"tanh\")(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"tanh\")(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"tanh\")(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"tanh\")(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"tanh\")(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"tanh\")(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                288       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 17)                153       \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475994504704.0000 - acc: 0.4989 - val_loss: 180751105717768992.0000 - val_acc: 0.5005\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475824616128.0000 - acc: 0.5001 - val_loss: 180751105717768992.0000 - val_acc: 0.5005\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475433351104.0000 - acc: 0.5001 - val_loss: 180751105717768992.0000 - val_acc: 0.5005\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475514321120.0000 - acc: 0.4953 - val_loss: 180751105717768992.0000 - val_acc: 0.5005\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475930275424.0000 - acc: 0.1579 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475875614752.0000 - acc: 0.0305 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475799791872.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475261673760.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475693052544.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475785244960.0000 - acc: 2.4557e-07 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475776722656.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475906885600.0000 - acc: 2.4557e-07 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475702131712.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475623389280.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475846369056.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475361645888.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475632080320.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475959926208.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475498238496.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475687618528.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475659570976.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475956652288.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475906514336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475776570784.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475719851296.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476073162688.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475872492736.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475623423040.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475949361952.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475697187104.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475813140640.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475563851552.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476080739936.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475562518336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475685593440.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475741739168.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476099067040.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475653647584.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476031985792.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475660887296.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 9.8230e-07\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475888744160.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475709270176.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475773921312.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475692478784.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475824970560.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475685492192.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475590430912.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475591713472.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475884980864.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475542048000.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475842774496.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475760066272.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475768335424.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475570568096.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475472654784.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476015734432.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475678708128.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475810389856.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475501664288.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475894313152.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475553169184.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475963908864.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475615440800.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475735680768.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475918580544.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475892507456.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475792535296.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475833847232.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874476000984960.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475351756672.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475489648704.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475605011584.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475604285920.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475397574432.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475970287936.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475624924992.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475614327008.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475874416608.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476062784096.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475631742816.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475840327488.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475598109376.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475887461568.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475407733632.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475862012896.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475733841312.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476032407680.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475948433760.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475488788032.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475910733280.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475812144928.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475952973376.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475733790688.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475494171424.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475680547584.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475746464384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475617550272.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475409134336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475549794016.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475806930336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475790982720.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475699077184.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475613753216.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475605889120.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475842656384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475746633152.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475775575136.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475853507520.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475928469728.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475626038816.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475498120384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475657731520.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475539550400.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476045469536.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475953007104.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475721083232.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475429739680.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475635843616.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476012561760.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475470950336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475758530592.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475301483712.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475646458528.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475647994176.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475672902848.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475830792704.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475720492544.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475643505248.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475807723488.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475693288800.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475567581088.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475788265728.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475724171488.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475671704672.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475756640512.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475735140768.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475937987680.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475682032672.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475689272384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475653866976.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475575276448.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475569774944.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475868729440.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475571411872.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476184509120.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475774495072.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475586718240.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476077719168.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475861894752.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475762108256.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475483910912.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475643690880.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475720627552.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475636704288.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475860831584.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475592574112.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475834252224.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475844310208.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475757399904.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475967739680.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475337361600.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475703194912.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475646610400.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475950053856.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475732322496.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475808111648.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475823772384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475628300160.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475638324384.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475686116608.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475985054240.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475845744640.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475859430880.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475526201632.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475915964768.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475735630144.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475529391168.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475533019456.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475498052832.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475937329504.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475545946336.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475846942816.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475635641120.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476011296064.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874476116398496.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475823114240.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475619406624.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475944518592.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475698756544.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475525324128.0000 - acc: 2.4557e-07 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475551515328.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475762310752.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476049604128.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874476141357760.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475944248576.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475865725568.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874476025269216.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 180874475614917664.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475957529856.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 180874475836260448.0000 - acc: 0.0000e+00 - val_loss: 180751105717768992.0000 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1e2875e1d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xuc1XW97/HXWxlFYECEkbhIUKe85GXUyby0O5rJxbyVhlm2kTpRHmtbJw3pRkrnbPdOK4uSbEdYGUleEksTbGu4d5kOxE3UQAEZULl5Vwzwc/74fUcX48wwA/Ndi4H38/FYj1nz/X1/v/X5/WbNvOf7uy1FBGZmZh1tj0oXYGZmuyYHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhizCpA0VdK32th3uaQP7OhyzMrNAWNmZlk4YMzMLAsHjFkL0q6pSyUtkPSSpJ9K6ifpTkkvSLpbUu+S/mdIekjSs5LulXRwybQjJc1N890IdG3yWqdJmpfm/bOkw7ez5k9LWippg6QZkgakdkn6rqQ1kp5L63RomnaqpMWptlWSLtmuDWbWhAPGrHVnA6cA7wROB+4EvgL0pfj9+RcASe8EpgFfAGqAO4DbJe0laS/gt8AvgP2A36TlkuY9CpgCfAboA/wYmCFp7/YUKun9wL8Co4D+wArg12nyMOB9aT32Bc4F1qdpPwU+ExHVwKHAf7bndc1astsHjKQp6b+6RW3o+770X+hmSeeUtJ+U/vtsfGyUdFbeyq1MfhART0fEKuA+4K8R8beIeBW4FTgy9TsX+H1EzIqITcBVwD7A8cCxQBXwvYjYFBE3AQ+WvMangR9HxF8jYktEXA+8muZrj48DUyJibqpvPHCcpCHAJqAaOAhQRDwcEU+m+TYBh0jqGRHPRMTcdr6uWbN2+4ABpgIj2tj3CeAC4FeljRFxT0TURkQt8H7gZWBmB9ZolfN0yfNXmvm+R3o+gGLEAEBEvAasBAamaati6zvLrih5/lbgS2n32LOSngUOSPO1R9MaXqQYpQyMiP8EJgE/BJ6WdJ2knqnr2cCpwApJf5J0XDtf16xZu33ARMRsYENpm6S3S/qDpDmS7pN0UOq7PCIWAK+1sshzgDsj4uV8VdtOaDVFUADFMQ+KkFgFPAkMTG2NBpc8Xwn834jYt+TRLSKm7WAN3Sl2ua0CiIjvR8TRwLsodpVdmtofjIgzgf0pduVNb+frmjVrtw+YFlwHfD79Ml4C/Kgd836UYl+87V6mAx+UdLKkKuBLFLu5/gz8BdgM/IukLpI+DBxTMu9PgM9Kek86GN9d0gclVbezhl8BYyTVpuM3/49il95ySe9Oy68CXgI2AlvSMaKPS+qVdu09D2zZge1g9joHTBOSelDsN/+NpHkUB1z7t3He/sBhwF35KrSdUUQ8CpwP/ABYR3FCwOkR8Y+I+AfwYYrdq89QHK+5pWTeeorjMJPS9KWpb3tr+CPwdeBmilHT2yn+4QHoSRFkz1DsRltPcZwI4BPAcknPA59N62G2w+QPHIN0EPR3EXFo2i/9aES0GCqSpqb+NzVpvxh4V0SMzViumVmn4BFMExHxPLBM0kfg9esHjmjj7Ofh3WNmZoBHMEiaBpxIcV3D08AEiusArqXYNVYF/DoirpD0bopTU3tT7MN+KiLelZYzBPhv4IB0BpGZ2W4ta8BImgKcBqyJiEObmd4L+CXFGTVdgKsi4mdp2mjga6nrt9K1AUg6j+JCt6A4a+b8iFgnqRaYTHGF9Gbgf0fEA9lWzszMWpU7YN4HvAj8vIWA+QrQKyLGSaoBHgXeQnFtQT1QRxEkc4CjgRcoQuWQFCr/DrwcEd+UNBP4bkTcKelU4MsRcWK2lTMzs1Z1ybnwiJiddh212AWoTtcH9KC4HmUzMByYFREbACTNorgY8iZAQHdJ6ynOjFlasqzGC8d6UQRRq/r27RtDhrRWnpmZNTVnzpx1EVGzrX5ZA6YNJgEzKMKgGjg3Il6TNJDi4rNGDRRXI2+SdCGwkOJc/iXARanPF4C7JF1FcfLC8c29oKSxwFiAwYMHU19f3/FrZWa2C5O0Ytu9Kn8W2XBgHsUtLmqBSek0YTXTN9JFYhdS3P9pALCA4n5LpPYvRsQBwBcpbuD35oVEXBcRdRFRV1OzzQA2M7PtVOmAGQPcEoWlwDKKm/E1UNxmo9EgilFOLUBEPJbu6zSdN0Yqo3nj4rXfsPWV0mZmVmaVDpgngJMBJPUDDgQep7gSfpik3io+b2NYaltFcdfXxqHHKcDD6flq4H+m5++n2H1mZmYVkvUYTOk1JpIaKK4xqQKIiMnARGCqpIUUu8XGRcS6NO9E3ril+RUlB/wvB2ZL2kRxy4sLUp9PA9dI6kJxjYqvpjezLDZt2kRDQwMbN26sdClZde3alUGDBlFVVbVd8+/WF1rW1dWFD/KbWXstW7aM6upq+vTpw9Y3yd51RATr16/nhRdeYOjQoVtNkzQnIuq2tYxK7yIzM+t0Nm7cuEuHC4Ak+vTps0OjNAeMmdl22JXDpdGOrmOlr4PplB5cvoH7/r620mWYWYWc0HcTTz3XuY+/9NynC932yhsBDpjtMHfFM/zgnqXb7mhmu6TDT+/PmhcqFzDPP/ccd/72N5w7+n+1a76L/vkj/OsP/oOevXpRtec+2QPGB/l9kN/M2unhhx/m4IMPrtjrL1++nNNOO41FixZt1b5lyxb23HPPDn2t5ta1rQf5PYIxM+tkLrvsMh577DFqa2upqqqiR48e9O/fn3nz5rF48WLOOussVq5cycaNG7n44osZO7a4amPIkCHU19fz4osvMnLkSN773vfy5z//mYEDB3Lbbbexzz77dGidDhgzsx1w+e0PsXj18x26zEMG9GTC6e9qcfqVV17JokWLmDdvHvfeey8f/OAHWbRo0eunE0+ZMoX99tuPV155hXe/+92cffbZ9OnTZ6tlLFmyhGnTpvGTn/yEUaNGcfPNN3P++R37adkOGDOzTu6YY47Z6lqV73//+9x6660ArFy5kiVLlrwpYIYOHUptbS0ARx99NMuXL+/wuhwwZmY7oLWRRrl079799ef33nsvd999N3/5y1/o1q0bJ554YrPXsuy9996vP99zzz155ZVXOrwuXwdjZtbJVFdX88ILLzQ77bnnnqN3795069aNRx55hPvvv7/M1b3BIxgzs06mT58+nHDCCRx66KHss88+9OvX7/VpI0aMYPLkyRx++OEceOCBHHvssRWr06cp+zRlM2unSp+mXE47cpqyd5GZmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmadzLPPPsuPfvSj7Zr3e9/7Hi+//HIHV9Q8B4yZWSfTWQLGV/KbmXUypbfrP+WUU9h///2ZPn06r776Kh/60Ie4/PLLeemllxg1ahQNDQ1s2bKFr3/96zz99NOsXr2ak046ib59+3LPPfdkrdMBY2a2I+68DJ5a2LHLfMthMPLKFieX3q5/5syZ3HTTTTzwwANEBGeccQazZ89m7dq1DBgwgN///vdAcY+yXr168Z3vfId77rmHvn37dmzNzfAuMjOzTmzmzJnMnDmTI488kqOOOopHHnmEJUuWcNhhh3H33Xczbtw47rvvPnr16lX22rKNYCRNAU4D1kTEoc1M7wX8Ehic6rgqIn6Wpo0Gvpa6fisirk/t5wFfAQJYDZwfEevStM8DnwM2A7+PiC/nWjczs9e1MtIoh4hg/PjxfOYzn3nTtDlz5nDHHXcwfvx4hg0bxje+8Y2y1pZzBDMVGNHK9IuAxRFxBHAicLWkvSTtB0wA3gMcA0yQ1FtSF+Aa4KSIOBxYQBEoSDoJOBM4PCLeBVyVZ5XMzCqv9Hb9w4cPZ8qUKbz44osArFq1ijVr1rB69Wq6devG+eefzyWXXMLcuXPfNG9u2UYwETFb0pDWugDVkgT0ADZQjD6GA7MiYgOApFkUQXUTIKC7pPVAT2BpWtaFwJUR8Wp67TUdvkJmZjuJ0tv1jxw5ko997GMcd9xxAPTo0YNf/vKXLF26lEsvvZQ99tiDqqoqrr32WgDGjh3LyJEj6d+/f/aD/Flv158C5nct7CKrBmYABwHVwLkR8XtJlwBdI+Jbqd/XgVci4ipJ5wBTgJeAJRSjmS2S5gG3UQTRRuCSiHiwhZrGAmMBBg8efPSKFSs6cpXNbDfg2/Xv/LfrHw7MAwYAtcAkST0pRilNhaQqipHKkWmeBcD4NL0L0Bs4FrgUmJ5GRm9eUMR1EVEXEXU1NTUduT5mZlaikgEzBrglCkuBZRSjmQbggJJ+gygO6NcCRMRjUQy7pgPHpz4NJct6AHgNyH8OnpmZtaiSAfMEcDKApH7AgcDjwF3AsHRgvzcwLLWtAg6R1DjsOAV4OD3/LfD+tKx3AnsB68q0Hma2G9odPg14R9cx52nK0yjODusrqYHizLAqgIiYDEwEpkpaSLFbbFzJKccTgcZjKFeUHPC/HJgtaROwArgg9ZkCTJG0CPgHMDp2h5++mVVE165dWb9+PX369KGFvfGdXkSwfv16unbtut3LyHqQf2dXV1cX9fX1lS7DzDqZTZs20dDQwMaNGytdSlZdu3Zl0KBBVFVVbdXe1oP8vlWMmVk7VVVVMXTo0EqXsdPzrWLMzCwLB4yZmWXhgDEzsywcMGZmloUDxszMsnDAmJlZFg4YMzPLwgFjZmZZOGDMzCwLB4yZmWXhgDEzsywcMGZmloUDxszMsnDAmJlZFg4YMzPLwgFjZmZZOGDMzCwLB4yZmWXhgDEzsywcMGZmloUDxszMsnDAmJlZFlkDRtIUSWskLWphei9Jt0uaL+khSWNKpo2WtCQ9Rpe0nydpoaQFkv4gqW+TZV4iKZq2m5lZeeUewUwFRrQy/SJgcUQcAZwIXC1pL0n7AROA9wDHABMk9ZbUBbgGOCkiDgcWAJ9rXJikA4BTgCcyrIuZmbVD1oCJiNnAhta6ANWSBPRIfTcDw4FZEbEhIp4BZlEEldKje5qnJ7C6ZHnfBb6clmtmZhXUpcKvPwmYQRES1cC5EfGapIHAypJ+DcDAiNgk6UJgIfASsIRiFISkM4BVETG/yJ7mSRoLjAUYPHhwx6+RmZkBlT/IPxyYBwwAaoFJknpSjFKaCklVwIXAkWmeBcB4Sd2ArwLf2NYLRsR1EVEXEXU1NTUdtBpmZtZUpQNmDHBLFJYCy4CDKEYsB5T0G0QxyqkFiIjHIiKA6cDxwNuBocB8SctT/7mS3lKuFTEzs61VOmCeAE4GkNQPOBB4HLgLGJYO7PcGhqW2VcAhkhqHHqcAD0fEwojYPyKGRMQQioA6KiKeKu/qmJlZo6zHYCRNozg7rK+kBoozw6oAImIyMBGYKmkhxW6xcRGxLs07EXgwLeqKiNiQ2i8HZkvaBKwALsi5DmZmtn1U7GnaPdXV1UV9fX2lyzAz61QkzYmIum31q/QuMjMz20U5YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkW2QJG0hRJayQtamF6L0m3S5ov6SFJY0qmjZa0JD1Gl7SfJ2mhpAWS/iCpb2r/tqRHUvutkvbNtV5mZtY2OUcwU4ERrUy/CFgcEUcAJwJXS9pL0n7ABOA9wDHABEm9JXUBrgFOiojDgQXA59KyZgGHpva/A+MzrI+ZmbVDtoCJiNnAhta6ANWSBPRIfTcDw4FZEbEhIp6hCI8RgNKje5qnJ7A6vdbMiNiclns/MCjDKpmZWTt0qeBrTwJmUIRENXBuRLwmaSCwsqRfAzAwIjZJuhBYCLwELKEYBTX1SeDGrJWbmdk2VfIg/3BgHjAAqAUmSepJMUppKiRVARcCR6Z5FtBkV5ikr1KMgm5o6UUljZVUL6l+7dq1HbIiZmb2ZpUMmDHALVFYCiwDDqIYsRxQ0m8QxSinFiAiHouIAKYDxzd2SicDnAZ8PE1vVkRcFxF1EVFXU1PT0etkZmZJJQPmCeBkAEn9gAOBx4G7gGHpwH5vYFhqWwUcIqkxFU4BHk7zjwDGAWdExMtlXQszM2tWtmMwkqZRnB3WV1IDxZlhVQARMRmYCEyVtJBit9i4iFiX5p0IPJgWdUVEbEjtlwOzJW0CVgAXpD6TgL2BWcXxf+6PiM/mWjczM9s2tbI3aZdXV1cX9fX1lS7DzKxTkTQnIuq21a9Nu8gkXSyppwo/lTRX0rAdL9PMzHZVbT0G88mIeJ7ieEgNxQH6K7NVZWZmnV5bA6bx1OFTgZ9FxHyaP53YzMwMaHvAzJE0kyJg7pJUDbyWrywzM+vs2noW2acorkN5PCJeTvcLG7ONeczMbDfW1hHMccCjEfGspPOBrwHP5SvLzMw6u7YGzLXAy5KOAL5McQ3Kz7NVZWZmnV5bA2Zzuv3KmcA1EXENxQ0qzczMmtXWYzAvSBoPfAL4J0l7kq7KNzMza05bRzDnAq9SXA/zFDAQ+Ha2qszMrNNrU8CkULkB6CXpNGBjRPgYjJmZtaitt4oZBTwAfAQYBfxV0jk5CzMzs86trcdgvgq8OyLWAKRb5t8N3JSrMDMz69zaegxmj8ZwSda3Y14zM9sNtXUE8wdJdwHT0vfnAnfkKcnMzHYFbQqYiLhU0tnACRQ3ubwuIm7NWpmZmXVqbf5Ey4i4Gbg5Yy1mZrYLaTVgJL0ANPeRlwIiInpmqcrMzDq9VgMmInw7GDMz2y4+E8zMzLJwwJiZWRYOGDMzy8IBY2ZmWWQNGElTJK2RtKiF6b0k3S5pvqSHJI0pmTZa0pL0GF3Sfp6khZIWSPqDpL6pfT9Js1L/WZJ651w3MzNrXe4RzFRgRCvTLwIWR8QRwInA1ZL2krQfMAF4D3AMMEFSb0ldgGuAkyLicGAB8Lm0rMuAP0bEO4A/pu/NzKxCsgZMRMwGNrTWBaiWJKBH6rsZGA7MiogNEfEMMIsiqJQe3dM8PYHVaVlnAten59cDZ3Xw6piZWTu0+Ur+TCYBMyhCoho4NyJekzQQWFnSrwEYGBGbJF0ILAReApZQjIIA+kXEkwAR8aSk/cu1EmZm9maVPsg/HJgHDABqgUmSelKMUpoKSVXAhcCRaZ4FwPj2vKCksZLqJdWvXbt2h4o3M7OWVTpgxgC3RGEpsAw4iGLEckBJv0EUo5xagIh4LCICmA4cn/o8Lak/QPpa+vECr4uI6yKiLiLqampqcqyTmZlR+YB5AjgZQFI/4EDgceAuYFg6sN8bGJbaVgGHpA88AzgFeDg9nwE0nm02GritLGtgZmbNynoMRtI0irPD+kpqoDgzrAogIiYDE4GpkhZS7BYbFxHr0rwTgQfToq6IiA2p/XJgtqRNwArggtTnSmC6pE9RBNdHcq6bmZm1TsWept1TXV1d1NfXV7oMM7NORdKciKjbVr9K7yIzM7NdlAPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZZEtYCRNkbRG0qIWpveSdLuk+ZIekjSmZNpoSUvSY3Rqq5Y0r+SxTtL30rTBku6R9DdJCySdmmu9zMysbbpkXPZUYBLw8xamXwQsjojTJdUAj0q6AegBTADqgADmSJoREc8AtY0zS5oD3JK+/RowPSKulXQIcAcwpONXyczM2irbCCYiZgMbWusCVEsSRahsADYDw4FZEbEhhcosYETpjJLeAewP3FeyrJ7peS9gdUeth5mZbZ+cI5htmQTMoAiDauDciHhN0kBgZUm/BmBgk3nPA26MiEjffxOYKenzQHfgAzkLNzOzbavkQf7hwDxgAMWur0mSegJqpm80+f6jwLSS788DpkbEIOBU4BeSml03SWMl1UuqX7t27Y6ug5mZtaCSATMGuCUKS4FlwEEUI5YDSvoNomSXl6QjgC4RMaekz6eA6QAR8RegK9C3uReNiOsioi4i6mpqajpyfczMrEQlA+YJ4GQASf2AA4HHgbuAYZJ6S+oNDEttjc5j69FL02UdTBEwHp6YmVVQtmMwkqYBJwJ9JTVQnBlWBRARk4GJwFRJCyl2i42LiHVp3onAg2lRV0RE6ckCoyh2g5X6EvATSV+k2J12QcnxGTMzqwDtzn+H6+rqor6+vtJlmJl1KpLmRETdtvr5Sn4zM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkWDhgzM8vCAWNmZlk4YMzMLAsHjJmZZeGAMTOzLBwwZmaWhQPGzMyycMCYmVkW2QJG0hRJayQtamF6L0m3S5ov6SFJY0qmjZa0JD1Gp7ZqSfNKHuskfa9knlGSFqdl/SrXepmZWdt0ybjsqcAk4OctTL8IWBwRp0uqAR6VdAPQA5gA1AEBzJE0IyKeAWobZ5Y0B7glPX8HMB44ISKekbR/pnUyM7M2yjaCiYjZwIbWugDVkkQRKhuAzcBwYFZEbEihMgsYUTpjCpT9gftS06eBH6b+RMSajlwXMzNrv0oeg5kEHAysBhYCF0fEa8BAYGVJv4bUVuo84MaIiPT9O4F3SvpvSfdLGkELJI2VVC+pfu3atR21LmZm1kQlA2Y4MA8YQLHra5KknoCa6RtNvv8oMK3k+y7AO4ATKcLnPyTt29yLRsR1EVEXEXU1NTU7tgZmZtaiSgbMGOCWKCwFlgEHUYxYDijpN4hilAOApCOALhExp6RPA3BbRGyKiGXAoxSBY2ZmFVLJgHkCOBlAUj/gQOBx4C5gmKTeknoDw1Jbo/PYevQC8FvgpLSsvhS7zB7PWr2ZmbUq21lkkqZR7LLqK6mB4sywKoCImAxMBKZKWkixW2xcRKxL804EHkyLuiIiSk8WGAWc2uTlGkNpMbAFuDQi1mdZMTMzaxO9cZx891NXVxf19fWVLsPMrFORNCci6rbVz1fym5lZFg4YMzPLwgFjZmZZOGDMzCwLB4yZmWXhgDEzsywcMGZmlkXO2/Xvuu68DJ5aWOkqzMy231sOg5FXZn0Jj2DMzCwLj2C2R+bUNzPbFXgEY2ZmWThgzMwsCweMmZll4YAxM7MsHDBmZpaFA8bMzLJwwJiZWRYOGDMzy2K3/shkSWuBFds5e19gXQeW05F21tpcV/u4rvbbWWvb1ep6a0TUbKvTbh0wO0JSfVs+k7oSdtbaXFf7uK7221lr213r8i4yMzPLwgFjZmZZOGC233WVLqAVO2ttrqt9XFf77ay17ZZ1+RiMmZll4RGMmZll4YAxM7MsHDDbQdIISY9KWirpsgrWcYCkeyQ9LOkhSRen9m9KWiVpXnqcWoHalktamF6/PrXtJ2mWpCXpa+8y13RgyTaZJ+l5SV+o1PaSNEXSGkmLStqa3UYqfD+95xZIOqrMdX1b0iPptW+VtG9qHyLplZJtN7nMdbX4s5M0Pm2vRyUNz1VXK7XdWFLXcknzUntZtlkrfx/K9x6LCD/a8QD2BB4D3gbsBcwHDqlQLf2Bo9LzauDvwCHAN4FLKrydlgN9m7T9O3BZen4Z8G8V/jk+Bby1UtsLeB9wFLBoW9sIOBW4ExBwLPDXMtc1DOiSnv9bSV1DSvtVYHs1+7NLvwfzgb2Boel3ds9y1tZk+tXAN8q5zVr5+1C295hHMO13DLA0Ih6PiH8AvwbOrEQhEfFkRMxNz18AHgYGVqKWNjoTuD49vx44q4K1nAw8FhHbeyeHHRYRs4ENTZpb2kZnAj+Pwv3AvpL6l6uuiJgZEZvTt/cDg3K8dnvrasWZwK8j4tWIWAYspfjdLXttkgSMAqblev0Wamrp70PZ3mMOmPYbCKws+b6BneCPuqQhwJHAX1PT59Iwd0q5d0UlAcyUNEfS2NTWLyKehOLND+xfgboafZStf+Ervb0atbSNdqb33Scp/tNtNFTS3yT9SdI/VaCe5n52O9P2+ifg6YhYUtJW1m3W5O9D2d5jDpj2UzNtFT3XW1IP4GbgCxHxPHAt8HagFniSYnhebidExFHASOAiSe+rQA3NkrQXcAbwm9S0M2yvbdkp3neSvgpsBm5ITU8CgyPiSOD/AL+S1LOMJbX0s9sptldyHlv/M1PWbdbM34cWuzbTtkPbzAHTfg3AASXfDwJWV6gWJFVRvHluiIhbACLi6YjYEhGvAT8h466BlkTE6vR1DXBrquHpxiF3+rqm3HUlI4G5EfF0qrHi26tES9uo4u87SaOB04CPR9ppn3ZBrU/P51Ac63hnuWpq5WdX8e0FIKkL8GHgxsa2cm6z5v4+UMb3mAOm/R4E3iFpaPpP+KPAjEoUkvbt/hR4OCK+U9Jeut/0Q8CipvNmrqu7pOrG5xQHiBdRbKfRqdto4LZy1lViq/8oK729mmhpG80A/jmd6XMs8Fzjbo5ykDQCGAecEREvl7TXSNozPX8b8A7g8TLW1dLPbgbwUUl7Sxqa6nqgXHWV+ADwSEQ0NDaUa5u19PeBcr7Hcp/JsCs+KM62+DvFfx5frWAd76UYwi4A5qXHqcAvgIWpfQbQv8x1vY3iDJ75wEON2wjoA/wRWJK+7leBbdYNWA/0KmmryPaiCLkngU0U/z1+qqVtRLH74ofpPbcQqCtzXUsp9s83vs8mp75np5/xfGAucHqZ62rxZwd8NW2vR4GR5f5ZpvapwGeb9C3LNmvl70PZ3mO+VYyZmWXhXWRmZpaFA8bMzLJwwJiZWRYOGDMzy8IBY2ZmWThgzDopSSdK+l2l6zBriQPGzMyycMCYZSbpfEkPpM/++LGkPSW9KOlqSXMl/VFSTepbK+l+vfG5K42f1fE/JN0taX6a5+1p8T0k3aTis1puSFdvm+0UHDBmGUk6GDiX4uaftcAW4ONAd4ouRuVQAAABRklEQVT7oR0F/AmYkGb5OTAuIg6nuJq6sf0G4IcRcQRwPMVV41DcIfcLFJ/z8TbghOwrZdZGXSpdgNku7mTgaODBNLjYh+Lmgq/xxg0QfwncIqkXsG9E/Cm1Xw/8Jt3XbWBE3AoQERsB0vIeiHSfKxWfmDgE+K/8q2W2bQ4Ys7wEXB8R47dqlL7epF9r92xqbbfXqyXPt+DfaduJeBeZWV5/BM6RtD+8/nnob6X43Tsn9fkY8F8R8RzwTMkHUH0C+FMUn+HRIOmstIy9JXUr61qYbQf/t2OWUUQslvQ1ik/33IPibrsXAS8B75I0B3iO4jgNFLdPn5wC5HFgTGr/BPBjSVekZXykjKthtl18N2WzCpD0YkT0qHQdZjl5F5mZmWXhEYyZmWXhEYyZmWXhgDEzsywcMGZmloUDxszMsnDAmJlZFv8fkBJi0TG+3Q4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.272524e+06</td>\n",
       "      <td>1.272524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.809449e+17</td>\n",
       "      <td>1.273060e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.143650e+17</td>\n",
       "      <td>3.565727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.306247e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.644901e+16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.729621e+17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.591126e+17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.421040e+17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error    true_class\n",
       "count          1.272524e+06  1.272524e+06\n",
       "mean           1.809449e+17  1.273060e-03\n",
       "std            1.143650e+17  3.565727e-02\n",
       "min            7.306247e+11  0.000000e+00\n",
       "25%            8.644901e+16  0.000000e+00\n",
       "50%            1.729621e+17  0.000000e+00\n",
       "75%            2.591126e+17  0.000000e+00\n",
       "max            5.421040e+17  1.000000e+00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
