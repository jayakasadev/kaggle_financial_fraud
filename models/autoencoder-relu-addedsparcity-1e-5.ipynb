{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 23:50:59 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 28%   53C    P2    62W / 200W |   1080MiB /  8118MiB |     50%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           389MiB |\r\n",
      "|    0      2077      G   compiz                                       190MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    73MiB |\r\n",
      "|    0     19984      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     22421      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 949422069809.2938 - acc: 0.6024 - val_loss: 832017021071.6791 - val_acc: 0.5917\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 823434083600.9252 - acc: 0.5918 - val_loss: 831591466078.3711 - val_acc: 0.5999\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 823008410457.7842 - acc: 0.6065 - val_loss: 831880249963.3771 - val_acc: 0.6099\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 822095402871.5209 - acc: 0.6400 - val_loss: 830998287108.9137 - val_acc: 0.6590\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 638069995089.3951 - acc: 0.6770 - val_loss: 6818199633.1740 - val_acc: 0.7663\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 5359647897.5799 - acc: 0.8519 - val_loss: 4959489953.3070 - val_acc: 0.7792\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4756035509.9155 - acc: 0.8374 - val_loss: 3956086088.3978 - val_acc: 0.8288\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 4524352288.6857 - acc: 0.8467 - val_loss: 3862775106.2318 - val_acc: 0.8448\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4481409817.7535 - acc: 0.8416 - val_loss: 3887128057.8642 - val_acc: 0.8153\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 4568853502.3207 - acc: 0.8330 - val_loss: 3824083699.5171 - val_acc: 0.8241\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4491800040.9334 - acc: 0.8278 - val_loss: 3708448009.0830 - val_acc: 0.8334\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4389982115.8944 - acc: 0.8301 - val_loss: 3739664742.0057 - val_acc: 0.8273\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4292052459.0944 - acc: 0.8234 - val_loss: 4096243268.9728 - val_acc: 0.8471\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4049810760.4691 - acc: 0.8340 - val_loss: 3252958491.2542 - val_acc: 0.8416\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 3329477954.5464 - acc: 0.8354 - val_loss: 2401021777.2344 - val_acc: 0.8477\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2645965448.8664 - acc: 0.8496 - val_loss: 1971475820.2371 - val_acc: 0.8433\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 2176639337.5721 - acc: 0.8684 - val_loss: 1793726104.3547 - val_acc: 0.8689\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 2064946054.6281 - acc: 0.8660 - val_loss: 2087884877.0148 - val_acc: 0.8477\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1893911294.5518 - acc: 0.8487 - val_loss: 1626379080.6643 - val_acc: 0.8149\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1871922419.5064 - acc: 0.8425 - val_loss: 1588052728.3654 - val_acc: 0.8434\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1817247841.8206 - acc: 0.8672 - val_loss: 1569514848.7475 - val_acc: 0.8588\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1859858050.5943 - acc: 0.8652 - val_loss: 1558641597.2326 - val_acc: 0.8333\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1797612882.2958 - acc: 0.8658 - val_loss: 1548628783.9978 - val_acc: 0.8461\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1743569928.3725 - acc: 0.8796 - val_loss: 1537274632.1727 - val_acc: 0.9092\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1822651681.8653 - acc: 0.8872 - val_loss: 1562496215.8279 - val_acc: 0.8806\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1776229624.6261 - acc: 0.8978 - val_loss: 1529757355.5946 - val_acc: 0.8962\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1795729621.9354 - acc: 0.9081 - val_loss: 1517563680.1477 - val_acc: 0.9050\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 2031268247.2695 - acc: 0.9284 - val_loss: 1518151315.4561 - val_acc: 0.9371\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1791600471.9468 - acc: 0.9264 - val_loss: 1531641771.0992 - val_acc: 0.9194\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1695345195.8465 - acc: 0.9275 - val_loss: 1518957408.4985 - val_acc: 0.9177\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1827519686.0083 - acc: 0.9283 - val_loss: 1526072920.4314 - val_acc: 0.9416\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1804024192.7283 - acc: 0.9319 - val_loss: 1508183478.5108 - val_acc: 0.9312\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1744135444.7381 - acc: 0.9312 - val_loss: 1503843350.8359 - val_acc: 0.9332\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1691466920.1154 - acc: 0.9342 - val_loss: 1528419545.8070 - val_acc: 0.9363\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1801696479.0556 - acc: 0.9338 - val_loss: 1504285145.1179 - val_acc: 0.9297\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1700385150.7573 - acc: 0.9377 - val_loss: 1647481915.2007 - val_acc: 0.9286\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1876459529.8092 - acc: 0.9374 - val_loss: 1534511074.9856 - val_acc: 0.9650\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1745840168.8336 - acc: 0.9440 - val_loss: 1512632670.2303 - val_acc: 0.9484\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1773399432.8901 - acc: 0.9420 - val_loss: 1519245390.4406 - val_acc: 0.9763\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1779831530.1254 - acc: 0.9449 - val_loss: 1543721809.1766 - val_acc: 0.9631\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1699582234.8737 - acc: 0.9491 - val_loss: 1497840876.3955 - val_acc: 0.9511\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1678445008.1071 - acc: 0.9500 - val_loss: 1617062227.1053 - val_acc: 0.9469\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1703275505.2702 - acc: 0.9509 - val_loss: 1502486898.6973 - val_acc: 0.9606\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1747055263.9641 - acc: 0.9473 - val_loss: 1501801316.1700 - val_acc: 0.9516\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1735810467.8383 - acc: 0.9523 - val_loss: 1579257647.4496 - val_acc: 0.9518\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1667881406.8940 - acc: 0.9540 - val_loss: 1512441088.0905 - val_acc: 0.9725\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1762429184.0541 - acc: 0.9531 - val_loss: 1491188363.0621 - val_acc: 0.9424\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1645582852.4533 - acc: 0.9554 - val_loss: 1523945097.1736 - val_acc: 0.9364\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1664885981.9856 - acc: 0.9571 - val_loss: 1632827441.7153 - val_acc: 0.9556\n",
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1658561818.0620 - acc: 0.9616 - val_loss: 1546260367.4955 - val_acc: 0.9622\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1682336039.7733 - acc: 0.9589 - val_loss: 1486804735.0771 - val_acc: 0.9614\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1633765112.6245 - acc: 0.9573 - val_loss: 1491657051.6553 - val_acc: 0.9601\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1637915166.8497 - acc: 0.9565 - val_loss: 1501019042.6800 - val_acc: 0.9604\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1685360590.6521 - acc: 0.9565 - val_loss: 1887105694.0555 - val_acc: 0.9397\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1658561164.6779 - acc: 0.9559 - val_loss: 1493483752.9303 - val_acc: 0.9621\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1648383690.1304 - acc: 0.9561 - val_loss: 1486642622.1655 - val_acc: 0.9580\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1672362610.0764 - acc: 0.9527 - val_loss: 1522549027.1251 - val_acc: 0.9458\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1695768408.6942 - acc: 0.9542 - val_loss: 1577988584.6084 - val_acc: 0.9470\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1667496383.9681 - acc: 0.9536 - val_loss: 1501088901.6228 - val_acc: 0.9546\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1665846652.9615 - acc: 0.9534 - val_loss: 1627927534.9303 - val_acc: 0.9529\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1659700193.3553 - acc: 0.9518 - val_loss: 1533590342.2980 - val_acc: 0.9598\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1642037551.3726 - acc: 0.9526 - val_loss: 1561191887.4112 - val_acc: 0.9668\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1669807545.3654 - acc: 0.9560 - val_loss: 1543093893.4015 - val_acc: 0.9629\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1619687649.9164 - acc: 0.9528 - val_loss: 1582849614.9008 - val_acc: 0.9562\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1634361232.2676 - acc: 0.9527 - val_loss: 1725792125.3369 - val_acc: 0.9709\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1720844315.1259 - acc: 0.9529 - val_loss: 1477371448.6584 - val_acc: 0.9681\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1702227135.6446 - acc: 0.9521 - val_loss: 1483532290.9145 - val_acc: 0.9598\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1621888563.5964 - acc: 0.9548 - val_loss: 1554539650.3060 - val_acc: 0.9468\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1653530617.9612 - acc: 0.9510 - val_loss: 1477434809.6793 - val_acc: 0.9339\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1667347032.4239 - acc: 0.9536 - val_loss: 1480785316.8225 - val_acc: 0.9380\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1625129496.4457 - acc: 0.9526 - val_loss: 1490172020.3394 - val_acc: 0.9638\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1649786223.7466 - acc: 0.9508 - val_loss: 1824620817.8417 - val_acc: 0.9509\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1708462391.5330 - acc: 0.9525 - val_loss: 1521246670.4205 - val_acc: 0.9661\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1624560180.6215 - acc: 0.9533 - val_loss: 1508711770.0811 - val_acc: 0.9546\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1649657906.9388 - acc: 0.9533 - val_loss: 1728302508.8670 - val_acc: 0.9349\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1594745588.2417 - acc: 0.9542 - val_loss: 1512208461.4800 - val_acc: 0.9490\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1671491058.2652 - acc: 0.9501 - val_loss: 1495128780.7004 - val_acc: 0.9547\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1655397436.0780 - acc: 0.9546 - val_loss: 1481620572.8975 - val_acc: 0.9598\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1696118072.8433 - acc: 0.9495 - val_loss: 1497857171.4209 - val_acc: 0.9603\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1612659521.7684 - acc: 0.9549 - val_loss: 1470532078.2564 - val_acc: 0.9592\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1717280147.2029 - acc: 0.9497 - val_loss: 1611252682.1430 - val_acc: 0.9406\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1616148097.1742 - acc: 0.9546 - val_loss: 1472269530.8179 - val_acc: 0.9532\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1631851732.5596 - acc: 0.9506 - val_loss: 1547994923.3155 - val_acc: 0.9558\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1703831459.0252 - acc: 0.9488 - val_loss: 1464746398.2944 - val_acc: 0.9546\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1616415043.6249 - acc: 0.9517 - val_loss: 1466123023.8249 - val_acc: 0.9567\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1706574698.6553 - acc: 0.9518 - val_loss: 1495624236.0372 - val_acc: 0.9703\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1609288302.0209 - acc: 0.9526 - val_loss: 1471039948.9695 - val_acc: 0.9613\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1655950610.3117 - acc: 0.9503 - val_loss: 1478170956.9972 - val_acc: 0.9473\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1589696138.4333 - acc: 0.9520 - val_loss: 1531813114.6538 - val_acc: 0.9611\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1750034491.7038 - acc: 0.9473 - val_loss: 1829942664.9171 - val_acc: 0.9446\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1696075843.8875 - acc: 0.9496 - val_loss: 1568678089.4640 - val_acc: 0.9501\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1625154186.9694 - acc: 0.9503 - val_loss: 1479013726.9193 - val_acc: 0.9575\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1607210373.5366 - acc: 0.9484 - val_loss: 1474469783.5450 - val_acc: 0.9561\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1649536434.2193 - acc: 0.9508 - val_loss: 1694698406.4269 - val_acc: 0.8880\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1712448829.1927 - acc: 0.9486 - val_loss: 10649258820.0021 - val_acc: 0.9196\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1593058535.8034 - acc: 0.9531 - val_loss: 1660997688.6986 - val_acc: 0.9603\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1603398256.6093 - acc: 0.9536 - val_loss: 1533459547.5169 - val_acc: 0.9419\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1596283095.7059 - acc: 0.9518 - val_loss: 1785205322.8722 - val_acc: 0.9484\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1592251237.6624 - acc: 0.9472 - val_loss: 1493877249.2674 - val_acc: 0.9546\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1683751668.4942 - acc: 0.9510 - val_loss: 1463050131.5089 - val_acc: 0.9522\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1582801358.2572 - acc: 0.9523 - val_loss: 1524670569.6495 - val_acc: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1581390697.0251 - acc: 0.9519 - val_loss: 1627086081.8458 - val_acc: 0.9431\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1580527180.2591 - acc: 0.9485 - val_loss: 1496714015.7781 - val_acc: 0.9497\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1579853217.0844 - acc: 0.9488 - val_loss: 1496631550.2297 - val_acc: 0.9188\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1583746348.3947 - acc: 0.9516 - val_loss: 1550952351.5115 - val_acc: 0.9528\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1599530171.5047 - acc: 0.9550 - val_loss: 1485513526.8931 - val_acc: 0.9586\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1583873224.2363 - acc: 0.9528 - val_loss: 1454676240.1669 - val_acc: 0.9630\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1600600341.7275 - acc: 0.9513 - val_loss: 1475286631.4265 - val_acc: 0.9546\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1674255585.4801 - acc: 0.9512 - val_loss: 1458044923.6421 - val_acc: 0.9603\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1608185444.8846 - acc: 0.9515 - val_loss: 1506559749.9196 - val_acc: 0.9599\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1598031934.4777 - acc: 0.9521 - val_loss: 1563246014.2309 - val_acc: 0.9229\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1572173635.8961 - acc: 0.9543 - val_loss: 1655473786.0980 - val_acc: 0.9289\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1644109325.1871 - acc: 0.9484 - val_loss: 1699378291.6026 - val_acc: 0.9400\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1574187499.4799 - acc: 0.9456 - val_loss: 1492836324.7810 - val_acc: 0.9441\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1646553594.6839 - acc: 0.9475 - val_loss: 1610433018.4476 - val_acc: 0.9143\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1561731550.8049 - acc: 0.9468 - val_loss: 1497084600.2435 - val_acc: 0.9479\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1590465647.1693 - acc: 0.9514 - val_loss: 1465803870.6553 - val_acc: 0.9659\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1581286879.7625 - acc: 0.9486 - val_loss: 1460369610.1857 - val_acc: 0.9627\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1651662243.9602 - acc: 0.9514 - val_loss: 5773497732.8408 - val_acc: 0.8521\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1545170446.7097 - acc: 0.9505 - val_loss: 1485901401.4147 - val_acc: 0.9623\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1618790630.0885 - acc: 0.9514 - val_loss: 1456614234.8933 - val_acc: 0.9632\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1589747549.2255 - acc: 0.9542 - val_loss: 1451750652.2104 - val_acc: 0.9610\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1566315670.8419 - acc: 0.9500 - val_loss: 2634348483.2201 - val_acc: 0.9515\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1572861745.3184 - acc: 0.9503 - val_loss: 1506404682.4850 - val_acc: 0.9618\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1596444733.2875 - acc: 0.9522 - val_loss: 1486070826.1813 - val_acc: 0.9560\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1635163989.2147 - acc: 0.9504 - val_loss: 1474964009.0170 - val_acc: 0.9564\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1538194200.5190 - acc: 0.9532 - val_loss: 1461099170.1997 - val_acc: 0.9574\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1550175253.3980 - acc: 0.9521 - val_loss: 1507472030.3699 - val_acc: 0.9535\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1573249984.9878 - acc: 0.9527 - val_loss: 1467494540.9808 - val_acc: 0.9695\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1550651571.5926 - acc: 0.9524 - val_loss: 1451517789.5438 - val_acc: 0.9500\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1565753595.5808 - acc: 0.9525 - val_loss: 1558408743.3498 - val_acc: 0.9317\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1632740212.0872 - acc: 0.9576 - val_loss: 1459348714.1348 - val_acc: 0.9644\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1551081217.4339 - acc: 0.9592 - val_loss: 1623516150.5045 - val_acc: 0.9638\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1545846979.3423 - acc: 0.9600 - val_loss: 1457675462.6224 - val_acc: 0.9557\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1609003096.1326 - acc: 0.9540 - val_loss: 1470836633.9842 - val_acc: 0.9650\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1533317705.9785 - acc: 0.9574 - val_loss: 1457993596.5700 - val_acc: 0.9477\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1627931151.6502 - acc: 0.9548 - val_loss: 1466622537.6526 - val_acc: 0.9602\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1579922038.4467 - acc: 0.9532 - val_loss: 1438559975.5095 - val_acc: 0.9194\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1542785841.9471 - acc: 0.9528 - val_loss: 1762173485.0984 - val_acc: 0.9486\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1539635534.0369 - acc: 0.9513 - val_loss: 1476988030.7577 - val_acc: 0.9641\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1537283917.5151 - acc: 0.9543 - val_loss: 2164632594.8627 - val_acc: 0.9446\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1572925082.5012 - acc: 0.9472 - val_loss: 1440123794.7143 - val_acc: 0.9597\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1531202706.7934 - acc: 0.9501 - val_loss: 1930959840.3301 - val_acc: 0.9508\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1572359052.7408 - acc: 0.9489 - val_loss: 1438676359.7176 - val_acc: 0.9595\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1497875083.6722 - acc: 0.9414 - val_loss: 1419093185.7716 - val_acc: 0.9536\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1539385934.0068 - acc: 0.9466 - val_loss: 1420322915.2974 - val_acc: 0.9513\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1531931552.6888 - acc: 0.9471 - val_loss: 1430173920.4457 - val_acc: 0.9256\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1519914883.0966 - acc: 0.9383 - val_loss: 1413259234.6511 - val_acc: 0.9398\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1506933092.1048 - acc: 0.9453 - val_loss: 1402778204.0299 - val_acc: 0.9356\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1545020986.2037 - acc: 0.9349 - val_loss: 1394908131.7676 - val_acc: 0.9485\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1498641966.6434 - acc: 0.9431 - val_loss: 2811000424.8196 - val_acc: 0.9306\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1563296106.1763 - acc: 0.9305 - val_loss: 1561813811.5259 - val_acc: 0.9366\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1509229835.1584 - acc: 0.9382 - val_loss: 1428047419.6031 - val_acc: 0.9370\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1561417135.4276 - acc: 0.9392 - val_loss: 1389350939.8325 - val_acc: 0.9448\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1520532538.4051 - acc: 0.9376 - val_loss: 1390441083.4560 - val_acc: 0.9387\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1493126582.2287 - acc: 0.9355 - val_loss: 1389717854.1875 - val_acc: 0.9275\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1502864936.0608 - acc: 0.9350 - val_loss: 1396179683.9889 - val_acc: 0.9455\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1477624574.7679 - acc: 0.9374 - val_loss: 1460290366.1882 - val_acc: 0.9338\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1490952997.7932 - acc: 0.9350 - val_loss: 1407377506.1381 - val_acc: 0.9405\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1500420200.6770 - acc: 0.9343 - val_loss: 1380971660.3848 - val_acc: 0.9481\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1527519926.5902 - acc: 0.9355 - val_loss: 1400869755.6999 - val_acc: 0.9411\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1469784206.6186 - acc: 0.9297 - val_loss: 1407880132.2687 - val_acc: 0.9532\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1511854517.6368 - acc: 0.9356 - val_loss: 1378756387.7110 - val_acc: 0.9459\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1502556617.5188 - acc: 0.9332 - val_loss: 2445065332.0502 - val_acc: 0.9116\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1508994489.5030 - acc: 0.9337 - val_loss: 1539928071.0914 - val_acc: 0.9151\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1503544633.3308 - acc: 0.9361 - val_loss: 1388654008.1881 - val_acc: 0.9538\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1482023028.8738 - acc: 0.9346 - val_loss: 1404209705.6658 - val_acc: 0.9300\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1491563495.1310 - acc: 0.9304 - val_loss: 1594882693.1350 - val_acc: 0.9096\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1588217822.0442 - acc: 0.9273 - val_loss: 1373892480.6840 - val_acc: 0.9237\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1514722724.4060 - acc: 0.9259 - val_loss: 1380998314.8276 - val_acc: 0.9306\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1463845038.6053 - acc: 0.9304 - val_loss: 2063955270.7859 - val_acc: 0.9260\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1471895019.7040 - acc: 0.9279 - val_loss: 1385330818.1400 - val_acc: 0.9243\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1671650088.7970 - acc: 0.9277 - val_loss: 3283541371.4585 - val_acc: 0.9447\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1440028818.2497 - acc: 0.9196 - val_loss: 1375498458.5765 - val_acc: 0.9300\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1470857201.7236 - acc: 0.9210 - val_loss: 1393038099.7478 - val_acc: 0.9163\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1491548153.6925 - acc: 0.9231 - val_loss: 1376717268.4381 - val_acc: 0.9333\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1444337257.6192 - acc: 0.9253 - val_loss: 1388988958.7772 - val_acc: 0.9365\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1504243895.7101 - acc: 0.9181 - val_loss: 1374974769.4085 - val_acc: 0.9341\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1497790286.9274 - acc: 0.9231 - val_loss: 1388475662.0068 - val_acc: 0.9244\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1461972545.3926 - acc: 0.9191 - val_loss: 1389619903.8529 - val_acc: 0.9265\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1475640971.3368 - acc: 0.9251 - val_loss: 1394889166.5990 - val_acc: 0.9147\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1474133798.5234 - acc: 0.9232 - val_loss: 1972743893.5798 - val_acc: 0.9119\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1511792426.1714 - acc: 0.9182 - val_loss: 1371464355.3238 - val_acc: 0.9238\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 26s 7us/step - loss: 1455711338.7181 - acc: 0.9195 - val_loss: 1377684334.3444 - val_acc: 0.9165\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1530197984.4106 - acc: 0.9224 - val_loss: 1431694773.6005 - val_acc: 0.9162\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1467385768.6653 - acc: 0.9168 - val_loss: 1428004740.7603 - val_acc: 0.9160\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1467650060.1115 - acc: 0.9090 - val_loss: 1606938587.8464 - val_acc: 0.9146\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1444494638.3912 - acc: 0.9185 - val_loss: 1394742550.7906 - val_acc: 0.8993\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1454026365.0440 - acc: 0.9065 - val_loss: 1979632046.4312 - val_acc: 0.9219\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1471666484.6896 - acc: 0.9180 - val_loss: 1541159535.5665 - val_acc: 0.9206\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1491796896.7045 - acc: 0.9144 - val_loss: 1369001620.7788 - val_acc: 0.9158\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1448011513.6976 - acc: 0.9097 - val_loss: 9228408150.7466 - val_acc: 0.8598\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1496437586.5870 - acc: 0.9113 - val_loss: 1782689403.3881 - val_acc: 0.8926\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1453941141.5838 - acc: 0.9116 - val_loss: 1400112009.8123 - val_acc: 0.9107\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1485857252.7270 - acc: 0.9137 - val_loss: 1363651328.7217 - val_acc: 0.9131\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1452847231.4899 - acc: 0.9157 - val_loss: 1390343026.4509 - val_acc: 0.9053\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1489092272.4066 - acc: 0.9211 - val_loss: 1388042554.2904 - val_acc: 0.9393\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1450727758.8406 - acc: 0.9226 - val_loss: 1388876480.5620 - val_acc: 0.9188\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1524660589.8868 - acc: 0.9246 - val_loss: 1374342037.3396 - val_acc: 0.9182\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1429882163.5769 - acc: 0.9253 - val_loss: 1391440869.3946 - val_acc: 0.9235\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE4.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe4cd698cf8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE4.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.       7328.0405 183959.64   188912.78   287902.75   286495.25\n",
      "      0.          0.          0.          0.          0.    ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
