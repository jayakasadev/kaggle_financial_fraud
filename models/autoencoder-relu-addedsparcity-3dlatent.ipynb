{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 23:56:30 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 29%   53C    P2    64W / 200W |   1299MiB /  8118MiB |     71%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           389MiB |\r\n",
      "|    0      2077      G   compiz                                       190MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    82MiB |\r\n",
      "|    0     19984      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     22421      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     23089      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4, 3]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(encoder2)\n",
    "latent = Dense(hidden_layer[3], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(encoder3)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(latent)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\", activity_regularizer=regularizers.l1(1e-5))(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 414\n",
      "Trainable params: 414\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 2193060158957.8557 - acc: 0.5111 - val_loss: 2026429162095.6423 - val_acc: 0.5534\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1995185270314.0833 - acc: 0.5531 - val_loss: 2025638703794.8342 - val_acc: 0.5580\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1994795159833.6653 - acc: 0.7028 - val_loss: 2025212696309.2019 - val_acc: 0.7087\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 1994478511922.1438 - acc: 0.6757 - val_loss: 2025891941051.8875 - val_acc: 0.7044\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1994491142011.2002 - acc: 0.6713 - val_loss: 2025094233670.7026 - val_acc: 0.6556\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1994417632947.3108 - acc: 0.6615 - val_loss: 2027135649264.2380 - val_acc: 0.6439\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1994462023889.0974 - acc: 0.6808 - val_loss: 2024978398817.0969 - val_acc: 0.7345\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1994281282355.0420 - acc: 0.7436 - val_loss: 2025376392972.2969 - val_acc: 0.7461\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 33s 8us/step - loss: 1994171184932.0012 - acc: 0.7526 - val_loss: 2024607448267.8503 - val_acc: 0.7690\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1993716230555.0269 - acc: 0.7574 - val_loss: 2024220351918.6147 - val_acc: 0.7629\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1993098793889.1912 - acc: 0.7555 - val_loss: 2023589447439.0330 - val_acc: 0.7484\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992740713015.8889 - acc: 0.7480 - val_loss: 2023453311163.6558 - val_acc: 0.7406\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992670168064.7212 - acc: 0.7511 - val_loss: 2023489024059.3064 - val_acc: 0.7507\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992643943614.1455 - acc: 0.7523 - val_loss: 2023382459062.4556 - val_acc: 0.7508\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992800653151.1606 - acc: 0.7477 - val_loss: 2053180997324.6047 - val_acc: 0.7380\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992699993213.6567 - acc: 0.7552 - val_loss: 2026583971434.1501 - val_acc: 0.7693\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992446657408.7739 - acc: 0.7638 - val_loss: 2023325955740.2024 - val_acc: 0.7663\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992443826429.6294 - acc: 0.7680 - val_loss: 2026111594724.9370 - val_acc: 0.7408\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992410434287.6804 - acc: 0.7611 - val_loss: 2023944149793.1990 - val_acc: 0.7744\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992376020505.2974 - acc: 0.7639 - val_loss: 2023218888198.9707 - val_acc: 0.7567\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992276117817.6338 - acc: 0.7119 - val_loss: 2023087827088.8054 - val_acc: 0.7352\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992169110424.3416 - acc: 0.6987 - val_loss: 2023043793738.6814 - val_acc: 0.7021\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992315929446.3960 - acc: 0.7478 - val_loss: 2023230356114.4453 - val_acc: 0.7640\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992859917181.2634 - acc: 0.7102 - val_loss: 2024055783038.7102 - val_acc: 0.6235\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1993128135022.3418 - acc: 0.6352 - val_loss: 2023875118645.4421 - val_acc: 0.6178\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1993116354975.9275 - acc: 0.6536 - val_loss: 2024467630341.1653 - val_acc: 0.7588\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1993101186635.8667 - acc: 0.6767 - val_loss: 2022801352601.9238 - val_acc: 0.6892\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992524941316.0979 - acc: 0.6006 - val_loss: 2023551382430.4502 - val_acc: 0.6065\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992545183523.1279 - acc: 0.6088 - val_loss: 2024229719339.6904 - val_acc: 0.6076\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992595462204.7690 - acc: 0.6368 - val_loss: 2023225819875.0359 - val_acc: 0.6090\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1987199242408.9106 - acc: 0.5679 - val_loss: 2009238854332.7727 - val_acc: 0.5383\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1977204877820.3472 - acc: 0.5514 - val_loss: 2007061729723.1479 - val_acc: 0.5917\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1973305575042.4053 - acc: 0.6856 - val_loss: 2002066953279.2295 - val_acc: 0.6261\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1971774491660.3833 - acc: 0.6430 - val_loss: 2001580188820.4470 - val_acc: 0.7108\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1971978599840.3228 - acc: 0.6750 - val_loss: 2001398691022.6267 - val_acc: 0.7065\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1971027931564.0803 - acc: 0.7539 - val_loss: 2001042856079.2766 - val_acc: 0.7616\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1993271655083.9207 - acc: 0.7278 - val_loss: 2024154102759.3157 - val_acc: 0.7230\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1993186787381.4160 - acc: 0.7323 - val_loss: 2023776486115.6797 - val_acc: 0.7388\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992848523921.4238 - acc: 0.7404 - val_loss: 2023617833354.2625 - val_acc: 0.7461\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992953404869.5081 - acc: 0.7483 - val_loss: 2023583146281.5776 - val_acc: 0.7458\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992806791737.7014 - acc: 0.7506 - val_loss: 2023563852569.9563 - val_acc: 0.7532\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992715299847.4185 - acc: 0.7544 - val_loss: 2023624671583.4927 - val_acc: 0.7458\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992704648641.9771 - acc: 0.7593 - val_loss: 2023543429576.7874 - val_acc: 0.7676\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992707766016.2080 - acc: 0.7612 - val_loss: 2023534186566.0891 - val_acc: 0.7609\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992608825410.0981 - acc: 0.7590 - val_loss: 2023578955916.0984 - val_acc: 0.7553\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992647514704.1445 - acc: 0.7546 - val_loss: 2023531453462.1492 - val_acc: 0.7584\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992624839580.8721 - acc: 0.7533 - val_loss: 2023525827542.6985 - val_acc: 0.7619\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617264084.8650 - acc: 0.7521 - val_loss: 2023526155624.4045 - val_acc: 0.7506\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992607372438.9465 - acc: 0.7511 - val_loss: 2023525248569.9082 - val_acc: 0.7362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992616502261.6887 - acc: 0.7516 - val_loss: 2023524461541.7871 - val_acc: 0.7630\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992609758908.7036 - acc: 0.7520 - val_loss: 2023663595436.7139 - val_acc: 0.7530\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992673057001.6506 - acc: 0.7530 - val_loss: 2023518127586.0752 - val_acc: 0.7547\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992657740467.2454 - acc: 0.7531 - val_loss: 2023519393457.9292 - val_acc: 0.7659\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992596750844.1521 - acc: 0.7524 - val_loss: 2023685169560.6262 - val_acc: 0.7591\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617293770.3164 - acc: 0.7508 - val_loss: 2023518698681.8252 - val_acc: 0.7641\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992612548721.5793 - acc: 0.7523 - val_loss: 2023549582683.7307 - val_acc: 0.7611\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992621040116.2417 - acc: 0.7510 - val_loss: 2023518550059.6550 - val_acc: 0.7411\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992621532672.0840 - acc: 0.7518 - val_loss: 2025615928823.9226 - val_acc: 0.7651\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992611992527.1555 - acc: 0.7556 - val_loss: 2023518207000.2415 - val_acc: 0.7482\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617640588.7244 - acc: 0.7548 - val_loss: 2023556988786.0510 - val_acc: 0.7497\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617433097.1233 - acc: 0.7598 - val_loss: 2023600678843.7212 - val_acc: 0.7657\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992609639378.0347 - acc: 0.7600 - val_loss: 2023520206575.0461 - val_acc: 0.7622\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992601292528.4338 - acc: 0.7585 - val_loss: 2023525202099.4885 - val_acc: 0.7554\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992610618532.6204 - acc: 0.7570 - val_loss: 2023588664243.8552 - val_acc: 0.7660\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992610204218.4136 - acc: 0.7569 - val_loss: 2023523433002.1562 - val_acc: 0.7590\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992597367715.4102 - acc: 0.7561 - val_loss: 2023516925683.3108 - val_acc: 0.7533\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992619446063.2007 - acc: 0.7558 - val_loss: 2023578690253.3093 - val_acc: 0.7675\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992593921724.9592 - acc: 0.7549 - val_loss: 2023519356614.2883 - val_acc: 0.7537\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992596494176.2329 - acc: 0.7555 - val_loss: 2023516878388.8792 - val_acc: 0.7648\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992634719682.8845 - acc: 0.7560 - val_loss: 2023517346345.3516 - val_acc: 0.7658\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992582544961.9871 - acc: 0.7551 - val_loss: 2023603028216.8130 - val_acc: 0.7473\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992628099703.6233 - acc: 0.7559 - val_loss: 2023516269254.6702 - val_acc: 0.7634\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992592041784.6938 - acc: 0.7543 - val_loss: 2023617579280.4309 - val_acc: 0.7598\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992647429645.5728 - acc: 0.7539 - val_loss: 2023515815208.5518 - val_acc: 0.7564\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992594086833.2632 - acc: 0.7489 - val_loss: 2023524617141.0222 - val_acc: 0.7686\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992595284398.7800 - acc: 0.7506 - val_loss: 2023536197140.1677 - val_acc: 0.7359\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992589197630.5486 - acc: 0.7517 - val_loss: 2023515599807.9260 - val_acc: 0.7627\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992589435054.2256 - acc: 0.7524 - val_loss: 2023515552101.4875 - val_acc: 0.7559\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992587258373.1343 - acc: 0.7557 - val_loss: 2023674062601.0781 - val_acc: 0.7282\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992640950661.6157 - acc: 0.7534 - val_loss: 2023515677574.0679 - val_acc: 0.7597\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992654767919.9790 - acc: 0.7545 - val_loss: 2023517739746.7542 - val_acc: 0.7563\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992569740030.3301 - acc: 0.7552 - val_loss: 2023517126518.2356 - val_acc: 0.7382\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992592245016.3416 - acc: 0.7577 - val_loss: 2023515732034.8301 - val_acc: 0.7454\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992581584698.1543 - acc: 0.7598 - val_loss: 2023515945067.7090 - val_acc: 0.7736\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992590754061.4780 - acc: 0.7586 - val_loss: 2023527171418.8857 - val_acc: 0.7743\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992613965787.0120 - acc: 0.7554 - val_loss: 2023516340650.1687 - val_acc: 0.7713\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992583588910.1243 - acc: 0.7573 - val_loss: 2023527981536.0835 - val_acc: 0.7364\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992609748830.1245 - acc: 0.7561 - val_loss: 2023516062430.2078 - val_acc: 0.7356\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992579270380.2556 - acc: 0.7570 - val_loss: 2023517849693.1438 - val_acc: 0.7720\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992636255055.7134 - acc: 0.7574 - val_loss: 2023515908171.2795 - val_acc: 0.7660\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992580789062.5740 - acc: 0.7594 - val_loss: 2023520662623.2561 - val_acc: 0.7655\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992605955880.8613 - acc: 0.7570 - val_loss: 2023530241315.8845 - val_acc: 0.7301\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992572917593.1677 - acc: 0.7555 - val_loss: 2023516635047.8052 - val_acc: 0.7391\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992597850030.7676 - acc: 0.7539 - val_loss: 2023530814644.1924 - val_acc: 0.7337\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992587718514.7551 - acc: 0.7559 - val_loss: 2023517171878.8142 - val_acc: 0.7605\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992593865587.2771 - acc: 0.7583 - val_loss: 2023533883378.5618 - val_acc: 0.7440\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617734527.6125 - acc: 0.7587 - val_loss: 2023516211969.0913 - val_acc: 0.7560\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992576709227.2288 - acc: 0.7580 - val_loss: 2023516198124.4004 - val_acc: 0.7502\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992592476376.6353 - acc: 0.7552 - val_loss: 2023516768624.0293 - val_acc: 0.7621\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992579365260.4495 - acc: 0.7529 - val_loss: 2023515443053.7861 - val_acc: 0.7635\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992605025331.7715 - acc: 0.7554 - val_loss: 2023520210144.0486 - val_acc: 0.7344\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992591710782.5701 - acc: 0.7586 - val_loss: 2023533105011.5801 - val_acc: 0.7550\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992625597817.3389 - acc: 0.7546 - val_loss: 2024302039449.3508 - val_acc: 0.7727\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992605797447.5769 - acc: 0.7543 - val_loss: 2023515547026.0879 - val_acc: 0.7710\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992589779973.1550 - acc: 0.7566 - val_loss: 2023515455225.9902 - val_acc: 0.7489\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992617938983.4988 - acc: 0.7546 - val_loss: 2023515457153.4058 - val_acc: 0.7503\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992594579547.3416 - acc: 0.7569 - val_loss: 2023521960228.5889 - val_acc: 0.7730\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992581689113.3367 - acc: 0.7577 - val_loss: 2023516484211.1023 - val_acc: 0.7444\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992600637950.6023 - acc: 0.7562 - val_loss: 2023515446728.3652 - val_acc: 0.7698\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992587756384.2639 - acc: 0.7572 - val_loss: 2023515668569.1609 - val_acc: 0.7594\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992576191113.4031 - acc: 0.7575 - val_loss: 2023540043920.9265 - val_acc: 0.7373\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992591194071.2400 - acc: 0.7577 - val_loss: 2023517099112.1077 - val_acc: 0.7734\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992601291938.1057 - acc: 0.7560 - val_loss: 2023515840648.6377 - val_acc: 0.7725\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992591157525.8811 - acc: 0.7573 - val_loss: 2023515456383.4695 - val_acc: 0.7587\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992580200668.1719 - acc: 0.7569 - val_loss: 2023525698850.0337 - val_acc: 0.7404\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992602039777.0750 - acc: 0.7554 - val_loss: 2023516034806.4893 - val_acc: 0.7720\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992602633466.2749 - acc: 0.7537 - val_loss: 2023515622732.2000 - val_acc: 0.7658\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992581384965.0210 - acc: 0.7559 - val_loss: 2023523658395.2168 - val_acc: 0.7365\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992605691299.0063 - acc: 0.7559 - val_loss: 2023515694909.2224 - val_acc: 0.7593\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992589228411.3577 - acc: 0.7605 - val_loss: 2023654211688.6111 - val_acc: 0.7273\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992587753448.4604 - acc: 0.7533 - val_loss: 2023529645958.1282 - val_acc: 0.7330\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992603361870.3843 - acc: 0.7577 - val_loss: 2023515711758.9824 - val_acc: 0.7736\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992584518666.4238 - acc: 0.7561 - val_loss: 2023515592672.4958 - val_acc: 0.7424\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992585152157.8938 - acc: 0.7579 - val_loss: 2023517939382.9988 - val_acc: 0.7499\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992584325020.4285 - acc: 0.7594 - val_loss: 2023920880873.3828 - val_acc: 0.7291\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992572748504.6882 - acc: 0.7615 - val_loss: 2023518441971.1550 - val_acc: 0.7730\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 1992604757476.2676 - acc: 0.7543 - val_loss: 2023515810324.9927 - val_acc: 0.7406\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992594319375.5649 - acc: 0.7551 - val_loss: 2023515627040.2385 - val_acc: 0.7512\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992580261026.4739 - acc: 0.7560 - val_loss: 2023969567453.3223 - val_acc: 0.7666\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992607522799.8015 - acc: 0.7603 - val_loss: 2023516276787.9434 - val_acc: 0.7662\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992570261394.8223 - acc: 0.7564 - val_loss: 2023663605850.1665 - val_acc: 0.7740\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992604516389.3381 - acc: 0.7550 - val_loss: 2023515500089.5864 - val_acc: 0.7513\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992569259808.5447 - acc: 0.7554 - val_loss: 2023523757400.2705 - val_acc: 0.7662\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992601050562.2568 - acc: 0.7552 - val_loss: 2023516112379.5037 - val_acc: 0.7373\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992586262826.6140 - acc: 0.7551 - val_loss: 2023515626652.6948 - val_acc: 0.7604\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992580584139.4468 - acc: 0.7540 - val_loss: 2023515538519.4507 - val_acc: 0.7543\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992592603108.3406 - acc: 0.7546 - val_loss: 2023516739920.0828 - val_acc: 0.7333\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992585806411.2703 - acc: 0.7557 - val_loss: 2023515434042.4417 - val_acc: 0.7566\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992571388625.6873 - acc: 0.7561 - val_loss: 2023516457623.8367 - val_acc: 0.7734\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992597426132.0032 - acc: 0.7522 - val_loss: 2023515632513.4812 - val_acc: 0.7492\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992579406349.8535 - acc: 0.7541 - val_loss: 2023846859447.8235 - val_acc: 0.7263\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992589043582.5579 - acc: 0.7560 - val_loss: 2023515665279.5498 - val_acc: 0.7510\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992579604297.3250 - acc: 0.7550 - val_loss: 2023837628814.0444 - val_acc: 0.7714\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992581526617.6045 - acc: 0.7550 - val_loss: 2023515800789.6274 - val_acc: 0.7639\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992573185869.6907 - acc: 0.7589 - val_loss: 2023523616956.4202 - val_acc: 0.7316\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992594796571.5488 - acc: 0.7568 - val_loss: 2023562609321.0974 - val_acc: 0.7310\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992576763741.8630 - acc: 0.7540 - val_loss: 2023515351119.7859 - val_acc: 0.7564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 1992586179386.6804 - acc: 0.7543 - val_loss: 2023515479037.3643 - val_acc: 0.7519\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1992582769028.9297 - acc: 0.7599 - val_loss: 2023521769751.7537 - val_acc: 0.7744\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992582228488.5625 - acc: 0.7612 - val_loss: 2023515611203.7559 - val_acc: 0.7611\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992585766310.9392 - acc: 0.7599 - val_loss: 2023515761197.1335 - val_acc: 0.7520\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1992587239152.5183 - acc: 0.7608 - val_loss: 2023519725828.3601 - val_acc: 0.7733\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992590367395.2219 - acc: 0.7594 - val_loss: 2023516479313.3799 - val_acc: 0.7506\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992570297235.6501 - acc: 0.7591 - val_loss: 2023515530472.4578 - val_acc: 0.7722\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992576768862.4917 - acc: 0.7578 - val_loss: 2023747070283.1738 - val_acc: 0.7301\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1992574263820.7207 - acc: 0.7568 - val_loss: 2023593453596.6074 - val_acc: 0.7719\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992589755725.4165 - acc: 0.7528 - val_loss: 2023652527868.8064 - val_acc: 0.7704\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992586960466.5879 - acc: 0.7568 - val_loss: 2023515545730.8442 - val_acc: 0.7652\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992585152822.2542 - acc: 0.7564 - val_loss: 2023515463070.8425 - val_acc: 0.7674\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992585550179.0476 - acc: 0.7582 - val_loss: 2023603237732.1904 - val_acc: 0.7271\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992572086611.7463 - acc: 0.7590 - val_loss: 2023516416353.7056 - val_acc: 0.7690\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1992590909033.9724 - acc: 0.7581 - val_loss: 2023515292602.0515 - val_acc: 0.7720\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992573120019.5999 - acc: 0.7585 - val_loss: 2023526798935.8027 - val_acc: 0.7748\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992600970711.0925 - acc: 0.7587 - val_loss: 2023519792257.8989 - val_acc: 0.7747\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992568561326.4043 - acc: 0.7562 - val_loss: 2023518969646.6172 - val_acc: 0.7570\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992586189159.4736 - acc: 0.7552 - val_loss: 2023515429337.8469 - val_acc: 0.7425\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992572302932.0022 - acc: 0.7605 - val_loss: 2023730378586.7549 - val_acc: 0.7717\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992580657016.6936 - acc: 0.7594 - val_loss: 2023517060437.6152 - val_acc: 0.7749\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992577556970.6819 - acc: 0.7603 - val_loss: 2023515338630.8523 - val_acc: 0.7664\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 1992583526053.9688 - acc: 0.7589 - val_loss: 2023515396054.1958 - val_acc: 0.7741\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992590232597.0098 - acc: 0.7595 - val_loss: 2023527405343.0464 - val_acc: 0.7352\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992587635226.1694 - acc: 0.7600 - val_loss: 2023516231921.7717 - val_acc: 0.7332\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992572079040.3552 - acc: 0.7610 - val_loss: 2023532129973.5098 - val_acc: 0.7288\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 1992596973963.0293 - acc: 0.7605 - val_loss: 2023515872504.4309 - val_acc: 0.7554\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992574754317.4319 - acc: 0.7621 - val_loss: 2023516002805.2073 - val_acc: 0.7506\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992577939670.2173 - acc: 0.7585 - val_loss: 2023515535686.9092 - val_acc: 0.7578\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992583170877.4951 - acc: 0.7583 - val_loss: 2023607773276.3391 - val_acc: 0.7274\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992608951602.0105 - acc: 0.7582 - val_loss: 2023525982999.0198 - val_acc: 0.7734\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992567574912.0071 - acc: 0.7595 - val_loss: 2023515297623.3755 - val_acc: 0.7731\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992588993499.8899 - acc: 0.7588 - val_loss: 2023515373651.3667 - val_acc: 0.7613\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992575282696.9780 - acc: 0.7577 - val_loss: 2023515847785.3552 - val_acc: 0.7702\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 26s 7us/step - loss: 1992576519560.3145 - acc: 0.7596 - val_loss: 2023515386397.8040 - val_acc: 0.7502\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 26s 6us/step - loss: 1992581317299.7202 - acc: 0.7561 - val_loss: 2023515416139.4910 - val_acc: 0.7734\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992575095763.2759 - acc: 0.7589 - val_loss: 2023515459790.2441 - val_acc: 0.7739\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992578275684.2708 - acc: 0.7588 - val_loss: 2023515501247.0657 - val_acc: 0.7730\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992628332421.0920 - acc: 0.7554 - val_loss: 2023517609591.4680 - val_acc: 0.7744\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992573073229.9583 - acc: 0.7590 - val_loss: 2023629166632.5969 - val_acc: 0.7730\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992578215283.7383 - acc: 0.7581 - val_loss: 2023521125756.6829 - val_acc: 0.7629\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992582677955.8684 - acc: 0.7578 - val_loss: 2023515399918.0398 - val_acc: 0.7742\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992572034998.2815 - acc: 0.7552 - val_loss: 2023515998494.5938 - val_acc: 0.7730\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992580874657.4946 - acc: 0.7606 - val_loss: 2023515498093.9316 - val_acc: 0.7418\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992597851584.2893 - acc: 0.7614 - val_loss: 2024153855451.3965 - val_acc: 0.7278\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992584901248.4087 - acc: 0.7604 - val_loss: 2023515337665.2134 - val_acc: 0.7734\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992577283140.8591 - acc: 0.7606 - val_loss: 2023515325948.7910 - val_acc: 0.7745\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992571931844.2253 - acc: 0.7595 - val_loss: 2023515365219.4055 - val_acc: 0.7422\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992571908772.8936 - acc: 0.7611 - val_loss: 2023515292859.5552 - val_acc: 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992582327792.9978 - acc: 0.7577 - val_loss: 2023515384917.1570 - val_acc: 0.7607\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992574974252.2073 - acc: 0.7576 - val_loss: 2023545365903.5132 - val_acc: 0.7669\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992572661967.3027 - acc: 0.7585 - val_loss: 2023515504142.6958 - val_acc: 0.7588\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 1992568230302.5212 - acc: 0.7622 - val_loss: 2023516114760.1262 - val_acc: 0.7753\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE4.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f21450024e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE4.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.       7151.7344 186529.     193934.75   291309.72   288420.\n",
      "      0.          0.          0.          0.          0.    ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
