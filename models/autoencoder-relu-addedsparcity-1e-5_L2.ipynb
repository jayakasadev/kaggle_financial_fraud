{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 26 18:27:01 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "|  0%   46C    P8    14W / 200W |    689MiB /  8118MiB |     10%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1213      G   /usr/lib/xorg/Xorg                           385MiB |\r\n",
      "|    0      2714      G   compiz                                       206MiB |\r\n",
      "|    0      3011      G   ...-token=BBC2FE444913F04620C8FE4B1878C0D2    94MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "regulizer_value = 1e-5\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\", activity_regularizer=regularizers.l2(regulizer_value))(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 1997016703469.3860 - acc: 0.4080 - val_loss: 1564392731422.9858 - val_acc: 0.5239\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 1224612594740.9663 - acc: 0.5322 - val_loss: 479832136086.4938 - val_acc: 0.5492\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 454038015615.8000 - acc: 0.5721 - val_loss: 453124138047.6316 - val_acc: 0.5817\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 439584701817.8982 - acc: 0.5781 - val_loss: 452900923604.9435 - val_acc: 0.5852\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 432867499042.5649 - acc: 0.5566 - val_loss: 471075146672.4755 - val_acc: 0.5567\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 429150384579.4166 - acc: 0.5589 - val_loss: 471879682807.7166 - val_acc: 0.5403\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 426900045534.6138 - acc: 0.5899 - val_loss: 445120872248.6760 - val_acc: 0.6967\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 425586670050.4700 - acc: 0.6566 - val_loss: 440009419861.5194 - val_acc: 0.6398\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 425055655616.5200 - acc: 0.7088 - val_loss: 434184232294.1113 - val_acc: 0.7566\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 424446408190.4223 - acc: 0.7239 - val_loss: 465240050285.1876 - val_acc: 0.7442\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 423538279371.7719 - acc: 0.7570 - val_loss: 445175895401.6922 - val_acc: 0.7644\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 423364191745.7578 - acc: 0.7571 - val_loss: 427730229179.1982 - val_acc: 0.7829\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 423654751592.0359 - acc: 0.7688 - val_loss: 435340478681.1481 - val_acc: 0.7408\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 422922585296.9052 - acc: 0.7547 - val_loss: 437271102596.5742 - val_acc: 0.7721\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 422708422719.0302 - acc: 0.7695 - val_loss: 429502708423.8974 - val_acc: 0.7721\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 422111924076.9410 - acc: 0.7694 - val_loss: 431030059461.1262 - val_acc: 0.7639\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 422056966334.8326 - acc: 0.7727 - val_loss: 450903389830.8777 - val_acc: 0.7835\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 421790105181.0812 - acc: 0.7722 - val_loss: 428681598151.7062 - val_acc: 0.7618\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421517528978.6896 - acc: 0.7624 - val_loss: 440487376133.2255 - val_acc: 0.7918\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421547930564.1112 - acc: 0.7749 - val_loss: 443909226986.7458 - val_acc: 0.7671\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421258565971.6660 - acc: 0.7821 - val_loss: 442742377500.4662 - val_acc: 0.8043\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421770171376.0639 - acc: 0.7914 - val_loss: 437019365245.6387 - val_acc: 0.7537\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420718410466.4069 - acc: 0.7620 - val_loss: 462189185888.9311 - val_acc: 0.7622\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421097955429.0381 - acc: 0.7682 - val_loss: 457604745938.9821 - val_acc: 0.7880\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420909883124.5419 - acc: 0.7734 - val_loss: 446027000596.0320 - val_acc: 0.7712\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420708727462.5854 - acc: 0.7658 - val_loss: 426712194937.1123 - val_acc: 0.7884\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420619358617.4315 - acc: 0.7704 - val_loss: 464338193263.1743 - val_acc: 0.7752\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 421533654149.6235 - acc: 0.7709 - val_loss: 431934904223.9994 - val_acc: 0.7732\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420340334825.9666 - acc: 0.7771 - val_loss: 446759556297.6778 - val_acc: 0.7832\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420234459364.2673 - acc: 0.7875 - val_loss: 440622478829.3611 - val_acc: 0.7938\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420665242995.8650 - acc: 0.7849 - val_loss: 436000829294.1885 - val_acc: 0.7801\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 420441458125.4442 - acc: 0.7859 - val_loss: 441848158787.5444 - val_acc: 0.7670\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420210010271.1877 - acc: 0.7971 - val_loss: 492848272480.8657 - val_acc: 0.8026\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 420280966901.8978 - acc: 0.7973 - val_loss: 439518789254.5356 - val_acc: 0.8155\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420241262727.9762 - acc: 0.7826 - val_loss: 433152515059.2656 - val_acc: 0.7635\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419732034127.5069 - acc: 0.7935 - val_loss: 535556836079.3176 - val_acc: 0.7688\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 420284981197.6781 - acc: 0.7970 - val_loss: 467400071441.0747 - val_acc: 0.7741\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419927946128.6385 - acc: 0.7997 - val_loss: 433529895469.3146 - val_acc: 0.8029\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 420041904363.0006 - acc: 0.7883 - val_loss: 435665544893.4765 - val_acc: 0.7955\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419603434748.4736 - acc: 0.7966 - val_loss: 475703573289.8494 - val_acc: 0.8014\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419696563228.9702 - acc: 0.7993 - val_loss: 437300573204.3790 - val_acc: 0.8104\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419768461869.8282 - acc: 0.7962 - val_loss: 436475611318.3650 - val_acc: 0.8083\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419708361027.8013 - acc: 0.8020 - val_loss: 444889380396.3691 - val_acc: 0.7791\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419367011746.4008 - acc: 0.8078 - val_loss: 443580790269.3546 - val_acc: 0.7950\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419485529789.1925 - acc: 0.8096 - val_loss: 431096419956.4903 - val_acc: 0.7993\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419789867349.5087 - acc: 0.8148 - val_loss: 440305771168.4067 - val_acc: 0.8373\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419631610306.4611 - acc: 0.8139 - val_loss: 431942803529.1271 - val_acc: 0.8062\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419398550635.8107 - acc: 0.8214 - val_loss: 441074919961.7202 - val_acc: 0.8454\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419283193944.3762 - acc: 0.8246 - val_loss: 460678715979.0081 - val_acc: 0.8458\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419997238485.4839 - acc: 0.8257 - val_loss: 429835080987.6766 - val_acc: 0.8262\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419996630821.3630 - acc: 0.8248 - val_loss: 440907779280.1154 - val_acc: 0.8899\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419043006120.9543 - acc: 0.8378 - val_loss: 459166541829.9146 - val_acc: 0.8172\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419857915161.8573 - acc: 0.8053 - val_loss: 431759257323.8876 - val_acc: 0.7410\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419219639423.1416 - acc: 0.8189 - val_loss: 431564528726.6058 - val_acc: 0.8364\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419299932151.3796 - acc: 0.8021 - val_loss: 430593839578.5916 - val_acc: 0.7635\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419230977213.1960 - acc: 0.8104 - val_loss: 426185202780.9830 - val_acc: 0.8110\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419541187797.9869 - acc: 0.8061 - val_loss: 433195716093.8373 - val_acc: 0.8514\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418972140932.6832 - acc: 0.8288 - val_loss: 448274329354.3253 - val_acc: 0.8378\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419819348076.6425 - acc: 0.8348 - val_loss: 440853778403.4332 - val_acc: 0.8102\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418664200294.8467 - acc: 0.8432 - val_loss: 460758033986.0759 - val_acc: 0.8198\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419096173085.1749 - acc: 0.8473 - val_loss: 437363490189.1594 - val_acc: 0.8750\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420325117020.9419 - acc: 0.8298 - val_loss: 435254041075.3763 - val_acc: 0.8469\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418900140012.8320 - acc: 0.8402 - val_loss: 441076510385.0640 - val_acc: 0.8126\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420548606192.7884 - acc: 0.8500 - val_loss: 442123141259.3739 - val_acc: 0.8735\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419054954566.0109 - acc: 0.8562 - val_loss: 521924869495.1106 - val_acc: 0.7691\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 420392885893.1392 - acc: 0.8285 - val_loss: 431926032682.6843 - val_acc: 0.8479\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418744321734.0804 - acc: 0.8382 - val_loss: 427509510895.8507 - val_acc: 0.8159\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419386814872.9788 - acc: 0.8336 - val_loss: 425463760752.6227 - val_acc: 0.8625\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418756540376.7045 - acc: 0.8359 - val_loss: 453684077409.1272 - val_acc: 0.8345\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418918630323.5182 - acc: 0.8403 - val_loss: 428040073693.9310 - val_acc: 0.8878\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418746710783.0543 - acc: 0.8386 - val_loss: 445346400664.8074 - val_acc: 0.8984\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419267890601.8463 - acc: 0.8462 - val_loss: 428627060831.8598 - val_acc: 0.8124\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418948279593.5569 - acc: 0.8412 - val_loss: 428422365472.5250 - val_acc: 0.8098\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418740267834.4089 - acc: 0.8357 - val_loss: 435980253652.7374 - val_acc: 0.8135\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418674023032.5969 - acc: 0.8480 - val_loss: 442134665089.0989 - val_acc: 0.8516\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419315077860.1028 - acc: 0.8402 - val_loss: 443583562155.5367 - val_acc: 0.8893\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419014245364.4153 - acc: 0.8495 - val_loss: 438363383565.8660 - val_acc: 0.8650\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419049399329.0219 - acc: 0.8585 - val_loss: 458564178160.7761 - val_acc: 0.8159\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418725455765.8621 - acc: 0.8707 - val_loss: 458322852669.8663 - val_acc: 0.7884\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419670669365.8928 - acc: 0.8631 - val_loss: 428358734875.4805 - val_acc: 0.8605\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418810089742.9063 - acc: 0.8674 - val_loss: 467779550168.0618 - val_acc: 0.8505\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419092657924.3361 - acc: 0.8704 - val_loss: 437200128564.6776 - val_acc: 0.8898\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419039024861.5133 - acc: 0.8653 - val_loss: 426284959955.8773 - val_acc: 0.8606\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418638243036.6619 - acc: 0.8665 - val_loss: 444405176171.0904 - val_acc: 0.8278\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418581565863.6143 - acc: 0.8647 - val_loss: 451534975886.0948 - val_acc: 0.8827\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418721511066.0131 - acc: 0.8725 - val_loss: 425176296530.2805 - val_acc: 0.9178\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418987374118.7418 - acc: 0.8767 - val_loss: 425002189215.6272 - val_acc: 0.8417\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418958271726.8561 - acc: 0.8778 - val_loss: 436742112259.0377 - val_acc: 0.6815\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419025808278.2131 - acc: 0.8253 - val_loss: 444114189882.1496 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418782334734.5377 - acc: 0.8315 - val_loss: 470128836918.4832 - val_acc: 0.8373\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418906847501.5162 - acc: 0.8420 - val_loss: 442967369484.3572 - val_acc: 0.8011\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418911108551.2782 - acc: 0.8507 - val_loss: 435927699203.2842 - val_acc: 0.8733\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418762785300.1844 - acc: 0.8572 - val_loss: 439750176314.4111 - val_acc: 0.8510\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418810643970.2466 - acc: 0.8619 - val_loss: 424386501411.9550 - val_acc: 0.9378\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418790926813.4708 - acc: 0.8715 - val_loss: 456658315241.6897 - val_acc: 0.8606\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418651325298.5588 - acc: 0.8550 - val_loss: 429954298363.4635 - val_acc: 0.8514\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419316721788.8593 - acc: 0.8677 - val_loss: 472964225174.0764 - val_acc: 0.8773\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419965706397.0936 - acc: 0.8483 - val_loss: 426012220949.4150 - val_acc: 0.8902\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418729109056.5566 - acc: 0.8673 - val_loss: 459803565695.8868 - val_acc: 0.7651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418802793752.2398 - acc: 0.8584 - val_loss: 444358131751.4504 - val_acc: 0.8244\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418489563212.6336 - acc: 0.8566 - val_loss: 431965041500.5455 - val_acc: 0.8773\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418968121810.5330 - acc: 0.8723 - val_loss: 433056758498.9554 - val_acc: 0.9041\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418699416174.2062 - acc: 0.8639 - val_loss: 425417223088.5963 - val_acc: 0.8727\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418552485752.4241 - acc: 0.8577 - val_loss: 444363414207.0658 - val_acc: 0.9138\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418634879866.8472 - acc: 0.8602 - val_loss: 447188551570.9230 - val_acc: 0.8907\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418683431782.3656 - acc: 0.8692 - val_loss: 464904119488.5847 - val_acc: 0.9146\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418576211237.4449 - acc: 0.8703 - val_loss: 471586202118.6890 - val_acc: 0.8619\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418924122717.8064 - acc: 0.8639 - val_loss: 435754644528.6441 - val_acc: 0.8207\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418327306435.5823 - acc: 0.8637 - val_loss: 458463298585.6096 - val_acc: 0.8847\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418793668598.4643 - acc: 0.8611 - val_loss: 425411200744.0049 - val_acc: 0.8680\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418864204859.9894 - acc: 0.8702 - val_loss: 461387827461.3362 - val_acc: 0.8503\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418825980151.4151 - acc: 0.8654 - val_loss: 446814541250.7121 - val_acc: 0.8243\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418599099626.0903 - acc: 0.8889 - val_loss: 433596909458.4000 - val_acc: 0.8792\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418497434371.4218 - acc: 0.8775 - val_loss: 425646904182.9798 - val_acc: 0.8863\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419050806838.4877 - acc: 0.8733 - val_loss: 435478628976.8490 - val_acc: 0.8613\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418253260796.5176 - acc: 0.8854 - val_loss: 539070296771.2905 - val_acc: 0.8729\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418721191904.6650 - acc: 0.8817 - val_loss: 440987439761.8015 - val_acc: 0.8438\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418776058264.6287 - acc: 0.8960 - val_loss: 461481306907.2441 - val_acc: 0.8553\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418636378987.6988 - acc: 0.8765 - val_loss: 424597134656.2905 - val_acc: 0.8955\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419786395226.8079 - acc: 0.8666 - val_loss: 436294302362.2106 - val_acc: 0.8592\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418271459895.2552 - acc: 0.8697 - val_loss: 438859862793.0378 - val_acc: 0.8924\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418580669338.6249 - acc: 0.8756 - val_loss: 435793204514.1545 - val_acc: 0.8518\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418397424701.0838 - acc: 0.8775 - val_loss: 438520766491.0781 - val_acc: 0.8602\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418793846330.8995 - acc: 0.8614 - val_loss: 448029306865.8574 - val_acc: 0.8549\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 419372583881.7381 - acc: 0.8810 - val_loss: 429692603140.4308 - val_acc: 0.9037\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418363236081.0012 - acc: 0.8794 - val_loss: 463577138150.5312 - val_acc: 0.7921\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418582214466.9780 - acc: 0.8603 - val_loss: 430789235595.8819 - val_acc: 0.8611\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418578992405.7553 - acc: 0.8549 - val_loss: 425242393026.9334 - val_acc: 0.8364\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418983293880.4531 - acc: 0.8511 - val_loss: 427533413931.0011 - val_acc: 0.9005\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418892206642.9440 - acc: 0.8751 - val_loss: 454110268589.2316 - val_acc: 0.8262\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418544541594.4227 - acc: 0.8751 - val_loss: 426830223533.1713 - val_acc: 0.8727\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418984479396.9158 - acc: 0.8861 - val_loss: 435011009308.5518 - val_acc: 0.9216\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418691954065.7998 - acc: 0.8741 - val_loss: 430418703953.6066 - val_acc: 0.8215\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 420557111923.4106 - acc: 0.8526 - val_loss: 434253242380.5734 - val_acc: 0.8477\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418587759012.0575 - acc: 0.8655 - val_loss: 442272749219.8066 - val_acc: 0.8958\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418524765135.4636 - acc: 0.8616 - val_loss: 451677721373.8392 - val_acc: 0.8380\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418947191962.3022 - acc: 0.8488 - val_loss: 434259580423.0511 - val_acc: 0.8805\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418458766136.6104 - acc: 0.8643 - val_loss: 450810780736.1144 - val_acc: 0.8424\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418625457741.6188 - acc: 0.8602 - val_loss: 428187159467.0439 - val_acc: 0.8705\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419112603595.6180 - acc: 0.8653 - val_loss: 431822560068.8873 - val_acc: 0.7725\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418221826522.6363 - acc: 0.8644 - val_loss: 485465105909.5490 - val_acc: 0.8249\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418466401779.0579 - acc: 0.8751 - val_loss: 443105515056.0506 - val_acc: 0.8878\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418540957493.1985 - acc: 0.8736 - val_loss: 424840100141.1788 - val_acc: 0.8904\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418829403002.3403 - acc: 0.8770 - val_loss: 431752374094.1011 - val_acc: 0.8026\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418064511444.1584 - acc: 0.8824 - val_loss: 439161588302.1464 - val_acc: 0.8694\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418928745269.5244 - acc: 0.8813 - val_loss: 427914908065.2769 - val_acc: 0.9002\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418502512834.7776 - acc: 0.8805 - val_loss: 439011164571.8451 - val_acc: 0.8665\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418670684927.4566 - acc: 0.8684 - val_loss: 434394136285.8455 - val_acc: 0.8390\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418224247787.0788 - acc: 0.8868 - val_loss: 455147650859.7807 - val_acc: 0.8793\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419079873566.0203 - acc: 0.8772 - val_loss: 436690697409.7917 - val_acc: 0.7902\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418523022110.7534 - acc: 0.8784 - val_loss: 434740149633.0285 - val_acc: 0.8312\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418766318905.1883 - acc: 0.8764 - val_loss: 433285524479.0545 - val_acc: 0.8618\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418441339329.7993 - acc: 0.8784 - val_loss: 425172382777.3751 - val_acc: 0.8793\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418696638786.1416 - acc: 0.8824 - val_loss: 456181505669.1274 - val_acc: 0.8838\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418643934234.8076 - acc: 0.8858 - val_loss: 436998946409.2245 - val_acc: 0.8966\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418247409018.9846 - acc: 0.9106 - val_loss: 467939047979.3632 - val_acc: 0.8895\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418465693030.5532 - acc: 0.8982 - val_loss: 450763934509.6717 - val_acc: 0.8773\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418666844407.8517 - acc: 0.8765 - val_loss: 435072702923.0206 - val_acc: 0.8883\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418425424533.7632 - acc: 0.8882 - val_loss: 435863830398.1617 - val_acc: 0.8833\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418175920570.6595 - acc: 0.8969 - val_loss: 453663846484.8555 - val_acc: 0.8392\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418860334677.6699 - acc: 0.8797 - val_loss: 443373747461.5071 - val_acc: 0.8079\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418548100816.3223 - acc: 0.8827 - val_loss: 427017603719.3001 - val_acc: 0.8782\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418327038501.6564 - acc: 0.8784 - val_loss: 433376171115.9705 - val_acc: 0.7835\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418302512783.4715 - acc: 0.8674 - val_loss: 424101266630.0969 - val_acc: 0.8558\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418673824817.0555 - acc: 0.8661 - val_loss: 474081244625.7398 - val_acc: 0.9109\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418591465462.2953 - acc: 0.8787 - val_loss: 443334520991.3707 - val_acc: 0.8769\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418491428812.9136 - acc: 0.8499 - val_loss: 480135181937.3117 - val_acc: 0.8019\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418361129751.3361 - acc: 0.8638 - val_loss: 454778423595.3079 - val_acc: 0.9091\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418607898916.6936 - acc: 0.8801 - val_loss: 455126010063.6326 - val_acc: 0.8476\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418592983777.2763 - acc: 0.8862 - val_loss: 433882155861.4239 - val_acc: 0.8890\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418839165611.6390 - acc: 0.8760 - val_loss: 434980586628.5742 - val_acc: 0.8778\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418451545044.4929 - acc: 0.8915 - val_loss: 457203971536.5328 - val_acc: 0.8755\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418680435711.8723 - acc: 0.8688 - val_loss: 455354195380.9316 - val_acc: 0.8721\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418175700561.8596 - acc: 0.8866 - val_loss: 455759600814.4387 - val_acc: 0.8805\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418712975431.2783 - acc: 0.8808 - val_loss: 433537583160.2284 - val_acc: 0.8915\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418769801268.1083 - acc: 0.8816 - val_loss: 444737849472.8323 - val_acc: 0.8391\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418382419147.8718 - acc: 0.8881 - val_loss: 437552044000.4759 - val_acc: 0.8263\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418219733993.3728 - acc: 0.8885 - val_loss: 471281170816.1635 - val_acc: 0.9042\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418442567113.4509 - acc: 0.8879 - val_loss: 461375127391.3820 - val_acc: 0.8670\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418693208528.4628 - acc: 0.8869 - val_loss: 430221901941.0435 - val_acc: 0.9260\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418533833024.4698 - acc: 0.8772 - val_loss: 442964486908.8265 - val_acc: 0.8426\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418319702505.1269 - acc: 0.8888 - val_loss: 440818714341.7919 - val_acc: 0.8312\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418393291523.0858 - acc: 0.8947 - val_loss: 432858594321.8241 - val_acc: 0.8199\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418467126508.6854 - acc: 0.8855 - val_loss: 431441951243.8190 - val_acc: 0.9038\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418379087062.5763 - acc: 0.9017 - val_loss: 459029271625.0466 - val_acc: 0.8608\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418719778702.3220 - acc: 0.8980 - val_loss: 450633787672.2365 - val_acc: 0.8165\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418242577411.8756 - acc: 0.8865 - val_loss: 464943266753.0926 - val_acc: 0.9048\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418559726638.6454 - acc: 0.8898 - val_loss: 439462678780.6756 - val_acc: 0.9422\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418649073153.3272 - acc: 0.8940 - val_loss: 426585947383.3445 - val_acc: 0.9068\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418472493635.0059 - acc: 0.8997 - val_loss: 438847842195.5266 - val_acc: 0.8927\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 418474289994.4949 - acc: 0.9009 - val_loss: 451558075482.0258 - val_acc: 0.8977\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418163951664.8634 - acc: 0.9164 - val_loss: 459211243639.2564 - val_acc: 0.8891\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418409931034.6992 - acc: 0.9044 - val_loss: 494501021663.1281 - val_acc: 0.8907\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418740852599.4564 - acc: 0.8725 - val_loss: 439446513415.8508 - val_acc: 0.9020\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418740938625.0847 - acc: 0.8978 - val_loss: 433012433459.7723 - val_acc: 0.9408\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419091700074.9821 - acc: 0.8844 - val_loss: 444714610478.0540 - val_acc: 0.8767\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418635829138.6614 - acc: 0.8895 - val_loss: 445027153235.5429 - val_acc: 0.8719\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418301083173.6614 - acc: 0.9010 - val_loss: 428524599146.2455 - val_acc: 0.8826\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 418764408151.8051 - acc: 0.8763 - val_loss: 424041186896.3995 - val_acc: 0.9260\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 419395298059.4969 - acc: 0.8564 - val_loss: 427791507837.4879 - val_acc: 0.8678\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE4.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f76e3215048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE4.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7707455e+02 3.0563088e+05 8.3101984e+04 3.1146762e+05 3.8665406e+05\n",
      " 1.5763394e+05 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
