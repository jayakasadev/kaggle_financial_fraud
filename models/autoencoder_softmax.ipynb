{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 21:31:24 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 28%   52C    P2    62W / 200W |   1082MiB /  8118MiB |     39%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           389MiB |\r\n",
      "|    0      2077      G   compiz                                       188MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    77MiB |\r\n",
      "|    0     13425      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     14165      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"softmax\")(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"softmax\")(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"softmax\")(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"softmax\")(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"softmax\")(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"softmax\")(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111641577.0518 - acc: 0.3226 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111607123.9136 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111615848.4072 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111610356.8770 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111613094.7881 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111616598.7744 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111599429.1777 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111596348.1421 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111602961.8755 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111595229.8018 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111588828.7671 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111593093.0337 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111595210.6182 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590410.3564 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111588389.2070 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111587781.2402 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585890.6460 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590113.4541 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590191.4780 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589259.3135 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111599801.2710 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594513.6836 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111595568.1626 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589195.1948 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590618.9346 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585027.7500 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111600934.5464 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111596949.4141 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111591775.1279 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589281.4585 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589841.5298 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111586203.8994 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585323.1069 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111598992.5796 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111592201.5547 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111587959.5615 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111592609.5698 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111583931.9414 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111598459.1602 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590026.8042 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111591411.2749 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111590145.6426 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111591418.4849 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111597738.9214 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111593738.3389 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594810.5854 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111580587.6074 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585813.6519 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585413.2334 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585286.0259 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111591606.3340 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111587684.4185 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111586618.8672 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111593338.1772 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111588346.2041 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111596237.9307 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594449.3071 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585584.0869 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594857.7085 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594690.3311 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589959.2095 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111584271.4604 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111586990.7026 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594260.2993 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111596957.1387 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111581895.9854 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594536.0859 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111585177.8745 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111591405.0942 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111592174.0020 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111592048.0820 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111592829.8643 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111593749.9263 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111595592.8828 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111596690.6221 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111597283.9116 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111588955.7158 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590084.6138 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111587317.9907 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593726.4932 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111591549.0396 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111581230.3379 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584419.6538 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584344.0762 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111587111.4722 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111598982.9233 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111588558.6450 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584591.7310 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111588384.8296 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593109.2563 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111589689.3452 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111591458.6553 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590398.7690 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111580756.0151 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111602863.6382 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590095.6865 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111583315.3477 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111594709.3862 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111598548.2563 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111603166.3340 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111597542.4458 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111589965.6470 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111588346.4619 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111589355.1050 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111587830.1660 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593887.1758 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111597025.3779 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111586661.6128 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593428.5610 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111588967.0464 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111601232.4785 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111596056.9048 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111587655.8359 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590200.4907 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111592925.9136 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584808.6138 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590045.4731 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584947.6660 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111584276.6104 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111595263.7925 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111595145.3408 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111594139.9165 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111591458.9126 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111591545.3057 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111598727.6079 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111594030.6060 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111594144.5513 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111587587.8550 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111602609.8677 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111586556.6797 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111589694.8818 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111586061.8857 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111592742.5708 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111586203.5127 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111592992.8643 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111598946.1001 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111587162.9731 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593628.8994 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111600381.0415 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111600825.8799 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111584258.8428 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593332.7700 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593857.5630 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111583681.7764 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111594278.8398 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111594792.8179 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111593525.5762 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111583065.8267 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111594161.6753 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590045.9883 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111592120.6982 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111588926.8755 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111587558.2422 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111592039.0698 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111584979.7251 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111589962.0420 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 25s 6us/step - loss: 4202111588215.9072 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111586979.3726 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111583530.6211 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111591568.9956 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111590124.7847 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111599405.2300 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111583422.3408 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111588671.4316 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111585274.1812 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111597391.9346 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111589621.1064 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111585619.8804 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111595209.9741 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111589508.3198 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111596618.7788 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111581684.0601 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593743.4888 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111596025.6182 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111591627.5781 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111587258.5068 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593478.0020 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111592668.6670 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111591822.5088 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111585291.6909 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111594411.7119 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111598791.8550 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111590920.6006 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593164.3623 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111597950.8472 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111588871.2549 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111593937.1318 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111582003.1069 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111590393.1040 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111583275.8213 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111591487.4956 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111592825.9375 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111597542.9609 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111594949.3804 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111595222.3345 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111594443.3848 - acc: 0.3720 - val_loss: 4259460097290.5767 - val_acc: 0.3714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111586080.1689 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111589394.7607 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111592245.8452 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111589002.0669 - acc: 0.3720 - val_loss: 4259460075917.7622 - val_acc: 0.3714\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE3.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc0fcb810b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE3.h5')\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.272524e+06</td>\n",
       "      <td>1.272524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.273575e+12</td>\n",
       "      <td>1.273060e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.157309e+13</td>\n",
       "      <td>3.565727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.370081e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.821978e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.526166e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.597485e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.300391e+16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error    true_class\n",
       "count          1.272524e+06  1.272524e+06\n",
       "mean           4.273575e+12  1.273060e-03\n",
       "std            7.157309e+13  3.565727e-02\n",
       "min            1.370081e+01  0.000000e+00\n",
       "25%            3.821978e+08  0.000000e+00\n",
       "50%            2.526166e+10  0.000000e+00\n",
       "75%            5.597485e+11  0.000000e+00\n",
       "max            2.300391e+16  1.000000e+00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
