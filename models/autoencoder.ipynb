{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 21:04:56 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "|  0%   49C    P8    13W / 200W |    650MiB /  8118MiB |     15%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           388MiB |\r\n",
      "|    0      2077      G   compiz                                       188MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    71MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"tanh\")(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"tanh\")(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"tanh\")(encoder2)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"tanh\")(encoder3)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"tanh\")(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"tanh\")(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111187742.0273 - acc: 0.5849 - val_loss: 4259459665706.5132 - val_acc: 0.4685\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111170965.3818 - acc: 0.0702 - val_loss: 4259459665706.5132 - val_acc: 3.2416e-05\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111174817.5132 - acc: 0.0143 - val_loss: 4259459665706.5132 - val_acc: 0.0894\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111170532.2598 - acc: 0.0841 - val_loss: 4259459665706.5132 - val_acc: 1.0118e-04\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111173784.2788 - acc: 0.0277 - val_loss: 4259459612145.7271 - val_acc: 0.1371\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111082211.2085 - acc: 0.0192 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111066919.5889 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111062819.3525 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111055561.4722 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111058607.8740 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111062012.0771 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111068208.6543 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111063570.7490 - acc: 0.0024 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063367.0635 - acc: 0.0019 - val_loss: 4259459554722.3833 - val_acc: 0.0018\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111058189.9448 - acc: 0.0022 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111062826.8198 - acc: 0.0019 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111059386.0508 - acc: 0.0031 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111060854.0815 - acc: 0.0021 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111065651.8965 - acc: 0.0028 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111061538.2695 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111066941.9917 - acc: 0.0023 - val_loss: 4259459554722.3833 - val_acc: 0.0083\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111056570.7593 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111065596.5332 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111060254.6123 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111068489.3335 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111064954.3184 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111065019.5308 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111061463.8511 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111067227.6924 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111066598.9966 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111059050.5874 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111071702.3398 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111061368.8320 - acc: 0.0018 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111064584.5420 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111066409.9883 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111063479.5923 - acc: 0.0019 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 21s 5us/step - loss: 4202111069347.3369 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111068448.1333 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111065782.1938 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111060036.2490 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111071114.5874 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111065660.6519 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111066797.0171 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111065441.5156 - acc: 0.0023 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063183.2051 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111056880.9229 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111061990.1895 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111066019.8701 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111060221.6514 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111065624.0864 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063970.3955 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063605.6406 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111068069.8599 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111070457.8232 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111061544.7075 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111067493.3076 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111059515.8330 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111071968.2129 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111062784.0742 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111064083.1821 - acc: 0.0026 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111061543.1621 - acc: 0.0022 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111064117.9453 - acc: 0.0018 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063739.2856 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063757.1821 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111066196.7749 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111068528.7319 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111067951.9224 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111063181.2739 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067334.6851 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065560.2256 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069554.1128 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111054943.3340 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063584.6543 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111055084.9609 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111059039.9658 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060948.3276 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 24s 6us/step - loss: 4202111060475.2930 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070237.1421 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069515.2295 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064863.6768 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111059117.2163 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061843.6694 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064701.1919 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062533.5225 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062411.3374 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063606.2847 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064937.8379 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111071533.4175 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111072324.2129 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061944.7397 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066267.8462 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060541.2139 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111052483.6553 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065088.9927 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061388.4023 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069616.9438 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064526.9902 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111058681.6484 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064967.9663 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 100/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067616.7812 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065184.7847 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111071724.0991 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067965.3130 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066851.8652 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069942.6860 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066128.5366 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061518.1846 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060841.4634 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066419.0010 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061945.6406 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070157.9595 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062597.6416 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064757.0703 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067708.3237 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111056517.1987 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066843.1104 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067468.5874 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060044.4888 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063709.8013 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065304.1372 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067523.6934 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063518.2183 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066563.0747 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065623.8286 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066748.6064 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111056676.0786 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067144.3896 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111055352.7651 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111076812.1216 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062576.0112 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067512.1060 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069968.9517 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062950.9370 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063542.6812 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111056656.2505 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061565.0498 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064696.5566 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065907.5981 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111073773.8311 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111059241.9775 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062688.1538 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111068003.1660 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111077075.8057 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064107.3232 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070618.3770 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111072954.4536 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060039.0815 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061540.5225 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065068.1348 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 150/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066103.8164 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111054579.9956 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111062830.1675 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111070326.8823 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111071244.6265 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 22s 5us/step - loss: 4202111057512.1943 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063884.5176 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111058380.7554 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111063943.3574 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111063130.1597 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067014.8657 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064226.4185 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111074449.1357 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111068402.5547 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061337.6743 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060540.6987 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062231.7280 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111069364.7188 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111063383.8013 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060005.6055 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111053966.3633 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065250.7051 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111066199.8008 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067248.1641 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111058581.9946 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060409.1143 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061711.8276 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065321.5190 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111064310.3003 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065071.9976 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111056104.4199 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070702.3232 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111069470.1665 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065081.5254 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060683.3564 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070052.3828 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061930.7061 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067043.4487 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111065100.1938 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111068233.8896 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060221.6514 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111071086.6484 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111070144.5698 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111060583.7021 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 22s 6us/step - loss: 4202111060921.2900 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111058316.3794 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111061962.5073 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111056450.2471 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111059807.0698 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111062863.6426 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n",
      "Epoch 200/200\n",
      "4072076/4072076 [==============================] - 23s 6us/step - loss: 4202111067621.8022 - acc: 0.0017 - val_loss: 4259459554722.3833 - val_acc: 0.0017\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99b1856ba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.272524e+06</td>\n",
       "      <td>1.272524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.273575e+12</td>\n",
       "      <td>1.273060e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.157309e+13</td>\n",
       "      <td>3.565727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.149011e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.821797e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.526156e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.597479e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.300391e+16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error    true_class\n",
       "count          1.272524e+06  1.272524e+06\n",
       "mean           4.273575e+12  1.273060e-03\n",
       "std            7.157309e+13  3.565727e-02\n",
       "min            1.149011e+01  0.000000e+00\n",
       "25%            3.821797e+08  0.000000e+00\n",
       "50%            2.526156e+10  0.000000e+00\n",
       "75%            5.597479e+11  0.000000e+00\n",
       "max            2.300391e+16  1.000000e+00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3737323    0\n",
       "264914     0\n",
       "85647      0\n",
       "5899326    0\n",
       "2544263    0\n",
       "3494160    0\n",
       "2331654    0\n",
       "1414955    0\n",
       "2938135    0\n",
       "6133806    0\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:10]\n",
    "y_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
