{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 23 23:59:31 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 29%   53C    P2    65W / 200W |   1299MiB /  8118MiB |     73%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1214      G   /usr/lib/xorg/Xorg                           389MiB |\r\n",
      "|    0      2077      G   compiz                                       190MiB |\r\n",
      "|    0      2376      G   ...-token=DF74AF64EAA3938418CFC9F567B83A3C    82MiB |\r\n",
      "|    0     22421      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     23089      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "|    0     23714      C   /opt/anaconda/bin/python                     203MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  6362620  samples\n",
      "(6362620, 12)\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64      170136.00       160296.36             0.0   \n",
      "1     1   1864.28       21249.00        19384.72             0.0   \n",
      "2     1    181.00         181.00            0.00             0.0   \n",
      "3     1    181.00         181.00            0.00         21182.0   \n",
      "4     1  11668.14       41554.00        29885.86             0.0   \n",
      "5     1   7817.71       53860.00        46042.29             0.0   \n",
      "6     1   7107.77      183195.00       176087.23             0.0   \n",
      "7     1   7861.64      176087.23       168225.59             0.0   \n",
      "8     1   4024.36        2671.00            0.00             0.0   \n",
      "9     1   5337.77       41720.00        36382.23         41898.0   \n",
      "\n",
      "   newbalanceDest  isFraud  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
      "0            0.00        0        0         0      0        1         0  \n",
      "1            0.00        0        0         0      0        1         0  \n",
      "2            0.00        1        0         0      0        0         1  \n",
      "3            0.00        1        0         1      0        0         0  \n",
      "4            0.00        0        0         0      0        1         0  \n",
      "5            0.00        0        0         0      0        1         0  \n",
      "6            0.00        0        0         0      0        1         0  \n",
      "7            0.00        0        0         0      0        1         0  \n",
      "8            0.00        0        0         0      0        1         0  \n",
      "9        40348.79        0        0         0      1        0         0  \n",
      "               step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  6.362620e+06  6.362620e+06   6.362620e+06    6.362620e+06   \n",
      "mean   2.433972e+02  1.798619e+05   8.338831e+05    8.551137e+05   \n",
      "std    1.423320e+02  6.038582e+05   2.888243e+06    2.924049e+06   \n",
      "min    1.000000e+00  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%    1.560000e+02  1.338957e+04   0.000000e+00    0.000000e+00   \n",
      "50%    2.390000e+02  7.487194e+04   1.420800e+04    0.000000e+00   \n",
      "75%    3.350000e+02  2.087215e+05   1.073152e+05    1.442584e+05   \n",
      "max    7.430000e+02  9.244552e+07   5.958504e+07    4.958504e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest       isFraud       CASH_IN  \\\n",
      "count    6.362620e+06    6.362620e+06  6.362620e+06  6.362620e+06   \n",
      "mean     1.100702e+06    1.224996e+06  1.290820e-03  2.199226e-01   \n",
      "std      3.399180e+06    3.674129e+06  3.590480e-02  4.141940e-01   \n",
      "min      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%      0.000000e+00    0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%      1.327057e+05    2.146614e+05  0.000000e+00  0.000000e+00   \n",
      "75%      9.430367e+05    1.111909e+06  0.000000e+00  0.000000e+00   \n",
      "max      3.560159e+08    3.561793e+08  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           CASH_OUT         DEBIT       PAYMENT      TRANSFER  \n",
      "count  6.362620e+06  6.362620e+06  6.362620e+06  6.362620e+06  \n",
      "mean   3.516633e-01  6.511783e-03  3.381461e-01  8.375622e-02  \n",
      "std    4.774895e-01  8.043246e-02  4.730786e-01  2.770219e-01  \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "75%    1.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "\n",
    "fraud_dataset = pd.read_csv('../data/nonames.csv')\n",
    "print(\"There are \", len(fraud_dataset), \" samples\")\n",
    "print(fraud_dataset.shape)\n",
    "print(fraud_dataset.head(10))\n",
    "print(fraud_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (5090096, 12)\n",
      "X_train:  (5090096, 11)\n",
      "X_train:  (4072076, 11)\n",
      "X_val:  (1018020, 11)\n",
      "X_test:  (1272524, 12)\n",
      "X_test:  (1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(fraud_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "# y_train = X_train[\"isFraud\"].copy(deep=True)\n",
    "X_train.pop(\"isFraud\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "y_test = X_test[\"isFraud\"].copy(deep=True)\n",
    "X_test.pop(\"isFraud\")\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1]\n",
    "\n",
    "hidden_layer = [10, 8, 4, 2]\n",
    "input_layer = Input(shape=(input_shape,))\n",
    "encoder1 = Dense(hidden_layer[0], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(encoder1)\n",
    "encoder3 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(encoder2)\n",
    "latent = Dense(hidden_layer[3], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(encoder3)\n",
    "decoder1 = Dense(hidden_layer[2], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(latent)\n",
    "decoder2 = Dense(hidden_layer[1], activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(decoder1)\n",
    "decoder3 = Dense(input_shape, activation=\"relu\", activity_regularizer=regularizers.l1(10e-5))(decoder2)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                99        \n",
      "=================================================================\n",
      "Total params: 405\n",
      "Trainable params: 405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4072076 samples, validate on 1018020 samples\n",
      "Epoch 1/200\n",
      "4072076/4072076 [==============================] - 33s 8us/step - loss: 2505320429483.0942 - acc: 0.3929 - val_loss: 890393460848.9797 - val_acc: 0.5245\n",
      "Epoch 2/200\n",
      "4072076/4072076 [==============================] - 33s 8us/step - loss: 869410075012.0063 - acc: 0.5415 - val_loss: 865403241203.1299 - val_acc: 0.5686\n",
      "Epoch 3/200\n",
      "4072076/4072076 [==============================] - 33s 8us/step - loss: 855894278718.8938 - acc: 0.5644 - val_loss: 864738552211.5969 - val_acc: 0.5656\n",
      "Epoch 4/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855648047884.2252 - acc: 0.5621 - val_loss: 864611817234.8148 - val_acc: 0.5646\n",
      "Epoch 5/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855648330072.2427 - acc: 0.5610 - val_loss: 864858003176.4877 - val_acc: 0.5612\n",
      "Epoch 6/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855582119325.9226 - acc: 0.5612 - val_loss: 864574169163.1589 - val_acc: 0.5615\n",
      "Epoch 7/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855556831107.5834 - acc: 0.5620 - val_loss: 864559934623.0287 - val_acc: 0.5646\n",
      "Epoch 8/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855461232086.9779 - acc: 0.5622 - val_loss: 864453386261.7269 - val_acc: 0.5634\n",
      "Epoch 9/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855377820527.6881 - acc: 0.5628 - val_loss: 864538501123.3798 - val_acc: 0.5666\n",
      "Epoch 10/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855413705169.8992 - acc: 0.5647 - val_loss: 864255221618.3729 - val_acc: 0.5713\n",
      "Epoch 11/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855315819146.8435 - acc: 0.5675 - val_loss: 864338445837.8508 - val_acc: 0.5706\n",
      "Epoch 12/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855259759101.1660 - acc: 0.5669 - val_loss: 864183659682.4084 - val_acc: 0.5694\n",
      "Epoch 13/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855247966898.4366 - acc: 0.5671 - val_loss: 864128217538.3903 - val_acc: 0.5705\n",
      "Epoch 14/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855172693538.5714 - acc: 0.5686 - val_loss: 864064840092.4889 - val_acc: 0.5688\n",
      "Epoch 15/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855145273340.0066 - acc: 0.5670 - val_loss: 864295172825.5807 - val_acc: 0.5684\n",
      "Epoch 16/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 855077115449.1171 - acc: 0.5834 - val_loss: 864269731992.3699 - val_acc: 0.5761\n",
      "Epoch 17/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855093344068.6074 - acc: 0.5783 - val_loss: 864040765087.1796 - val_acc: 0.5863\n",
      "Epoch 18/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855119940974.3065 - acc: 0.5948 - val_loss: 864182576847.3007 - val_acc: 0.5944\n",
      "Epoch 19/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855112586243.1715 - acc: 0.6082 - val_loss: 864097990115.2823 - val_acc: 0.6243\n",
      "Epoch 20/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855002159973.0902 - acc: 0.6656 - val_loss: 863993683818.0040 - val_acc: 0.7015\n",
      "Epoch 21/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855019376450.4811 - acc: 0.6998 - val_loss: 864468382122.7321 - val_acc: 0.6990\n",
      "Epoch 22/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854983966645.1951 - acc: 0.6994 - val_loss: 864110257661.7570 - val_acc: 0.6978\n",
      "Epoch 23/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854969303188.2740 - acc: 0.7004 - val_loss: 864117491538.1447 - val_acc: 0.7029\n",
      "Epoch 24/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 855016663206.0438 - acc: 0.7043 - val_loss: 864133723339.7297 - val_acc: 0.7045\n",
      "Epoch 25/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854968870572.7405 - acc: 0.7047 - val_loss: 864166950595.9543 - val_acc: 0.7021\n",
      "Epoch 26/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 855094878104.2179 - acc: 0.7054 - val_loss: 864219729034.7502 - val_acc: 0.7084\n",
      "Epoch 27/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854912186887.6974 - acc: 0.7051 - val_loss: 864108467307.4877 - val_acc: 0.7079\n",
      "Epoch 28/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854960831867.8536 - acc: 0.7048 - val_loss: 864093276827.0555 - val_acc: 0.7075\n",
      "Epoch 29/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854936910878.8070 - acc: 0.7079 - val_loss: 864137307266.9044 - val_acc: 0.7127\n",
      "Epoch 30/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854921073569.4648 - acc: 0.7189 - val_loss: 864066406887.1248 - val_acc: 0.7108\n",
      "Epoch 31/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 854888474206.9904 - acc: 0.7196 - val_loss: 864022312569.1173 - val_acc: 0.7339\n",
      "Epoch 32/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 855075304385.1942 - acc: 0.7201 - val_loss: 864008610520.5747 - val_acc: 0.7179\n",
      "Epoch 33/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 854869902245.0659 - acc: 0.7179 - val_loss: 864100435434.1222 - val_acc: 0.7127\n",
      "Epoch 34/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 854916658894.8960 - acc: 0.7199 - val_loss: 864025865864.9297 - val_acc: 0.7195\n",
      "Epoch 35/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 842817801312.1365 - acc: 0.7134 - val_loss: 833736941637.6064 - val_acc: 0.7277\n",
      "Epoch 36/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 825250065782.8284 - acc: 0.7089 - val_loss: 833030490931.5460 - val_acc: 0.7155\n",
      "Epoch 37/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824917259698.0240 - acc: 0.7123 - val_loss: 833150571275.1099 - val_acc: 0.7277\n",
      "Epoch 38/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824906651243.4109 - acc: 0.7161 - val_loss: 832997787532.0026 - val_acc: 0.7140\n",
      "Epoch 39/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824820797663.4049 - acc: 0.7173 - val_loss: 834805136935.9634 - val_acc: 0.6482\n",
      "Epoch 40/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824781600563.2582 - acc: 0.7168 - val_loss: 833278058441.7633 - val_acc: 0.6816\n",
      "Epoch 41/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824737814071.5077 - acc: 0.7176 - val_loss: 834066098030.3092 - val_acc: 0.7269\n",
      "Epoch 42/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824845302490.7058 - acc: 0.7176 - val_loss: 850576670634.2996 - val_acc: 0.6941\n",
      "Epoch 43/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 825031982888.7266 - acc: 0.7180 - val_loss: 833011194964.9963 - val_acc: 0.7073\n",
      "Epoch 44/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824717612051.8450 - acc: 0.7156 - val_loss: 835062296354.7882 - val_acc: 0.7271\n",
      "Epoch 45/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824594229815.1385 - acc: 0.7176 - val_loss: 833162792401.4583 - val_acc: 0.7263\n",
      "Epoch 46/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824700355285.9844 - acc: 0.7169 - val_loss: 832891078600.9988 - val_acc: 0.7289\n",
      "Epoch 47/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824667161338.2441 - acc: 0.7183 - val_loss: 832923969008.0770 - val_acc: 0.7227\n",
      "Epoch 48/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824969337649.1709 - acc: 0.7167 - val_loss: 832939019593.3835 - val_acc: 0.7115\n",
      "Epoch 49/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824431855938.9523 - acc: 0.7203 - val_loss: 833417950270.7465 - val_acc: 0.6859\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824618839954.0579 - acc: 0.7192 - val_loss: 832739021862.2031 - val_acc: 0.7198\n",
      "Epoch 51/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824655894417.7219 - acc: 0.7160 - val_loss: 832712033884.4700 - val_acc: 0.7265\n",
      "Epoch 52/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824503246580.4935 - acc: 0.7150 - val_loss: 832747053162.8640 - val_acc: 0.7226\n",
      "Epoch 53/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 824405427719.3102 - acc: 0.7143 - val_loss: 832393448599.4645 - val_acc: 0.7233\n",
      "Epoch 54/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824081545826.5481 - acc: 0.7118 - val_loss: 832299418976.6796 - val_acc: 0.7156\n",
      "Epoch 55/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 824003161573.8407 - acc: 0.7167 - val_loss: 831982878886.4923 - val_acc: 0.7148\n",
      "Epoch 56/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 823683304046.4294 - acc: 0.7172 - val_loss: 831660232972.1259 - val_acc: 0.7273\n",
      "Epoch 57/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 823467100503.7216 - acc: 0.7213 - val_loss: 831319037656.5547 - val_acc: 0.7085\n",
      "Epoch 58/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 823207587938.0568 - acc: 0.7234 - val_loss: 838268909202.6262 - val_acc: 0.6566\n",
      "Epoch 59/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 823182174430.1958 - acc: 0.7241 - val_loss: 830877846389.8533 - val_acc: 0.7205\n",
      "Epoch 60/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 823054610086.3612 - acc: 0.7245 - val_loss: 830651405240.2007 - val_acc: 0.7310\n",
      "Epoch 61/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 822770549483.5503 - acc: 0.7234 - val_loss: 830805354416.7773 - val_acc: 0.7292\n",
      "Epoch 62/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 822919791937.5552 - acc: 0.7199 - val_loss: 830953798466.7549 - val_acc: 0.7125\n",
      "Epoch 63/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 822768819360.2590 - acc: 0.7207 - val_loss: 830743028064.6997 - val_acc: 0.7419\n",
      "Epoch 64/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 822503867696.5277 - acc: 0.7242 - val_loss: 830435723729.8002 - val_acc: 0.7271\n",
      "Epoch 65/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 347199778577.7485 - acc: 0.7796 - val_loss: 4039234440.5524 - val_acc: 0.8225\n",
      "Epoch 66/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 5180647586.2342 - acc: 0.8251 - val_loss: 3993199213.3838 - val_acc: 0.8268\n",
      "Epoch 67/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4839973884.3674 - acc: 0.8053 - val_loss: 5757616571.3089 - val_acc: 0.7553\n",
      "Epoch 68/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4865082798.0390 - acc: 0.8067 - val_loss: 3702769990.0692 - val_acc: 0.8104\n",
      "Epoch 69/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4692547139.2805 - acc: 0.7983 - val_loss: 3506280607.5266 - val_acc: 0.8006\n",
      "Epoch 70/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4718232620.1390 - acc: 0.8003 - val_loss: 3523747306.8012 - val_acc: 0.8038\n",
      "Epoch 71/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4795861574.9124 - acc: 0.8061 - val_loss: 5017179665.9901 - val_acc: 0.8246\n",
      "Epoch 72/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4719397375.9135 - acc: 0.8013 - val_loss: 5156236549.4971 - val_acc: 0.7874\n",
      "Epoch 73/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4778855520.1001 - acc: 0.7976 - val_loss: 4292812447.3556 - val_acc: 0.8145\n",
      "Epoch 74/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4759216654.7398 - acc: 0.7917 - val_loss: 3984988894.7559 - val_acc: 0.8160\n",
      "Epoch 75/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4654645211.1172 - acc: 0.7995 - val_loss: 3924577252.5698 - val_acc: 0.7786\n",
      "Epoch 76/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4946992290.9719 - acc: 0.7949 - val_loss: 4012413911.0861 - val_acc: 0.7437\n",
      "Epoch 77/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4695591817.8054 - acc: 0.7939 - val_loss: 3452795815.4177 - val_acc: 0.7703\n",
      "Epoch 78/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4604025708.6707 - acc: 0.7897 - val_loss: 3533905800.9347 - val_acc: 0.8026\n",
      "Epoch 79/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4874829856.6781 - acc: 0.7914 - val_loss: 3594558253.3448 - val_acc: 0.8000\n",
      "Epoch 80/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4609828155.2121 - acc: 0.7958 - val_loss: 4017022017.3516 - val_acc: 0.8168\n",
      "Epoch 81/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4801899429.0143 - acc: 0.7941 - val_loss: 4124813176.5138 - val_acc: 0.8208\n",
      "Epoch 82/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4614580714.5555 - acc: 0.7984 - val_loss: 3316176190.0021 - val_acc: 0.7898\n",
      "Epoch 83/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4802158483.8636 - acc: 0.7937 - val_loss: 7606510810.4759 - val_acc: 0.7450\n",
      "Epoch 84/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4575376733.7218 - acc: 0.7890 - val_loss: 4620671705.3241 - val_acc: 0.7986\n",
      "Epoch 85/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4393280933.7790 - acc: 0.7967 - val_loss: 3406235789.4812 - val_acc: 0.7929\n",
      "Epoch 86/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4713842770.4732 - acc: 0.7921 - val_loss: 3769784149.4943 - val_acc: 0.8264\n",
      "Epoch 87/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4468547645.8618 - acc: 0.8071 - val_loss: 3707986212.7546 - val_acc: 0.7913\n",
      "Epoch 88/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4573203168.5797 - acc: 0.8028 - val_loss: 3332376373.3164 - val_acc: 0.7964\n",
      "Epoch 89/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4556007102.6546 - acc: 0.8018 - val_loss: 3963415513.1431 - val_acc: 0.7866\n",
      "Epoch 90/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4528414607.1846 - acc: 0.7996 - val_loss: 3556261963.1187 - val_acc: 0.8163\n",
      "Epoch 91/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4373531311.6843 - acc: 0.8039 - val_loss: 3453270298.6003 - val_acc: 0.8207\n",
      "Epoch 92/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4441892734.4189 - acc: 0.8001 - val_loss: 3266546373.9108 - val_acc: 0.7728\n",
      "Epoch 93/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4571680823.4896 - acc: 0.7937 - val_loss: 3474138753.6220 - val_acc: 0.8165\n",
      "Epoch 94/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4508852121.2419 - acc: 0.7956 - val_loss: 3468168859.1661 - val_acc: 0.8030\n",
      "Epoch 95/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4466217395.9961 - acc: 0.7971 - val_loss: 6142709280.3992 - val_acc: 0.7799\n",
      "Epoch 96/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4427864405.6180 - acc: 0.7984 - val_loss: 3574649902.8737 - val_acc: 0.7684\n",
      "Epoch 97/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4462664712.7539 - acc: 0.8066 - val_loss: 3366291583.5499 - val_acc: 0.7992\n",
      "Epoch 98/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4329528797.4995 - acc: 0.8009 - val_loss: 3318501541.4814 - val_acc: 0.8065\n",
      "Epoch 99/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4402538465.1151 - acc: 0.8094 - val_loss: 4186637829.0998 - val_acc: 0.8056\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4319422417.3540 - acc: 0.7968 - val_loss: 4322762976.6218 - val_acc: 0.8221\n",
      "Epoch 101/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4339968469.0950 - acc: 0.8010 - val_loss: 3631126799.7017 - val_acc: 0.8101\n",
      "Epoch 102/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4475079014.1102 - acc: 0.7976 - val_loss: 3481940180.7474 - val_acc: 0.8416\n",
      "Epoch 103/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4307656504.4779 - acc: 0.8048 - val_loss: 3866275346.1259 - val_acc: 0.8366\n",
      "Epoch 104/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4354115287.9272 - acc: 0.7948 - val_loss: 3712582363.2655 - val_acc: 0.8466\n",
      "Epoch 105/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4342621022.6166 - acc: 0.8048 - val_loss: 3718892861.7657 - val_acc: 0.8148\n",
      "Epoch 106/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4343559782.3619 - acc: 0.8057 - val_loss: 3729669817.2921 - val_acc: 0.8295\n",
      "Epoch 107/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4666793667.8050 - acc: 0.8031 - val_loss: 3891075712.0981 - val_acc: 0.8309\n",
      "Epoch 108/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4215633872.4702 - acc: 0.7996 - val_loss: 3159579474.0290 - val_acc: 0.8115\n",
      "Epoch 109/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4314974467.5118 - acc: 0.8080 - val_loss: 4390295606.5133 - val_acc: 0.7813\n",
      "Epoch 110/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4365566368.0425 - acc: 0.8075 - val_loss: 3606237872.3197 - val_acc: 0.8176\n",
      "Epoch 111/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4215504172.5925 - acc: 0.8026 - val_loss: 3222690612.9945 - val_acc: 0.8236\n",
      "Epoch 112/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4284332193.4213 - acc: 0.7974 - val_loss: 7032318328.0863 - val_acc: 0.7786\n",
      "Epoch 113/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4332517126.3659 - acc: 0.7983 - val_loss: 3571847256.3862 - val_acc: 0.7899\n",
      "Epoch 114/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4438393971.0652 - acc: 0.7976 - val_loss: 3172509500.7246 - val_acc: 0.7926\n",
      "Epoch 115/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4267788793.0972 - acc: 0.8045 - val_loss: 10656628254.0958 - val_acc: 0.7312\n",
      "Epoch 116/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4339773275.2824 - acc: 0.8099 - val_loss: 3747205124.6522 - val_acc: 0.8051\n",
      "Epoch 117/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4359789603.5941 - acc: 0.8053 - val_loss: 3508914666.2832 - val_acc: 0.8328\n",
      "Epoch 118/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4208809085.8888 - acc: 0.8081 - val_loss: 9496578372.9275 - val_acc: 0.7325\n",
      "Epoch 119/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4333672924.2630 - acc: 0.8028 - val_loss: 3220512479.9478 - val_acc: 0.8057\n",
      "Epoch 120/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4303216235.1239 - acc: 0.8068 - val_loss: 3444324946.3207 - val_acc: 0.8260\n",
      "Epoch 121/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4342993544.0328 - acc: 0.8066 - val_loss: 3360450061.2172 - val_acc: 0.8444\n",
      "Epoch 122/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4312284716.6976 - acc: 0.8066 - val_loss: 3365115644.5800 - val_acc: 0.7994\n",
      "Epoch 123/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4213944182.0246 - acc: 0.8026 - val_loss: 3248941333.5006 - val_acc: 0.7970\n",
      "Epoch 124/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4395805937.2431 - acc: 0.8129 - val_loss: 8978276598.9522 - val_acc: 0.7394\n",
      "Epoch 125/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4230015953.4514 - acc: 0.8065 - val_loss: 3124519153.0225 - val_acc: 0.7956\n",
      "Epoch 126/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4220874687.1100 - acc: 0.8114 - val_loss: 3616454607.2956 - val_acc: 0.8167\n",
      "Epoch 127/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4341934403.6519 - acc: 0.8050 - val_loss: 3466256756.6060 - val_acc: 0.8528\n",
      "Epoch 128/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4296030759.8467 - acc: 0.8176 - val_loss: 3398681968.9798 - val_acc: 0.8086\n",
      "Epoch 129/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4192198278.1614 - acc: 0.8022 - val_loss: 3178653510.7934 - val_acc: 0.8015\n",
      "Epoch 130/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4895973431.2293 - acc: 0.8068 - val_loss: 3442371993.7931 - val_acc: 0.8111\n",
      "Epoch 131/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4258542513.5316 - acc: 0.8135 - val_loss: 3503572113.5802 - val_acc: 0.8054\n",
      "Epoch 132/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4245169122.1534 - acc: 0.8094 - val_loss: 3336145541.7762 - val_acc: 0.7971\n",
      "Epoch 133/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4205843948.9998 - acc: 0.8079 - val_loss: 3754344119.9593 - val_acc: 0.7945\n",
      "Epoch 134/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4265589901.2266 - acc: 0.8058 - val_loss: 3546974387.3021 - val_acc: 0.8164\n",
      "Epoch 135/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4104123794.9336 - acc: 0.8129 - val_loss: 3553992069.2783 - val_acc: 0.8318\n",
      "Epoch 136/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4132734281.6836 - acc: 0.8031 - val_loss: 4702748825.7931 - val_acc: 0.7779\n",
      "Epoch 137/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4119976118.1725 - acc: 0.8016 - val_loss: 3156329956.1423 - val_acc: 0.8089\n",
      "Epoch 138/200\n",
      "4072076/4072076 [==============================] - 31s 8us/step - loss: 4195418735.9474 - acc: 0.8026 - val_loss: 4266624880.1902 - val_acc: 0.7966\n",
      "Epoch 139/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4046458182.3572 - acc: 0.8002 - val_loss: 3348960760.7879 - val_acc: 0.8111\n",
      "Epoch 140/200\n",
      "4072076/4072076 [==============================] - 32s 8us/step - loss: 4291944205.9421 - acc: 0.8047 - val_loss: 3127588902.0019 - val_acc: 0.8093\n",
      "Epoch 141/200\n",
      "4072076/4072076 [==============================] - 30s 7us/step - loss: 4121321551.7968 - acc: 0.8080 - val_loss: 6394758557.2030 - val_acc: 0.7546\n",
      "Epoch 142/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4176015618.3145 - acc: 0.8076 - val_loss: 4373667578.9103 - val_acc: 0.8112\n",
      "Epoch 143/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4136550774.2490 - acc: 0.8089 - val_loss: 4022910948.3385 - val_acc: 0.8272\n",
      "Epoch 144/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4255855360.1203 - acc: 0.8086 - val_loss: 3204676512.6230 - val_acc: 0.7949\n",
      "Epoch 145/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4281279801.0417 - acc: 0.8028 - val_loss: 3487123210.0638 - val_acc: 0.8215\n",
      "Epoch 146/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4303601768.2430 - acc: 0.8118 - val_loss: 3217835404.2792 - val_acc: 0.7992\n",
      "Epoch 147/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4288846238.5672 - acc: 0.8039 - val_loss: 3426614735.2604 - val_acc: 0.8198\n",
      "Epoch 148/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4276070867.1080 - acc: 0.8146 - val_loss: 3422731616.7098 - val_acc: 0.8153\n",
      "Epoch 149/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4259217839.4551 - acc: 0.8203 - val_loss: 4709564562.8928 - val_acc: 0.8034\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4255649673.9238 - acc: 0.8100 - val_loss: 3492701384.8932 - val_acc: 0.8477\n",
      "Epoch 151/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4251735504.2715 - acc: 0.8138 - val_loss: 3434146115.5042 - val_acc: 0.8285\n",
      "Epoch 152/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4136885633.7395 - acc: 0.8056 - val_loss: 3429859504.7321 - val_acc: 0.8144\n",
      "Epoch 153/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4077210295.6352 - acc: 0.8102 - val_loss: 3180258009.3141 - val_acc: 0.8284\n",
      "Epoch 154/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4191553802.3862 - acc: 0.8027 - val_loss: 3341297524.8675 - val_acc: 0.8150\n",
      "Epoch 155/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4354365634.8682 - acc: 0.8126 - val_loss: 3140957619.4781 - val_acc: 0.8088\n",
      "Epoch 156/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4152882259.7568 - acc: 0.8207 - val_loss: 3414028599.3583 - val_acc: 0.8124\n",
      "Epoch 157/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4186303696.8416 - acc: 0.8086 - val_loss: 3455541268.8618 - val_acc: 0.8348\n",
      "Epoch 158/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4223957827.8611 - acc: 0.8142 - val_loss: 3516109502.8093 - val_acc: 0.8193\n",
      "Epoch 159/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4199378166.4725 - acc: 0.8117 - val_loss: 3167894980.8999 - val_acc: 0.8287\n",
      "Epoch 160/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4246589459.1173 - acc: 0.8154 - val_loss: 3195845763.3672 - val_acc: 0.8116\n",
      "Epoch 161/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4079325461.4832 - acc: 0.8168 - val_loss: 3433767755.8479 - val_acc: 0.8584\n",
      "Epoch 162/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4112914542.2507 - acc: 0.8238 - val_loss: 3124220270.0024 - val_acc: 0.7942\n",
      "Epoch 163/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4147411539.8492 - acc: 0.8170 - val_loss: 3210669775.4364 - val_acc: 0.8256\n",
      "Epoch 164/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4266741275.7212 - acc: 0.8044 - val_loss: 3247044524.1805 - val_acc: 0.8038\n",
      "Epoch 165/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4187445694.8837 - acc: 0.8195 - val_loss: 3166882215.8502 - val_acc: 0.8242\n",
      "Epoch 166/200\n",
      "4072076/4072076 [==============================] - 29s 7us/step - loss: 4004838109.4121 - acc: 0.8130 - val_loss: 3173670229.4892 - val_acc: 0.8149\n",
      "Epoch 167/200\n",
      "4072076/4072076 [==============================] - 28s 7us/step - loss: 4324858494.8542 - acc: 0.8085 - val_loss: 4251426585.2173 - val_acc: 0.7687\n",
      "Epoch 168/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3960138867.9384 - acc: 0.8146 - val_loss: 3159761133.6780 - val_acc: 0.7941\n",
      "Epoch 169/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4085783618.4422 - acc: 0.8207 - val_loss: 3285222961.5896 - val_acc: 0.8338\n",
      "Epoch 170/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4206648719.9497 - acc: 0.8218 - val_loss: 3389430226.2880 - val_acc: 0.8447\n",
      "Epoch 171/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4100167730.9833 - acc: 0.8167 - val_loss: 3183322030.2325 - val_acc: 0.8100\n",
      "Epoch 172/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4068556479.3176 - acc: 0.8169 - val_loss: 3599017711.5238 - val_acc: 0.7864\n",
      "Epoch 173/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3965632946.8461 - acc: 0.8040 - val_loss: 3170878979.7972 - val_acc: 0.8100\n",
      "Epoch 174/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4229613415.5450 - acc: 0.8085 - val_loss: 5281153019.1517 - val_acc: 0.7993\n",
      "Epoch 175/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4037373142.5427 - acc: 0.8112 - val_loss: 3281845479.6377 - val_acc: 0.8308\n",
      "Epoch 176/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4189811166.0589 - acc: 0.8125 - val_loss: 3367020887.7675 - val_acc: 0.8269\n",
      "Epoch 177/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4085664802.0205 - acc: 0.8184 - val_loss: 4283011623.8226 - val_acc: 0.8302\n",
      "Epoch 178/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4128551396.6611 - acc: 0.8123 - val_loss: 3359774354.0580 - val_acc: 0.7823\n",
      "Epoch 179/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4151468442.8120 - acc: 0.8125 - val_loss: 3152454852.0700 - val_acc: 0.7978\n",
      "Epoch 180/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4016236171.0738 - acc: 0.8086 - val_loss: 12022351112.8668 - val_acc: 0.7118\n",
      "Epoch 181/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4059999187.7527 - acc: 0.8152 - val_loss: 3242028358.3458 - val_acc: 0.8471\n",
      "Epoch 182/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4174476471.5466 - acc: 0.8164 - val_loss: 3187817911.0087 - val_acc: 0.8186\n",
      "Epoch 183/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3998819829.4866 - acc: 0.8152 - val_loss: 11339552898.0696 - val_acc: 0.7854\n",
      "Epoch 184/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4074184415.3687 - acc: 0.8192 - val_loss: 3888298459.1347 - val_acc: 0.7899\n",
      "Epoch 185/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3974637493.6517 - acc: 0.8028 - val_loss: 3088431173.2896 - val_acc: 0.8126\n",
      "Epoch 186/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3995810812.3614 - acc: 0.8126 - val_loss: 3206440613.7429 - val_acc: 0.8101\n",
      "Epoch 187/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4153932336.6104 - acc: 0.8149 - val_loss: 8454306879.1840 - val_acc: 0.7424\n",
      "Epoch 188/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4092996773.5917 - acc: 0.8157 - val_loss: 3206031749.1174 - val_acc: 0.7899\n",
      "Epoch 189/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4091195264.7523 - acc: 0.8143 - val_loss: 3131038304.9462 - val_acc: 0.8184\n",
      "Epoch 190/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 3970173505.4571 - acc: 0.8149 - val_loss: 3746546815.5800 - val_acc: 0.7963\n",
      "Epoch 191/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4044093066.3101 - acc: 0.8151 - val_loss: 3302761199.2472 - val_acc: 0.8240\n",
      "Epoch 192/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4147464852.9827 - acc: 0.8137 - val_loss: 3501020542.0863 - val_acc: 0.8210\n",
      "Epoch 193/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4012399893.5463 - acc: 0.8127 - val_loss: 3098837977.9629 - val_acc: 0.8090\n",
      "Epoch 194/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4142771931.9469 - acc: 0.8200 - val_loss: 4062920122.3181 - val_acc: 0.8022\n",
      "Epoch 195/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4168117433.7389 - acc: 0.8211 - val_loss: 3656898202.2659 - val_acc: 0.8339\n",
      "Epoch 196/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4059486938.2836 - acc: 0.8226 - val_loss: 3113927950.5500 - val_acc: 0.8275\n",
      "Epoch 197/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4070539319.5295 - acc: 0.8172 - val_loss: 3815152109.2857 - val_acc: 0.8072\n",
      "Epoch 198/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4039794561.5230 - acc: 0.8149 - val_loss: 3192965716.4783 - val_acc: 0.8100\n",
      "Epoch 199/200\n",
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4134323448.1056 - acc: 0.8174 - val_loss: 3089556159.6844 - val_acc: 0.8218\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072076/4072076 [==============================] - 27s 7us/step - loss: 4000302838.9257 - acc: 0.8175 - val_loss: 3102320627.2405 - val_acc: 0.8191\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 1000\n",
    "# using mean squared error\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"../saved/basicAE4.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_val, X_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f54995404e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "autoencoder = load_model('../saved/basicAE4.h5')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272524, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = autoencoder.predict(X_test)\n",
    "# calculate my own MSE\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse})\n",
    "error_df.describe()\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0.       7151.7344 186529.     193934.75   291309.72   288420.\n",
      "      0.          0.          0.          0.          0.    ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>CASH_IN</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737323</th>\n",
       "      <td>278</td>\n",
       "      <td>330218.42</td>\n",
       "      <td>20866.0</td>\n",
       "      <td>351084.42</td>\n",
       "      <td>452419.57</td>\n",
       "      <td>122201.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
       "3737323   278  330218.42        20866.0       351084.42       452419.57   \n",
       "\n",
       "         newbalanceDest  CASH_IN  CASH_OUT  DEBIT  PAYMENT  TRANSFER  \n",
       "3737323       122201.15        1         0      0        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions[0][:])\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
